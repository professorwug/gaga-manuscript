@article{bellemare_distributional_nodate,
	title        = {A {Distributional} {Perspective} on {Reinforcement} {Learning}},
	author       = {Bellemare, Marc G and Dabney, Will and Munos, Rémi},
	pages        = 19,
	abstract     = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a speciﬁc purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a signiﬁcant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.}
}

@article{DBLP:journals/corr/GulrajaniAADC17,
  author       = {Ishaan Gulrajani and
                  Faruk Ahmed and
                  Mart{\'{\i}}n Arjovsky and
                  Vincent Dumoulin and
                  Aaron C. Courville},
  title        = {Improved Training of Wasserstein GANs},
  journal      = {CoRR},
  volume       = {abs/1704.00028},
  year         = {2017},
  url          = {http://arxiv.org/abs/1704.00028},
  eprinttype    = {arXiv},
  eprint       = {1704.00028},
  timestamp    = {Mon, 13 Aug 2018 16:47:43 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GulrajaniAADC17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{li_yau,
	Author = {Peter Li and Shing Tung Yau},
	Doi = {10.1007/BF02399203},
	Journal = {Acta Mathematica},
	Number = {none},
	Pages = {153 -- 201},
	Publisher = {Institut Mittag-Leffler},
	Title = {{On the parabolic kernel of the Schr{\"o}dinger operator}},
	Url = {https://doi.org/10.1007/BF02399203},
	Volume = {156},
	Year = {1986},
	Bdsk-Url-1 = {https://doi.org/10.1007/BF02399203}
 }


@article{saloff2010heat,
  title={The heat kernel and its estimates},
  author={Saloff-Coste, Laurent},
  journal={Probabilistic approach to geometry},
  volume={57},
  pages={405--436},
  year={2010},
  publisher={Mathematical Society of Japan Tokyo}
}

@article{bellemare_cramer_2017,
	title        = {The {Cramer} {Distance} as a {Solution} to {Biased} {Wasserstein} {Gradients}},
	author       = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi},
	year         = 2017,
	journal      = {arXiv:1705.10743 [cs, stat]},
	abstract     = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram{\textbackslash}'er distance. We show that the Cram{\textbackslash}'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram{\textbackslash}'er distance in practice we design a new algorithm, the Cram{\textbackslash}'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
@inproceedings{pham_unbalanced_2020,
	title        = {On {Unbalanced} {Optimal} {Transport}: {An} {Analysis} of {Sinkhorn} {Algorithm}},
	shorttitle   = {On {Unbalanced} {Optimal} {Transport}},
	author       = {Pham, Khiem and Le, Khang and Ho, Nhat and Pham, Tung and Bui, Hung},
	year         = 2020,
	booktitle    = {International {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {7673--7682}
}
@article{sriperumbudur_empirical_2012,
	title        = {On the empirical estimation of integral probability metrics},
	author       = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	year         = 2012,
	journal      = {Electronic Journal of Statistics},
	volume       = 6,
	number       = {none},
	pages        = {1550--1599},
	abstract     = {Given two probability measures, \${\textbackslash}mathbb\{P\}\$ and \${\textbackslash}mathbb\{Q\}\$ defined on a measurable space, \$S\$, the integral probability metric (IPM) is defined as \$\${\textbackslash}gamma\_\{{\textbackslash}mathcal\{F\}\}({\textbackslash}mathbb\{P\},{\textbackslash}mathbb\{Q\})={\textbackslash}sup{\textbackslash}left{\textbackslash}\{{\textbackslash}left{\textbackslash}vert {\textbackslash}int\_\{S\}f{\textbackslash},d{\textbackslash}mathbb\{P\}-{\textbackslash}int\_\{S\}f{\textbackslash},d{\textbackslash}mathbb\{Q\}{\textbackslash}right{\textbackslash}vert{\textbackslash},:{\textbackslash},f{\textbackslash}in{\textbackslash}mathcal\{F\}{\textbackslash}right{\textbackslash}\},\$\$ where \${\textbackslash}mathcal\{F\}\$ is a class of real-valued bounded measurable functions on \$S\$. By appropriately choosing \${\textbackslash}mathcal\{F\}\$, various popular distances between \${\textbackslash}mathbb\{P\}\$ and \${\textbackslash}mathbb\{Q\}\$, including the Kantorovich metric, Fortet-Mourier metric, dual-bounded Lipschitz distance (also called the Dudley metric), total variation distance, and kernel distance, can be obtained. In this paper, we consider the problem of estimating \${\textbackslash}gamma\_\{{\textbackslash}mathcal\{F\}\}\$ from finite random samples drawn i.i.d. from \${\textbackslash}mathbb\{P\}\$ and \${\textbackslash}mathbb\{Q\}\$. Although the above mentioned distances cannot be computed in closed form for every \${\textbackslash}mathbb\{P\}\$ and \${\textbackslash}mathbb\{Q\}\$, we show their empirical estimators to be easily computable, and strongly consistent (except for the total-variation distance). We further analyze their rates of convergence. Based on these results, we discuss the advantages of certain choices of \${\textbackslash}mathcal\{F\}\$ (and therefore the corresponding IPMs) over others—in particular, the kernel distance is shown to have three favorable properties compared with the other mentioned distances: it is computationally cheaper, the empirical estimate converges at a faster rate to the population value, and the rate of convergence is independent of the dimension \$d\$ of the space (for \$S={\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$). We also provide a novel interpretation of IPMs and their empirical estimators by relating them to the problem of binary classification: while the IPM between class-conditional distributions is the negative of the optimal risk associated with a binary classifier, the smoothness of an appropriate binary classifier (e.g., support vector machine, Lipschitz classifier, etc.) is inversely related to the empirical estimator of the IPM between these class-conditional distributions.},
	keywords     = {62G05, dual-bounded Lipschitz distance (Dudley metric), empirical estimation, Integral probability metrics, Kantorovich metric, kernel distance, Lipschitz classifier, Rademacher average, ‎reproducing kernel Hilbert ‎space, Support Vector Machine}
}
@article{manupriya_integral_2021,
	title        = {Integral {Probability} {Metric} based {Regularization} for {Optimal} {Transport}},
	author       = {Manupriya, Piyushi and Nath, J. Saketha and Jawanpuria, Pratik},
	year         = 2021,
	journal      = {arXiv:2011.05001 [cs, math]},
	abstract     = {Regularization in Optimal Transport (OT) problems has been shown to critically affect the associated computational and sample complexities. It also has been observed that regularization effectively helps in handling noisy marginals as well as marginals with unequal masses. However, existing works on OT restrict themselves to \${\textbackslash}phi\$-divergences based regularization. In this work, we propose and analyze Integral Probability Metric (IPM) based regularization in OT problems. While it is expected that the well-established advantages of IPMs are inherited by the IPM-regularized OT variants, we interestingly observe that some useful aspects of \${\textbackslash}phi\$-regularization are preserved. For example, we show that the OT formulation, where the marginal constraints are relaxed using IPM-regularization, also lifts the ground metric to that over (perhaps un-normalized) measures. Infact, the lifted metric turns out to be another IPM whose generating set is the intersection of that of the IPM employed for regularization and the set of 1-Lipschitz functions under the ground metric. Also, in the special case where the regularization is squared maximum mean discrepancy based, the proposed OT variant, as well as the corresponding Barycenter formulation, turn out to be those of minimizing a convex quadratic subject to non-negativity/simplex constraints and hence can be solved efficiently. Simulations confirm that the optimal transport plans/maps obtained with IPM-regularization are intrinsically different from those obtained with \${\textbackslash}phi\$-regularization. Empirical results illustrate the efficacy of the proposed IPM-regularized OT formulation. This draft contains the main paper and the Appendices.},
	keywords     = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
	annote       = {
		UOT by replacing Csisz`ardivergences by integral probability metrics

		-primal formulation eq 5, dual form eq.6

		Question: can we show an equivalence with OT with tresholded ground distance? (maybe it is already done, since they do it with the TV norm)
	}
}
@article{liero_optimal_2018,
	title        = {Optimal {Entropy}-{Transport} problems and a new {Hellinger}–{Kantorovich} distance between positive measures},
	author       = {Liero, Matthias and Mielke, Alexander and Savaré, Giuseppe},
	year         = 2018,
	journal      = {Invent. math.},
	volume       = 211,
	number       = 3,
	pages        = {969--1117},
	annote       = {
		Ce papier c'est avec des panalty divergence, ici KL

		"which replaces the ’hard’ marginal constraintsof OT by ’soft’ penalties using Csisz`ar divergences." (from Unbalanced minibatch OT)
	}
}
@article{sommerfeld_optimal_nodate,
	title        = {Optimal {Transport}: {Fast} {Probabilistic} {Approximation} with {Exact} {Solvers}},
	author       = {Sommerfeld, Max and Schrieber, Jorn and Zemel, Yoav and Munk, Axel},
	pages        = 23,
	abstract     = {We propose a simple subsampling scheme for fast randomized approximate computation of optimal transport distances on ﬁnite spaces. This scheme operates on a random subset of the full data and can use any exact algorithm as a black-box back-end, including state-of-the-art solvers and entropically penalized versions. It is based on averaging the exact distances between empirical measures generated from independent samples from the original measures and can easily be tuned towards higher accuracy or shorter computation times. To this end, we give non-asymptotic deviation bounds for its accuracy in the case of discrete optimal transport problems. In particular, we show that in many important instances, including images (2D-histograms), the approximation error is independent of the size of the full problem. We present numerical experiments that demonstrate that a very good approximation in typical applications can be obtained in a computation time that is several orders of magnitude smaller than what is required for exact computation of the full problem.},
	annote       = {Probabilistic bound on the W1}
}
@article{muzellec_missing_2020,
	title        = {Missing {Data} {Imputation} using {Optimal} {Transport}},
	author       = {Muzellec, Boris and Josse, Julie and Boyer, Claire and Cuturi, Marco},
	year         = 2020,
	journal      = {arXiv:2002.03860 [cs, stat]},
	abstract     = {Missing data is a crucial issue when applying machine learning algorithms to real-world datasets. Starting from the simple assumption that two batches extracted randomly from the same dataset should share the same distribution, we leverage optimal transport distances to quantify that criterion and turn it into a loss function to impute missing data values. We propose practical methods to minimize these losses using end-to-end learning, that can exploit or not parametric assumptions on the underlying distributions of values. We evaluate our methods on datasets from the UCI repository, in MCAR, MAR and MNAR settings. These experiments show that OT-based methods match or out-perform state-of-the-art imputation methods, even for high percentages of missing values.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
@article{courty_joint_2017,
	title        = {Joint {Distribution} {Optimal} {Transportation} for {Domain} {Adaptation}},
	author       = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
	year         = 2017,
	journal      = {arXiv:1705.08848 [cs, stat]},
	abstract     = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \${\textbackslash}mathcal\{P\}\_s\$ and \${\textbackslash}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \${\textbackslash}mathcal\{P\}{\textasciicircum}f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Accepted for publication at NIPS 2017}
}
@inproceedings{shen_wasserstein_2018,
	title        = {Wasserstein {Distance} {Guided} {Representation} {Learning} for {Domain} {Adaptation}},
	author       = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
	year         = 2018,
	booktitle    = {Thirty-{Second} {AAAI} {Conference} on {Artificial} {Intelligence}},
	copyright    = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	abstract     = {Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classification. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classification adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.}
}
@inproceedings{noauthor_unbalanced_2021,
	title        = {Unbalanced minibatch {Optimal} {Transport}; applications to {Domain} {Adaptation}},
	year         = 2021,
	booktitle    = {International {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {3186--3197}
}
@inproceedings{mukherjee_outlier-robust_2021,
	title        = {Outlier-{Robust} {Optimal} {Transport}},
	author       = {Mukherjee, Debarghya and Guha, Aritra and Solomon, Justin M. and Sun, Yuekai and Yurochkin, Mikhail},
	year         = 2021,
	booktitle    = {International {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {7850--7860},
	annote       = {This paper show the link between tresholded cost and UOT}
}
@article{gavish_multiscale_nodate,
	title        = {Multiscale {Wavelets} on {Trees}, {Graphs} and {High} {Dimensional} {Data}: {Theory} and {Applications} to {Semi} {Supervised} {Learning}},
	author       = {Gavish, Matan and Nadler, Boaz and Coifman, Ronald R},
	pages        = 8,
	abstract     = {Harmonic analysis, and in particular the relation between function smoothness and approximate sparsity of its wavelet coeﬃcients, has played a key role in signal processing and statistical inference for low dimensional data. In contrast, harmonic analysis has thus far had little impact in modern problems involving high dimensional data, or data encoded as graphs or networks. The main contribution of this paper is the development of a harmonic analysis approach, including both learning algorithms and supporting theory, applicable to these more general settings. Given data (be it high dimensional, graph or network) that is represented by one or more hierarchical trees, we ﬁrst construct multiscale wavelet-like orthonormal bases on it. Second, we prove that in analogy to the Euclidean case, function smoothness with respect to a speciﬁc metric induced by the tree is equivalent to exponential rate of coeﬃcient decay, that is, to approximate sparsity. These results readily translate to simple practical algorithms for various learning tasks. We present an application to transductive semisupervised learning.}
}

@article{chizat_unbalanced_2019,
	title        = {Unbalanced {Optimal} {Transport}: {Dynamic} and {Kantorovich} {Formulation}},
	shorttitle   = {Unbalanced {Optimal} {Transport}},
	author       = {Chizat, Lenaic and Peyré, Gabriel and Schmitzer, Bernhard and Vialard, François-Xavier},
	year         = 2019,
	journal      = {arXiv:1508.05216 [math]},
	abstract     = {This article presents a new class of distances between arbitrary nonnegative Radon measures inspired by optimal transport. These distances are defined by two equivalent alternative formulations: (i) a dynamic formulation defining the distance as a geodesic distance over the space of measures (ii) a static "Kantorovich" formulation where the distance is the minimum of an optimization problem over pairs of couplings describing the transfer (transport, creation and destruction) of mass between two measures. Both formulations are convex optimization problems, and the ability to switch from one to the other depending on the targeted application is a crucial property of our models. Of particular interest is the Wasserstein-Fisher-Rao metric recently introduced independently by Chizat et al. and Kondratyev et al. Defined initially through a dynamic formulation, it belongs to this class of metrics and hence automatically benefits from a static Kantorovich formulation.},
	keywords     = {Mathematics - Optimization and Control},
	annote       = {Comment: 37 pages, comments welcome}
}
@article{cheng_mean_1995,
	title        = {Mean shift, mode seeking, and clustering},
	author       = {Cheng, Yizong},
	year         = 1995,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 17,
	number       = 8,
	pages        = {790--799},
	abstract     = {Mean shift, a simple interactive procedure that shifts each data point to the average of data points in its neighborhood is generalized and analyzed in the paper. This generalization makes some k-means like clustering algorithms its special cases. It is shown that mean shift is a mode-seeking process on the surface constructed with a "shadow" kernal. For Gaussian kernels, mean shift is a gradient mapping. Convergence is studied for mean shift iterations. Cluster analysis if treated as a deterministic problem of finding a fixed point of mean shift that characterizes the data. Applications in clustering and Hough transform are demonstrated. Mean shift is also considered as an evolutionary strategy that performs multistart global optimization.{\textless}{\textgreater}},
	keywords     = {Algorithm design and analysis, Clustering algorithms, Computer science, Convergence, Iterative algorithms, Kernel, Surface treatment}
}
@article{wang_clues_2007,
	title        = {{CLUES}: {A} non-parametric clustering method based on local shrinking},
	shorttitle   = {{CLUES}},
	author       = {Wang, Xiaogang and Qiu, Weiliang and Zamar, Ruben H.},
	year         = 2007,
	journal      = {Computational Statistics \& Data Analysis},
	volume       = 52,
	number       = 1,
	pages        = {286--298},
	abstract     = {A novel non-parametric clustering method based on non-parametric local shrinking is proposed. Each data point is transformed in such a way that it moves a specific distance toward a cluster center. The direction and the associated size of each movement are determined by the median of its K-nearest neighbors. This process is repeated until a pre-defined convergence criterion is satisfied. The optimal value of the number of neighbors is determined by optimizing some commonly used index functions that measure the strengths of clusters generated by the algorithm. The number of clusters and the final partition are determined automatically without any input parameter except the stopping rule for convergence. Experiments on simulated and real data sets suggest that the proposed algorithm achieves relatively high accuracies when compared with classical clustering algorithms.},
	keywords     = {-nearest neighbors, Automatic clustering, Local shrinking, Number of clusters}
}
@article{torous_optimal_2021,
	title        = {An {Optimal} {Transport} {Approach} to {Causal} {Inference}},
	author       = {Torous, William and Gunsilius, Florian and Rigollet, Philippe},
	year         = 2021,
	journal      = {arXiv:2108.05858 [econ, math, stat]},
	abstract     = {We propose a method based on optimal transport theory for causal inference in classical treatment and control study designs. Our approach sheds a new light on existing approaches and generalizes them to settings with high-dimensional data. The implementation of our method leverages recent advances in computational optimal transport to produce an estimate of high-dimensional counterfactual outcomes. The benefits of this extension are demonstrated both on synthetic and real data that are beyond the reach of existing methods. In particular, we revisit the classical Card \& Krueger dataset on the effect of a minimum wage increase on employment in fast food restaurants and obtain new insights about the impact of raising the minimum wage on employment of full- and part-time workers in the fast food industry.},
	keywords     = {Economics - Econometrics, Mathematics - Statistics Theory, Statistics - Methodology}
}
@article{kerber_geometry_nodate,
	title        = {Geometry {Helps} to {Compare} {Persistence} {Diagrams}},
	author       = {Kerber, Michael and Morozov, Dmitriy and Nigmetov, Arnur},
	pages        = 10
}
@incollection{jorgenson_heat_2006,
	title        = {Heat kernels on weighted manifolds and applications},
	author       = {Grigor′yan, Alexander},
	year         = 2006,
	booktitle    = {Contemporary {Mathematics}},
	publisher    = {American Mathematical Society},
	address      = {Providence, Rhode Island},
	volume       = 398,
	pages        = {93--191},
	isbn         = {978-0-8218-3698-9 978-0-8218-7988-7},
	editor       = {Jorgenson, Jay and Walling, Lynne}
}
@article{andersson_operator-lipschitz_2015,
	title        = {Operator-{Lipschitz} estimates for the singular value functional calculus},
	author       = {Andersson, Fredrik and Carlsson, Marcus and Perfekt, Karl-Mikael},
	year         = 2015,
	journal      = {Proc. Amer. Math. Soc.},
	volume       = 144,
	number       = 5,
	pages        = {1867--1875},
	abstract     = {We consider a functional calculus for compact operators, acting on the singular values rather than the spectrum, which appears frequently in applied mathematics. Necessary and suﬃcient conditions for this singular value functional calculus to be Lipschitz-continuous with respect to the Hilbert-Schmidt norm are given. We also provide sharp constants.}
}
@misc{noauthor_geometry_nodate,
	title        = {geometry - {Bounds} on {Hausdorff} distance via singular values},
	journal      = {Mathematics Stack Exchange}
}
@article{bani_measuring_nodate,
	title        = {Measuring closeness of graphs   the {Hausdor}  distance},
	author       = {Bani, Iztok and Taranenko, Andrej},
	pages        = 26,
	abstract     = {We introduce an apparatus to measure closeness or relationship of two given objects. This is a topology based apparatus that uses graph representations of the compared objects. In more detail, we obtain a metric on the class of all pairwise non-isomorphic connected simple graphs to measure closeness of two such graphs. To obtain such a measure, we use the theory of hyperspaces from topology to introduce the notion of the Hausdor  graph 2G of any graph G. Then, using this new concept of Hausdor  graphs combined with the notion of graph amalgams, we present the Hausdor  distance, which proves to be useful when examining the closeness of any two connected simple graphs. We also present many possible applications of these concepts in various areas.}
}
@article{oles_efficient_2019,
	title        = {Efficient estimation of a {Gromov}--{Hausdorff} distance between unweighted graphs},
	author       = {Oles, Vladyslav and Lemons, Nathan and Panchenko, Alexander},
	year         = 2019,
	journal      = {arXiv:1909.09772 [cs, math]},
	abstract     = {Gromov-Hausdorff distances measure shape difference between the objects representable as compact metric spaces, e.g. point clouds, manifolds, or graphs. Computing any Gromov-Hausdorff distance is equivalent to solving an NP-Hard optimization problem, deeming the notion impractical for applications. In this paper we propose polynomial algorithm for estimating the so-called modified Gromov-Hausdorff (mGH) distance, whose topological equivalence with the standard Gromov-Hausdorff (GH) distance was established in {\textbackslash}cite\{memoli12\} (M{\textbackslash}'emoli, F, {\textbackslash}textit\{Discrete {\textbackslash}\& Computational Geometry, 48\}(2) 416-440, 2012). We implement the algorithm for the case of compact metric spaces induced by unweighted graphs as part of Python library {\textbackslash}verb{\textbar}scikit-tda{\textbar}, and demonstrate its performance on real-world and synthetic networks. The algorithm finds the mGH distances exactly on most graphs with the scale-free property. We use the computed mGH distances to successfully detect outliers in real-world social and computer networks.},
	keywords     = {Computer Science - Computational Geometry, Mathematics - Geometric Topology, Mathematics - Metric Geometry},
	annote       = {Comment: Fixed a type in the proof of Claim 3}
}
@article{choi_gromov-hausdor_nodate,
	title        = {Gromov-{Hausdorﬀ} {Distance} {Between} {Metric} {Graphs}},
	author       = {Choi, Jiwon},
	pages        = 10,
	abstract     = {In this paper we study the Gromov-Hausdorﬀ distance between two metric graphs. We compute the precise value of the Gromov-Hausdorﬀ distance between two path graphs. Moreover, we compute the precise value of the Gromov-Hausdorﬀ distance between a cycle graph and a tree. Given a graph X, we consider a graph Y that results from adding an edge to X without changing the number of vertices. We compute the precise value of the Gromov-Hausdorﬀ distance between X and Y .}
}
@article{memoli_properties_2012,
	title        = {Some {Properties} of {Gromov}–{Hausdorff} {Distances}},
	author       = {Mémoli, Facundo},
	year         = 2012,
	journal      = {Discrete Comput Geom},
	volume       = 48,
	number       = 2,
	pages        = {416--440},
	abstract     = {The Gromov–Hausdorff distance between metric spaces appears to be a useful tool for modeling some object matching procedures. Since its conception it has been mainly used by pure mathematicians who are interested in the topology generated by this distance, and quantitative consequences of the deﬁnition are not very common. As a result, only few lower bounds for the distance are known, and the stability of many metric invariants is not understood. This paper aims at clarifying some of these points by proving several results dealing with explicit lower bounds for the Gromov–Hausdorff distance which involve different standard metric invariants. We also study a modiﬁed version of the Gromov–Hausdorff distance which is motivated by practical applications and both prove a structural theorem for it and study its topological equivalence to the usual notion. This structural theorem provides a decomposition of the modiﬁed Gromov–Hausdorff distance as the supremum over a family of pseudo-metrics, each of which involves the comparison of certain discrete analogues of curvature. This modiﬁed version relates the standard Gromov–Hausdorff distance to the work of Boutin and Kemper, and Olver.}
}
@article{edelsbrunner_herbert_2012,
	title        = {Herbert {Edelsbrunner}},
	author       = {Edelsbrunner, Herbert},
	year         = 2012,
	journal      = {Wiad. Mat.},
	volume       = 48,
	number       = 2,
	pages        = 47,
	abstract     = {The persistence diagram of a real-valued function on a topological space is a multiset of points in the extended plane. We prove that under mild assumptions on the function, the persistence diagram is stable: small changes in the function imply only small changes in the diagram. We apply this result to estimating the homology of sets in a metric space and to comparing and classifying geometric shapes.}
}
@article{solomon_convolutional_nodate,
	title        = {Convolutional {Wasserstein} {Distances}: {Efﬁcient} {Optimal} {Transportation} on {Geometric} {Domains}},
	author       = {Solomon, Justin and de Goes, Fernando and Peyre, Gabriel and Paris-Dauphine, Univ and Cuturi, Marco},
	pages        = 11,
	abstract     = {This paper introduces a new class of algorithms for optimization problems involving optimal transportation over geometric domains. Our main contribution is to show that optimal transportation can be made tractable over large domains used in graphics, such as images and triangle meshes, improving performance by orders of magnitude compared to previous work. To this end, we approximate optimal transportation distances using entropic regularization. The resulting objective contains a geodesic distance-based kernel that can be approximated with the heat kernel. This approach leads to simple iterative numerical schemes with linear convergence, in which each iteration only requires Gaussian convolution or the solution of a sparse, pre-factored linear system. We demonstrate the versatility and efﬁciency of our method on tasks including reﬂectance interpolation, color transfer, and geometry processing.}
}
@article{singer_spectral_2017,
	title        = {Spectral convergence of the connection {Laplacian} from random samples},
	author       = {Singer, Amit and Wu, Hau-Tieng},
	year         = 2017,
	journal      = {Information and Inference: A Journal of the IMA},
	volume       = 6,
	number       = 1,
	pages        = {58--123},
	abstract     = {Spectral methods that are based on eigenvectors and eigenvalues of discrete graph Laplacians, such as Diffusion Maps and Laplacian Eigenmaps, are often used for manifold learning and nonlinear dimensionality reduction. It was previously shown by Belkin \&amp; Niyogi (2007, Convergence of Laplacian eigenmaps, vol. 19. Proceedings of the 2006 Conference on Advances in Neural Information Processing Systems. The MIT Press, p. 129.) that the eigenvectors and eigenvalues of the graph Laplacian converge to the eigenfunctions and eigenvalues of the Laplace–Beltrami operator of the manifold in the limit of infinitely many data points sampled independently from the uniform distribution over the manifold. Recently, we introduced Vector Diffusion Maps and showed that the connection Laplacian of the tangent bundle of the manifold can be approximated from random samples. In this article, we present a unified framework for approximating other connection Laplacians over the manifold by considering its principle bundle structure. We prove that the eigenvectors and eigenvalues of these Laplacians converge in the limit of infinitely many independent random samples. We generalize the spectral convergence results to the case where the data points are sampled from a non-uniform distribution, and for manifolds with and without boundary.}
}
@incollection{heinonen_lipschitz_2001,
	title        = {Lipschitz {Functions}},
	author       = {Heinonen, Juha},
	year         = 2001,
	booktitle    = {Lectures on {Analysis} on {Metric} {Spaces}},
	publisher    = {Springer},
	address      = {New York, NY},
	series       = {Universitext},
	pages        = {43--48},
	isbn         = {978-1-4613-0131-8},
	abstract     = {Lipschitz functions are the smooth functions of metric spaces. A real-valued function f on a metric space X is said to be L-Lipschitz if there is a constant L ≥ 1 such that {\textbar}��(��)−��(��){\textbar}⩽��{\textbar}��−��{\textbar}{\textbar}f(x)−f(y){\textbar}⩽L{\textbar}x−y{\textbar} {\textbackslash}left{\textbar} \{f(x) - f(y)\} {\textbackslash}right{\textbar} {\textbackslash}leqslant L{\textbackslash}left{\textbar} \{x - y\} {\textbackslash}right{\textbar} (6.1) for all x and y in X. Of course, there is nothing special about having the real line as a target, and in general we call a map f : X → Y between metric spaces Lipschitz, or L-Lipschitz if the constant L ≥ 1 deserves to be mentioned, if condition (6.1) holds.},
	editor       = {Heinonen, Juha},
	keywords     = {Lebesgue Point, Lipschitz Function, Sobolev Embedding Theorem, Sobolev Space, Uniform Limit}
}
@article{steinerberger_kantorovich-rubinstein_2020,
	title        = {On a {Kantorovich}-{Rubinstein} inequality},
	author       = {Steinerberger, Stefan},
	year         = 2020,
	journal      = {arXiv:2010.12946 [math]},
	abstract     = {An easy consequence of Kantorovich-Rubinstein duality is the following: if \$f:[0,1]{\textasciicircum}d {\textbackslash}rightarrow {\textbackslash}infty\$ is Lipschitz and \${\textbackslash}left{\textbackslash}\{x\_1, {\textbackslash}dots, x\_N {\textbackslash}right{\textbackslash}\} {\textbackslash}subset [0,1]{\textasciicircum}d\$, then \$\$ {\textbackslash}left{\textbar} {\textbackslash}int\_\{[0,1]{\textasciicircum}d\} f(x) dx - {\textbackslash}frac\{1\}\{N\} {\textbackslash}sum\_\{k=1\}{\textasciicircum}\{N\}\{f(x\_k)\} {\textbackslash}right{\textbar} {\textbackslash}leq {\textbackslash}left{\textbackslash}{\textbar} {\textbackslash}nabla f {\textbackslash}right{\textbackslash}{\textbar}\_\{L{\textasciicircum}\{{\textbackslash}infty\}\} {\textbackslash}cdot W\_1{\textbackslash}left( {\textbackslash}frac\{1\}\{N\} {\textbackslash}sum\_\{k=1\}{\textasciicircum}\{N\}\{{\textbackslash}delta\_\{x\_k\}\} , dx{\textbackslash}right),\$\$ where \$W\_1\$ denotes the \$1-\$Wasserstein (or Earth Mover's) Distance. We prove another such inequality with a smaller norm on \${\textbackslash}nabla f\$ and a larger Wasserstein distance. Our inequality is sharp when the points are very regular, i.e. \$W\_\{{\textbackslash}infty\} {\textbackslash}sim N{\textasciicircum}\{-1/d\}\$. This prompts the question whether these two inequalities are specific instances of an entire underlying family of estimates capturing a duality between transport distance and function space.},
	keywords     = {Mathematics - Functional Analysis, Mathematics - Probability}
}
@incollection{gesztesy_spaces_2016,
	title        = {Spaces of {Lipschitz} and {Hölder} functions and their applications},
	author       = {Gesztesy, Fritz and Godefroy, Gilles and Grafakos, Loukas and Verbitsky, Igor},
	year         = 2016,
	booktitle    = {Nigel {J}. {Kalton} {Selecta}},
	publisher    = {Springer International Publishing},
	address      = {Cham},
	pages        = {325--376},
	isbn         = {978-3-319-18798-3 978-3-319-18799-0},
	abstract     = {We study the structure of Lipschitz and Ho¨lder-type spaces and their preduals on general metric spaces, and give applications to the uniform structure of Banach spaces. In particular we resolve a problem of Weaver who asks whether if M is a compact metric space and 0 {\textless} α {\textless} 1, it is always true the space of Ho¨lder continuous functions of class α is isomorphic to ∞. We show that, on the contrary, if M is a compact convex subset of a Hilbert space this isomorphism holds if and only if M is ﬁnite-dimensional. We also study the (related) problem of when a quotient map Q:Y → X between two Banach spaces admits a section which is uniformly continuous on the unit ball of X.},
	editor       = {Gesztesy, Fritz and Godefroy, Gilles and Grafakos, Loukas and Verbitsky, Igor}
}
@article{belkin_convergence_nodate,
	title        = {Convergence of {Laplacian} {Eigenmaps}},
	author       = {Belkin, Mikhail and Niyogi, Partha},
	pages        = 8,
	abstract     = {Geometrically based methods for various tasks of machine learning have attracted considerable attention over the last few years. In this paper we show convergence of eigenvectors of the point cloud Laplacian to the eigenfunctions of the Laplace-Beltrami operator on the underlying manifold, thus establishing the ﬁrst convergence results for a spectral dimensionality reduction algorithm in the manifold setting.}
}
@article{cai_eigenvalue_2020,
	title        = {Eigenvalue {Problems} for {Exponential}-{Type} {Kernels}},
	author       = {Cai, Difeng and Vassilevski, Panayot S.},
	year         = 2020,
	journal      = {Computational Methods in Applied Mathematics},
	volume       = 20,
	number       = 1,
	pages        = {61--78},
	abstract     = {{\textless}section class="abstract"{\textgreater}{\textless}h2 class="abstractTitle text-title my-1" id="d513e2"{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}We study approximations of eigenvalue problems for integral operators associated with kernel functions of exponential type. We show convergence rate {\textless}inline-formula xmlns:ifp="http://www.ifactory.com/press" id="j\_cmam-2018-0186\_ineq\_9999\_w2aab3b7e2028b1b6b1aab1c14b1b1Aa"{\textgreater}{\textless}alternatives{\textgreater}{\textless}math overflow="scroll"{\textgreater}{\textless}mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}-{\textless}/mo{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msub{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater}≤{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}C{\textless}/mi{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}mo{\textgreater}⁢{\textless}/mo{\textgreater}{\textless}msup{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msup{\textgreater}{\textless}/mrow{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} in the case of lowest order approximation for both Galerkin and Nyström methods, where {\textless}em{\textgreater}h{\textless}/em{\textgreater} is the mesh size, {\textless}inline-formula xmlns:ifp="http://www.ifactory.com/press" id="j\_cmam-2018-0186\_ineq\_9998\_w2aab3b7e2028b1b6b1aab1c14b1b5Aa"{\textgreater}{\textless}alternatives{\textgreater}{\textless}math overflow="scroll"{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}/math{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} and {\textless}inline-formula xmlns:ifp="http://www.ifactory.com/press" id="j\_cmam-2018-0186\_ineq\_9997\_w2aab3b7e2028b1b6b1aab1c14b1b7Aa"{\textgreater}{\textless}alternatives{\textgreater}{\textless}math overflow="scroll"{\textgreater}{\textless}msub{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msub{\textgreater}{\textless}/math{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} are the exact and approximate {\textless}em{\textgreater}k{\textless}/em{\textgreater}th largest eigenvalues, respectively. We prove that the two methods are numerically equivalent in the sense that {\textless}inline-formula xmlns:ifp="http://www.ifactory.com/press" id="j\_cmam-2018-0186\_ineq\_9996\_w2aab3b7e2028b1b6b1aab1c14b1c11Aa"{\textgreater}{\textless}alternatives{\textgreater}{\textless}math overflow="scroll"{\textgreater}{\textless}mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}msubsup{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}G{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msubsup{\textgreater}{\textless}mo{\textgreater}-{\textless}/mo{\textgreater}{\textless}msubsup{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}N{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msubsup{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater}{\textbar}{\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}mo{\textgreater}≤{\textless}/mo{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}C{\textless}/mi{\textgreater}{\textless}mo{\textgreater}⁢{\textless}/mo{\textgreater}{\textless}msup{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}mn{\textgreater}2{\textless}/mn{\textgreater}{\textless}/msup{\textgreater}{\textless}/mrow{\textgreater}{\textless}/mrow{\textgreater}{\textless}/math{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater}, where {\textless}inline-formula xmlns:ifp="http://www.ifactory.com/press" id="j\_cmam-2018-0186\_ineq\_9995\_w2aab3b7e2028b1b6b1aab1c14b1c13Aa"{\textgreater}{\textless}alternatives{\textgreater}{\textless}math overflow="scroll"{\textgreater}{\textless}msubsup{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}G{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msubsup{\textgreater}{\textless}/math{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} and {\textless}inline-formula xmlns:ifp="http://www.ifactory.com/press" id="j\_cmam-2018-0186\_ineq\_9994\_w2aab3b7e2028b1b6b1aab1c14b1c15Aa"{\textgreater}{\textless}alternatives{\textgreater}{\textless}math overflow="scroll"{\textgreater}{\textless}msubsup{\textgreater}{\textless}mi{\textgreater}λ{\textless}/mi{\textgreater}{\textless}mrow{\textgreater}{\textless}mi{\textgreater}k{\textless}/mi{\textgreater}{\textless}mo{\textgreater},{\textless}/mo{\textgreater}{\textless}mi{\textgreater}h{\textless}/mi{\textgreater}{\textless}/mrow{\textgreater}{\textless}mrow{\textgreater}{\textless}mo{\textgreater}({\textless}/mo{\textgreater}{\textless}mi{\textgreater}N{\textless}/mi{\textgreater}{\textless}mo{\textgreater}){\textless}/mo{\textgreater}{\textless}/mrow{\textgreater}{\textless}/msubsup{\textgreater}{\textless}/math{\textgreater}{\textless}/alternatives{\textgreater}{\textless}/inline-formula{\textgreater} denote the {\textless}em{\textgreater}k{\textless}/em{\textgreater}th largest eigenvalues computed by Galerkin and Nyström methods, respectively, and {\textless}em{\textgreater}C{\textless}/em{\textgreater} is a eigenvalue independent constant. The theoretical results are accompanied by a series of numerical experiments.{\textless}/p{\textgreater}{\textless}/section{\textgreater}}
}
@article{berard_embedding_1994,
	title        = {Embedding {Riemannian} manifolds},
	author       = {BERARD, P.},
	year         = 1994,
	journal      = {their heat kernel, Geom. Funct. Anal.},
	volume       = 4,
	number       = 4,
	pages        = {373--398}
}
@article{fukaya_collapsing_1987,
	title        = {Collapsing of {Riemannian} manifolds and eigenvalues of {Laplace} operator},
	author       = {Fukaya, Kenji},
	year         = 1987,
	journal      = {Invent Math},
	volume       = 87,
	number       = 3,
	pages        = {517--547}
}
@article{leeb_holderlipschitz_2016,
	title        = {Hölder–{Lipschitz} {Norms} and {Their} {Duals} on {Spaces} with {Semigroups}, with {Applications} to {Earth} {Mover}’s {Distance}},
	author       = {Leeb, William and Coifman, Ronald},
	year         = 2016,
	journal      = {J Fourier Anal Appl},
	volume       = 22,
	number       = 4,
	pages        = {910--953},
	abstract     = {We introduce a family of bounded, multiscale distances on any space equipped with an operator semigroup. In many examples, these distances are equivalent to a snowﬂake of the natural distance on the space. Under weak regularity assumptions on the kernels deﬁning the semigroup, we derive simple characterizations of the Hölder–Lipschitz norm and its dual with respect to these distances. As the dual norm of the difference of two probability measures is the Earth Mover’s Distance (EMD) between these measures, our characterizations give simple formulas for a metric equivalent to EMD. We extend these results to the mixed Hölder–Lipschitz norm and its dual on the product of spaces, each of which is equipped with its own semigroup. Additionally, we derive an approximation theorem for mixed Lipschitz functions in this setting.}
}
@misc{noauthor_laplacian_eigmapspdf_nodate,
	title        = {Laplacian\_eigmaps.pdf},
	journal      = {Google Docs}
}
@article{you_design_2021,
	title        = {Design {Space} for {Graph} {Neural} {Networks}},
	author       = {You, Jiaxuan and Ying, Rex and Leskovec, Jure},
	year         = 2021,
	journal      = {arXiv:2011.08843 [cs]},
	abstract     = {The rapid evolution of Graph Neural Networks (GNNs) has led to a growing number of new architectures as well as novel applications. However, current research focuses on proposing and evaluating specific architectural designs of GNNs, as opposed to studying the more general design space of GNNs that consists of a Cartesian product of different design dimensions, such as the number of layers or the type of the aggregation function. Additionally, GNN designs are often specialized to a single task, yet few efforts have been made to understand how to quickly find the best GNN design for a novel task or a novel dataset. Here we define and systematically study the architectural design space for GNNs which consists of 315,000 different designs over 32 different predictive tasks. Our approach features three key innovations: (1) A general GNN design space; (2) a GNN task space with a similarity metric, so that for a given novel task/dataset, we can quickly identify/transfer the best performing architecture; (3) an efficient and effective design space evaluation method which allows insights to be distilled from a huge number of model-task combinations. Our key results include: (1) A comprehensive set of guidelines for designing well-performing GNNs; (2) while best GNN designs for different tasks vary significantly, the GNN task space allows for transferring the best designs across different tasks; (3) models discovered using our design space achieve state-of-the-art performance. Overall, our work offers a principled and scalable approach to transition from studying individual GNN designs for specific tasks, to systematically studying the GNN design space and the task space. Finally, we release GraphGym, a powerful platform for exploring different GNN designs and tasks. GraphGym features modularized GNN implementation, standardized GNN evaluation, and reproducible and scalable experiment management.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks},
	annote       = {Comment: NeurIPS 2020 (Spotlight). Typos fixed}
}
@article{sinha_certifying_2020,
	title        = {Certifying {Some} {Distributional} {Robustness} with {Principled} {Adversarial} {Training}},
	author       = {Sinha, Aman and Namkoong, Hongseok and Volpi, Riccardo and Duchi, John},
	year         = 2020,
	journal      = {arXiv:1710.10571 [cs, stat]},
	abstract     = {Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations. By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: ICLR 2018: https://openreview.net/forum?id=Hk6kPgZA-}
}
@article{rezende_implicit_2021,
	title        = {Implicit {Riemannian} {Concave} {Potential} {Maps}},
	author       = {Rezende, Danilo J. and Racanière, Sébastien},
	year         = 2021,
	journal      = {arXiv:2110.01288 [cs, stat]},
	abstract     = {We are interested in the challenging problem of modelling densities on Riemannian manifolds with a known symmetry group using normalising flows. This has many potential applications in physical sciences such as molecular dynamics and quantum simulations. In this work we combine ideas from implicit neural layers and optimal transport theory to propose a generalisation of existing work on exponential map flows, Implicit Riemannian Concave Potential Maps, IRCPMs. IRCPMs have some nice properties such as simplicity of incorporating symmetries and are less expensive than ODE-flows. We provide an initial theoretical analysis of its properties and layout sufficient conditions for stable optimisation. Finally, we illustrate the properties of IRCPMs with density estimation experiments on tori and spheres.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
@misc{noauthor_untitled_nodate,
	title        = {Untitled document},
	journal      = {Google Docs},
	abstract     = {CRSNG brouillon et TODOs  Sites importants :  Formulaire en ligne  Instruction  Notes et instructions fournies par ESP  Instruction présentation des documents Critère de selection et autres information Précision bourse ES D   TODOs “administratif/rapide” TROUVER UN TITRE DE PROPOSITION  Autour de...}
}
@article{kreuzer_rethinking_2021,
	title        = {Rethinking {Graph} {Transformers} with {Spectral} {Attention}},
	author       = {Kreuzer, Devin and Beaini, Dominique and Hamilton, William L. and Létourneau, Vincent and Tossou, Prudencio},
	year         = 2021,
	journal      = {arXiv:2106.03893 [cs]},
	abstract     = {In recent years, the Transformer architecture has proven to be very successful in sequence processing, but its application to other data structures, such as graphs, has remained limited due to the difficulty of properly defining positions. Here, we present the \${\textbackslash}textit\{Spectral Attention Network\}\$ (SAN), which uses a learned positional encoding (LPE) that can take advantage of the full Laplacian spectrum to learn the position of each node in a given graph. This LPE is then added to the node features of the graph and passed to a fully-connected Transformer. By leveraging the full spectrum of the Laplacian, our model is theoretically powerful in distinguishing graphs, and can better detect similar sub-structures from their resonance. Further, by fully connecting the graph, the Transformer does not suffer from over-squashing, an information bottleneck of most GNNs, and enables better modeling of physical phenomenons such as heat transfer and electric interaction. When tested empirically on a set of 4 standard datasets, our model performs on par or better than state-of-the-art GNNs, and outperforms any attention-based model by a wide margin, becoming the first fully-connected architecture to perform well on graph benchmarks.},
	keywords     = {Computer Science - Machine Learning, talk vincent letourneau},
	annote       = {transformer {\textasciitilde} complete graph where the messages are the edges}
}
@article{beaini_directional_2021,
	title        = {Directional {Graph} {Networks}},
	author       = {Beaini, Dominique and Passaro, Saro and Létourneau, Vincent and Hamilton, William L. and Corso, Gabriele and Liò, Pietro},
	year         = 2021,
	journal      = {arXiv:2010.02863 [cs]},
	abstract     = {The lack of anisotropic kernels in graph neural networks (GNNs) strongly limits their expressiveness, contributing to well-known issues such as over-smoothing. To overcome this limitation, we propose the first globally consistent anisotropic kernels for GNNs, allowing for graph convolutions that are defined according to topologicaly-derived directional flows. First, by defining a vector field in the graph, we develop a method of applying directional derivatives and smoothing by projecting node-specific messages into the field. Then, we propose the use of the Laplacian eigenvectors as such vector field. We show that the method generalizes CNNs on an \$n\$-dimensional grid and is provably more discriminative than standard GNNs regarding the Weisfeiler-Lehman 1-WL test. We evaluate our method on different standard benchmarks and see a relative error reduction of 8\% on the CIFAR10 graph dataset and 11\% to 32\% on the molecular ZINC dataset, and a relative increase in precision of 1.6\% on the MolPCBA dataset. An important outcome of this work is that it enables graph networks to embed directions in an unsupervised way, thus allowing a better representation of the anisotropic features in different physical or biological problems.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Social and Information Networks, Computer Science - Computational Geometry, talk vincent letourneau},
	annote       = {Comment: 11 pages, 10 pages appendix, 6 figures, subtitle: Anisotropic aggregation in graph neural networks via directional vector fields},
	annote       = {
		they use the eigenvector of the laplacian to define a vector field. then they are able to add directions on the graph ? or porject on this field (something like that) similar to CNN where you have to vector for vertical and two for horizontal. generalize CNN if you do it on a grid graph. and same idea for any graph weighted adjacency matrix by these eigenvector and taking the gradient the MLP include the k first eigenvectore and their difference (i think)

		their goal was to use direction to define anisotropic aggregation. (isotropic aggregation makes it harder to differentiate between graph)
	}
}
@article{corso_principal_2020,
	title        = {Principal {Neighbourhood} {Aggregation} for {Graph} {Nets}},
	author       = {Corso, Gabriele and Cavalleri, Luca and Beaini, Dominique and Liò, Pietro and Veličković, Petar},
	year         = 2020,
	journal      = {arXiv:2004.05718 [cs, stat]},
	abstract     = {Graph Neural Networks (GNNs) have been shown to be effective models for different predictive tasks on graph-structured data. Recent work on their expressive power has focused on isomorphism tasks and countable feature spaces. We extend this theoretical framework to include continuous features - which occur regularly in real-world input domains and within the hidden layers of GNNs - and we demonstrate the requirement for multiple aggregation functions in this context. Accordingly, we propose Principal Neighbourhood Aggregation (PNA), a novel architecture combining multiple aggregators with degree-scalers (which generalize the sum aggregator). Finally, we compare the capacity of different models to capture and exploit the graph structure via a novel benchmark containing multiple tasks taken from classical graph theory, alongside existing benchmarks from real-world domains, all of which demonstrate the strength of our model. With this work, we hope to steer some of the GNN research towards new aggregation methods which we believe are essential in the search for powerful and robust models.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, talk vincent letourneau, Computer Science - Computer Vision and Pattern Recognition},
	annote       = {Comment: 34th Conference on Neural Information Processing Systems (NeurIPS 2020)}
}
@article{wang_exact_2015,
	title        = {Exact {Optimal} {Confidence} {Intervals} for {Hypergeometric} {Parameters}},
	author       = {Wang, Weizhen},
	year         = 2015,
	journal      = {Journal of the American Statistical Association},
	volume       = 110,
	number       = 512,
	pages        = {1491--1499}
}
@article{wang_construction_2010,
	title        = {On construction of the smallest one-sided confidence interval for the difference of two proportions},
	author       = {Wang, Weizhen},
	year         = 2010,
	journal      = {The Annals of Statistics},
	volume       = 38,
	number       = 2,
	pages        = {1227--1243},
	abstract     = {For any class of one-sided 1−α confidence intervals with a certain monotonicity ordering on the random confidence limit, the smallest interval, in the sense of the set inclusion for the difference of two proportions of two independent binomial random variables, is constructed based on a direct analysis of coverage probability function. A special ordering on the confidence limit is developed and the corresponding smallest confidence interval is derived. This interval is then applied to identify the minimum effective dose (MED) for binary data in dose-response studies, and a multiple test procedure that controls the familywise error rate at level α is obtained. A generalization of constructing the smallest one-sided confidence interval to other discrete sample spaces is discussed in the presence of nuisance parameters.},
	keywords     = {62F25, 62J15, 62P10, Binomial distribution, coverage probability, minimum effective dose, multiple tests, Poisson distribution, set inclusion}
}
@article{dwivedi_benchmarking_2020,
	title        = {Benchmarking {Graph} {Neural} {Networks}},
	author       = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K. and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
	year         = 2020,
	journal      = {arXiv:2003.00982 [cs, stat]},
	abstract     = {Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the ﬁeld grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difﬁcult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework2, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classiﬁcation and node/link prediction, with medium-scale datasets.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Benchmarking framework on GitHub at https://github.com/graphdeeplearning/benchmarking-gnns}
}
@article{dwivedi_generalization_2021,
	title        = {A {Generalization} of {Transformer} {Networks} to {Graphs}},
	author       = {Dwivedi, Vijay Prakash and Bresson, Xavier},
	year         = 2021,
	journal      = {arXiv:2012.09699 [cs]},
	abstract     = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.},
	keywords     = {Computer Science - Machine Learning},
	annote       = {Comment: AAAI 2021 Workshop on Deep Learning on Graphs: Methods and Applications (DLG-AAAI 2021); Code at https://github.com/graphdeeplearning/graphtransformer}
}
@misc{noauthor_transformers_2020,
	title        = {Transformers are {Graph} {Neural} {Networks}},
	year         = 2020,
	journal      = {The Gradient},
	abstract     = {My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications?  While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba and Twitter, a more subtle success story is the Transformer architecture, which has taken the NLP world by storm. Through}
}
@article{zaheer_deep_2018,
	title        = {Deep {Sets}},
	author       = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Ruslan and Smola, Alexander},
	year         = 2018,
	journal      = {arXiv:1703.06114 [cs, stat]},
	abstract     = {We study the problem of designing models for machine learning tasks defined on {\textbackslash}emph\{sets\}. In contrast to traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets that are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics {\textbackslash}cite\{poczos13aistats\}, to anomaly detection in piezometer data of embankment dams {\textbackslash}cite\{Jung15Exploration\}, to cosmology {\textbackslash}cite\{Ntampaka16Dynamical,Ravanbakhsh16ICML1\}. Our main theorem characterizes the permutation invariant functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We also derive the necessary and sufficient conditions for permutation equivariance in deep models. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: NIPS 2017}
}
@article{hamilton_graph_2020,
	title        = {Graph {Representation} {Learning}},
	author       = {Hamilton, William L.},
	year         = 2020,
	journal      = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
	volume       = 14,
	number       = 3,
	pages        = {1--159},
	abstract     = {Graph-structured data is ubiquitous throughout the natural and social sciences, from telecommunication networks to quantum chemistry. Building relational inductive biases into deep learning architectures is crucial if we want systems that can learn, reason, and generalize from this kind of data. Recent years have seen a surge in research on graph representation learning, including techniques for deep graph embeddings, generalizations of convolutional neural networks to graph-structured data, and neural message-passing approaches inspired by belief propagation. These advances in graph representation learning have led to new state-of-the-art results in numerous domains, including chemical synthesis, 3D-vision, recommender systems, question answering, and social network analysis.}
}
@article{yan_neural_2020,
	title        = {Neural {Execution} {Engines}: {Learning} to {Execute} {Subroutines}},
	shorttitle   = {Neural {Execution} {Engines}},
	author       = {Yan, Yujun and Swersky, Kevin and Koutra, Danai and Ranganathan, Parthasarathy and Hashemi, Milad},
	year         = 2020,
	journal      = {arXiv:2006.08084 [cs, stat]},
	abstract     = {A significant effort has been made to train neural networks that replicate algorithmic reasoning, but they often fail to learn the abstract concepts underlying these algorithms. This is evidenced by their inability to generalize to data distributions that are outside of their restricted training sets, namely larger inputs and unseen data. We study these generalization issues at the level of numerical subroutines that comprise common algorithms like sorting, shortest paths, and minimum spanning trees. First, we observe that transformer-based sequence-to-sequence models can learn subroutines like sorting a list of numbers, but their performance rapidly degrades as the length of lists grows beyond those found in the training set. We demonstrate that this is due to attention weights that lose fidelity with longer sequences, particularly when the input numbers are numerically similar. To address the issue, we propose a learned conditional masking mechanism, which enables the model to strongly generalize far outside of its training range with near-perfect accuracy on a variety of algorithms. Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication. This allows the embedding to handle missing data by faithfully interpolating numbers not seen during training.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Programming Languages},
	annote       = {Comment: Accepted at 34th Conference on Neural Information Processing Systems (NeurIPS 2020)}
}
@inproceedings{xu_what_2019,
	title        = {What {Can} {Neural} {Networks} {Reason} {About}?},
	author       = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
	year         = 2019,
	abstract     = {We develop a theoretical framework to characterize what a neural network can learn to reason about.}
}
@article{velickovic_pointer_2020,
	title        = {Pointer {Graph} {Networks}},
	author       = {Veličković, Petar and Buesing, Lars and Overlan, Matthew C. and Pascanu, Razvan and Vinyals, Oriol and Blundell, Charles},
	year         = 2020,
	journal      = {arXiv:2006.06380 [cs, stat]},
	abstract     = {Graph neural networks (GNNs) are typically applied to static graphs that are assumed to be known upfront. This static input structure is often informed purely by insight of the machine learning practitioner, and might not be optimal for the actual task the GNN is solving. In absence of reliable domain expertise, one might resort to inferring the latent graph structure, which is often difficult due to the vast search space of possible graphs. Here we introduce Pointer Graph Networks (PGNs) which augment sets or graphs with additional inferred edges for improved model generalisation ability. PGNs allow each node to dynamically point to another node, followed by message passing over these pointers. The sparsity of this adaptable graph structure makes learning tractable while still being sufficiently expressive to simulate complex algorithms. Critically, the pointing mechanism is directly supervised to model long-term sequences of operations on classical data structures, incorporating useful structural inductive biases from theoretical computer science. Qualitatively, we demonstrate that PGNs can learn parallelisable variants of pointer-based data structures, namely disjoint set unions and link/cut trees. PGNs generalise out-of-distribution to 5x larger test inputs on dynamic graph connectivity tasks, outperforming unrestricted GNNs and Deep Sets.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms},
	annote       = {Comment: To appear at NeurIPS 2020 (Spotlight talk)}
}
@inproceedings{velickovic_neural_2019,
	title        = {Neural {Execution} of {Graph} {Algorithms}},
	author       = {Veličković, Petar and Ying, Rex and Padovano, Matilde and Hadsell, Raia and Blundell, Charles},
	year         = 2019,
	abstract     = {We supervise graph neural networks to imitate intermediate and step-wise outputs of classical graph algorithms, recovering highly favourable insights.}
}
@article{velickovic_neural_2021,
	title        = {Neural {Algorithmic} {Reasoning}},
	author       = {Veličković, Petar and Blundell, Charles},
	year         = 2021,
	journal      = {Patterns},
	volume       = 2,
	number       = 7,
	pages        = 100273,
	abstract     = {Algorithms have been fundamental to recent global technological advances and, in particular, they have been the cornerstone of technical advances in one field rapidly being applied to another. We argue that algorithms possess fundamentally different qualities to deep learning methods, and this strongly suggests that, were deep learning methods better able to mimic algorithms, generalisation of the sort seen with algorithms would become possible with deep learning -- something far out of the reach of current machine learning methods. Furthermore, by representing elements in a continuous space of learnt algorithms, neural networks are able to adapt known algorithms more closely to real-world problems, potentially finding more efficient and pragmatic solutions than those proposed by human computer scientists. Here we present neural algorithmic reasoning -- the art of building neural networks that are able to execute algorithmic computation -- and provide our opinion on its transformative potential for running classical algorithms on inputs previously considered inaccessible to them.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control, Computer Science - Data Structures and Algorithms},
	annote       = {Comment: Accepted as an Opinion paper in Patterns. 7 pages, 1 figure}
}
@article{velickovic_graph_2018,
	title        = {Graph {Attention} {Networks}},
	author       = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	year         = 2018,
	journal      = {arXiv:1710.10903 [cs, stat]},
	abstract     = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Social and Information Networks, Statistics - Machine Learning},
	annote       = {Comment: To appear at ICLR 2018. 12 pages, 2 figures}
}
@inproceedings{sandryhaila_discrete_2013,
	title        = {Discrete signal processing on graphs: {Graph} fourier transform},
	shorttitle   = {Discrete signal processing on graphs},
	author       = {Sandryhaila, Aliaksei and Moura, José M. F.},
	year         = 2013,
	booktitle    = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	pages        = {6167--6170},
	abstract     = {We propose a novel discrete signal processing framework for the representation and analysis of datasets with complex structure. Such datasets arise in many social, economic, biological, and physical networks. Our framework extends traditional discrete signal processing theory to structured datasets by viewing them as signals represented by graphs, so that signal coefficients are indexed by graph nodes and relations between them are represented by weighted graph edges. We discuss the notions of signals and filters on graphs, and define the concepts of the spectrum and Fourier transform for graph signals. We demonstrate their relation to the generalized eigenvector basis of the graph adjacency matrix and study their properties. As a potential application of the graph Fourier transform, we consider the efficient representation of structured data that utilizes the sparseness of graph signals in the frequency domain.},
	keywords     = {Digital signal processing, Discrete Fourier transforms, generalized eigenvectors, graph filter, graph Fourier transform, graph signal, Graph signal processing, graph spectrum, Image edge detection, Matrix decomposition, sparse representation, Time series analysis}
}
@article{sandryhaila_discrete_2013-1,
	title        = {Discrete {Signal} {Processing} on {Graphs}},
	author       = {Sandryhaila, Aliaksei and Moura, José M. F.},
	year         = 2013,
	journal      = {IEEE Transactions on Signal Processing},
	volume       = 61,
	number       = 7,
	pages        = {1644--1656},
	abstract     = {In social settings, individuals interact through webs of relationships. Each individual is a node in a complex network (or graph) of interdependencies and generates data, lots of data. We label the data by its source, or formally stated, we index the data by the nodes of the graph. The resulting signals (data indexed by the nodes) are far removed from time or image signals indexed by well ordered time samples or pixels. DSP, discrete signal processing, provides a comprehensive, elegant, and efficient methodology to describe, represent, transform, analyze, process, or synthesize these well ordered time or image signals. This paper extends to signals on graphs DSP and its basic tenets, including filters, convolution, z-transform, impulse response, spectral representation, Fourier transform, frequency response, and illustrates DSP on graphs by classifying blogs, linear predicting and compressing data from irregularly located weather stations, or predicting behavior of customers of a mobile service provider.},
	keywords     = {Digital signal processing, Fourier transforms, Graph Fourier transform, graphical models, Graphical models, Laplace equations, Manifolds, Markov random fields, network science, signal processing}
}
@article{sandryhaila_discrete_2013-2,
	title        = {Discrete {Signal} {Processing} on {Graphs}},
	author       = {Sandryhaila, Aliaksei and Moura, José M. F.},
	year         = 2013,
	journal      = {IEEE Transactions on Signal Processing},
	volume       = 61,
	number       = 7,
	pages        = {1644--1656},
	abstract     = {In social settings, individuals interact through webs of relationships. Each individual is a node in a complex network (or graph) of interdependencies and generates data, lots of data. We label the data by its source, or formally stated, we index the data by the nodes of the graph. The resulting signals (data indexed by the nodes) are far removed from time or image signals indexed by well ordered time samples or pixels. DSP, discrete signal processing, provides a comprehensive, elegant, and efficient methodology to describe, represent, transform, analyze, process, or synthesize these well ordered time or image signals. This paper extends to signals on graphs DSP and its basic tenets, including filters, convolution, z-transform, impulse response, spectral representation, Fourier transform, frequency response, and illustrates DSP on graphs by classifying blogs, linear predicting and compressing data from irregularly located weather stations, or predicting behavior of customers of a mobile service provider.},
	keywords     = {Digital signal processing, Fourier transforms, Graph Fourier transform, graphical models, Graphical models, Laplace equations, Manifolds, Markov random fields, network science, signal processing}
}
@article{thorpe_introduction_nodate,
	title        = {Introduction to {Optimal} {Transport}},
	author       = {Thorpe, Matthew},
	pages        = 56
}
@article{ferradans_regularized_2014,
	title        = {Regularized {Discrete} {Optimal} {Transport}},
	author       = {Ferradans, Sira and Papadakis, Nicolas and Peyré, Gabriel and Aujol, Jean-François},
	year         = 2014,
	journal      = {SIAM J. Imaging Sci.},
	volume       = 7,
	number       = 3,
	pages        = {1853--1882},
	abstract     = {This article introduces a generalization of the discrete optimal transport, with applications to color image manipulations. This new formulation includes a relaxation of the mass conservation constraint and a regularization term. These two features are crucial for image processing tasks where one must take into account families of multimodal histograms with large mass variation across modes. The corresponding relaxed and regularized transportation problem is the solution of a convex optimization problem. Depending on the regularization used, this minimization can be solved using standard linear programming methods or ﬁrst order proximal splitting schemes. The resulting transportation plan can be used as a color transfer map, which is robust to mass variation across image color palettes. Furthermore, the regularization of the transport plan helps remove colorization artifacts due to noise ampliﬁcation. We also extend this framework to compute the barycenter of distributions. The barycenter is the solution of an optimization problem, which is separately convex with respect to the barycenter and the transportation plans, but not jointly convex. A block coordinate descent scheme converges to a stationary point of the energy. We show that the resulting algorithm can be used for color normalization across several images. The relaxed and regularized barycenter deﬁnes a common color palette for those images. Applying color transfer toward this average palette performs a color normalization of the input images.}
}
@article{chamberlain_beltrami_2021,
	title        = {Beltrami {Flow} and {Neural} {Diffusion} on {Graphs}},
	author       = {Chamberlain, Benjamin Paul and Rowbottom, James and Eynard, Davide and Di Giovanni, Francesco and Dong, Xiaowen and Bronstein, Michael M.},
	year         = 2021,
	journal      = {arXiv:2110.09443 [cs, stat]},
	abstract     = {We propose a novel class of graph neural networks based on the discretised Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with positional encodings derived from the graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature learning and topology evolution. The resulting model generalises many popular graph neural networks and achieves state-of-the-art results on several benchmarks.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: 21 pages, 5 figures. Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) 2021}
}
@article{le_tree-sliced_2019,
	title        = {Tree-{Sliced} {Variants} of {Wasserstein} {Distances}},
	author       = {Le, Tam and Yamada, Makoto and Fukumizu, Kenji and Cuturi, Marco},
	year         = 2019,
	journal      = {arXiv:1902.00342 [cs, stat]},
	abstract     = {Optimal transport ({\textbackslash}OT) theory defines a powerful set of tools to compare probability distributions. {\textbackslash}OT{\textasciitilde}suffers however from a few drawbacks, computational and statistical, which have encouraged the proposal of several regularized variants of OT in the recent literature, one of the most notable being the {\textbackslash}textit\{sliced\} formulation, which exploits the closed-form formula between univariate distributions by projecting high-dimensional measures onto random lines. We consider in this work a more general family of ground metrics, namely {\textbackslash}textit\{tree metrics\}, which also yield fast closed-form computations and negative definite, and of which the sliced-Wasserstein distance is a particular case (the tree is a chain). We propose the tree-sliced Wasserstein distance, computed by averaging the Wasserstein distance between these measures using random tree metrics, built adaptively in either low or high-dimensional spaces. Exploiting the negative definiteness of that distance, we also propose a positive definite kernel, and test it against other baselines on a few benchmark tasks.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Camera-ready for NeurIPS 2019}
}
@inproceedings{shirdhonkar_approximate_2008,
	title        = {Approximate earth mover’s distance in linear time},
	author       = {Shirdhonkar, Sameer and Jacobs, David W.},
	year         = 2008,
	booktitle    = {2008 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	pages        = {1--8},
	abstract     = {The earth moverpsilas distance (EMD) is an important perceptually meaningful metric for comparing histograms, but it suffers from high (O(N3 logN)) computational complexity. We present a novel linear time algorithm for approximating the EMD for low dimensional histograms using the sum of absolute values of the weighted wavelet coefficients of the difference histogram. EMD computation is a special case of the Kantorovich-Rubinstein transshipment problem, and we exploit the Holder continuity constraint in its dual form to convert it into a simple optimization problem with an explicit solution in the wavelet domain. We prove that the resulting wavelet EMD metric is equivalent to EMD, i.e. the ratio of the two is bounded. We also provide estimates for the bounds. The weighted wavelet transform can be computed in time linear in the number of histogram bins, while the comparison is about as fast as for normal Euclidean distance or chi2 statistic. We experimentally show that wavelet EMD is a good approximation to EMD, has similar performance, but requires much less computation.},
	keywords     = {Computational complexity, Earth, Extraterrestrial measurements, Histograms, Image retrieval, Jacobian matrices, Pattern matching, Statistics, Wavelet coefficients, Wavelet domain}
}
@article{tong_diffusion_2021,
	title        = {Diffusion {Earth} {Mover}'s {Distance} and {Distribution} {Embeddings}},
	author       = {Tong, Alexander and Huguet, Guillaume and Natik, Amine and MacDonald, Kincaid and Kuchroo, Manik and Coifman, Ronald and Wolf, Guy and Krishnaswamy, Smita},
	year         = 2021,
	journal      = {arXiv:2102.12833 [cs]},
	abstract     = {We propose a new fast method of measuring distances between large numbers of related high dimensional datasets called the Diffusion Earth Mover's Distance (EMD). We model the datasets as distributions supported on common data graph that is derived from the affinity matrix computed on the combined data. In such cases where the graph is a discretization of an underlying Riemannian closed manifold, we prove that Diffusion EMD is topologically equivalent to the standard EMD with a geodesic ground distance. Diffusion EMD can be computed in \${\textbackslash}tilde\{O\}(n)\$ time and is more accurate than similarly fast algorithms such as tree-based EMDs. We also show Diffusion EMD is fully differentiable, making it amenable to future uses in gradient-descent frameworks such as deep neural networks. Finally, we demonstrate an application of Diffusion EMD to single cell data collected from 210 COVID-19 patient samples at Yale New Haven Hospital. Here, Diffusion EMD can derive distances between patients on the manifold of cells at least two orders of magnitude faster than equally accurate methods. This distance matrix between patients can be embedded into a higher level patient manifold which uncovers structure and heterogeneity in patients. More generally, Diffusion EMD is applicable to all datasets that are massively collected in parallel in many medical and biological systems.},
	keywords     = {Computer Science - Machine Learning},
	annote       = {Comment: Presented at ICML 2021}
}
@inproceedings{backurs_scalable_2020,
	title        = {Scalable {Nearest} {Neighbor} {Search} for {Optimal} {Transport}},
	author       = {Backurs, Arturs and Dong, Yihe and Indyk, Piotr and Razenshteyn, Ilya and Wagner, Tal},
	year         = 2020,
	booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {497--506},
	abstract     = {The Optimal Transport (a.k.a. Wasserstein) distance is an increasingly popular similarity measure for rich data domains, such as images or text documents. This raises the necessity for fast nearest neighbor search algorithms according to this distance, which poses a substantial computational bottleneck on massive datasets. In this work we introduce Flowtree, a fast and accurate approximation algorithm for the Wasserstein-1 distance. We formally analyze its approximation factor and running time. We perform extensive experimental evaluation of nearest neighbor search algorithms in the W\_1 distance on real-world dataset. Our results show that compared to previous state of the art, Flowtree achieves up to 7.4 times faster running time.}
}
@article{kusner_word_nodate,
	title        = {From {Word} {Embeddings} {To} {Document} {Distances}},
	author       = {Kusner, Matt J and Sun, Yu and Kolkin, Nicholas I and Weinberger, Kilian Q},
	pages        = 10,
	abstract     = {We present the Word Mover’s Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to “travel” to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover’s Distance, a well studied transportation problem for which several highly efﬁcient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classiﬁcation data sets, in comparison with seven stateof-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classiﬁcation error rates.}
}
@techreport{badia-i-mompel_decoupler_2021,
	title        = {{decoupleR}: {Ensemble} of computational methods to infer biological activities from omics data},
	shorttitle   = {{decoupleR}},
	author       = {Badia-i-Mompel, Pau and Vélez, Jesús and Braunger, Jana and Geiss, Celina and Dimitrov, Daniel and Müller-Dott, Sophia and Taus, Petr and Dugourd, Aurelien and Holland, Christian Haydar and Flores, Ricardo Omar Ramírez and Saez-Rodriguez, Julio},
	year         = 2021,
	pages        = {2021.11.04.467271},
	copyright    = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	abstract     = {Summary: Many methods allow us to extract biological activities from omics data using information from prior knowledge resources, reducing the dimensionality for increased statistical power and better interpretability. Here, we present decoupleR, a Bioconductor package containing computational methods to extract these activities within a unified framework. decoupleR allows us to flexibly run any method with a given resource, including methods that leverage mode of regulation and weights of interactions. Using decoupleR, we evaluated the performance of methods on transcriptomic and phospho-proteomic perturbation experiments. Our findings suggest that simple linear models and the consensus score across methods perform better than other methods at predicting perturbed regulators. Availability and Implementation: decoupleR is open source available in Bioconductor (https://www.bioconductor.org/packages/release/bioc/html/decoupleR.html). The code to reproduce the results is in Github (https://github.com/saezlab/decoupleR\_manuscript) and the data in Zenodo (https://zenodo.org/record/5645208).}
}
@techreport{badia-i-mompel_decoupler_2021-1,
	title        = {{decoupleR}: {Ensemble} of computational methods to infer biological activities from omics data},
	shorttitle   = {{decoupleR}},
	author       = {Badia-i-Mompel, Pau and Vélez, Jesús and Braunger, Jana and Geiss, Celina and Dimitrov, Daniel and Müller-Dott, Sophia and Taus, Petr and Dugourd, Aurelien and Holland, Christian Haydar and Flores, Ricardo Omar Ramírez and Saez-Rodriguez, Julio},
	year         = 2021,
	pages        = {2021.11.04.467271},
	copyright    = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	abstract     = {Summary: Many methods allow us to extract biological activities from omics data using information from prior knowledge resources, reducing the dimensionality for increased statistical power and better interpretability. Here, we present decoupleR, a Bioconductor package containing computational methods to extract these activities within a unified framework. decoupleR allows us to flexibly run any method with a given resource, including methods that leverage mode of regulation and weights of interactions. Using decoupleR, we evaluated the performance of methods on transcriptomic and phospho-proteomic perturbation experiments. Our findings suggest that simple linear models and the consensus score across methods perform better than other methods at predicting perturbed regulators. Availability and Implementation: decoupleR is open source available in Bioconductor (https://www.bioconductor.org/packages/release/bioc/html/decoupleR.html). The code to reproduce the results is in Github (https://github.com/saezlab/decoupleR\_manuscript) and the data in Zenodo (https://zenodo.org/record/5645208).}
}
@article{singer_graph_2006,
	title        = {From graph to manifold {Laplacian}: {The} convergence rate},
	shorttitle   = {From graph to manifold {Laplacian}},
	author       = {Singer, A.},
	year         = 2006,
	journal      = {Applied and Computational Harmonic Analysis},
	series       = {Special {Issue}: {Diffusion} {Maps} and {Wavelets}},
	volume       = 21,
	number       = 1,
	pages        = {128--134},
	abstract     = {The convergence of the discrete graph Laplacian to the continuous manifold Laplacian in the limit of sample size N→∞ while the kernel bandwidth ε→0, is the justification for the success of Laplacian based algorithms in machine learning, such as dimensionality reduction, semi-supervised learning and spectral clustering. In this paper we improve the convergence rate of the variance term recently obtained by Hein et al. [From graphs to manifolds—Weak and strong pointwise consistency of graph Laplacians, in: P. Auer, R. Meir (Eds.), Proc. 18th Conf. Learning Theory (COLT), Lecture Notes Comput. Sci., vol. 3559, Springer-Verlag, Berlin, 2005, pp. 470–485], improve the bias term error, and find an optimal criteria to determine the parameter ε given N.}
}
@article{wang_heterogeneous_2021,
	title        = {Heterogeneous {Graph} {Attention} {Network}},
	author       = {Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Cui, Peng and Yu, P. and Ye, Yanfang},
	year         = 2021,
	journal      = {arXiv:1903.07293 [cs]},
	abstract     = {Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its metapath based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.},
	keywords     = {Computer Science - Social and Information Networks},
	annote       = {Comment: 10 pages}
}
@article{xia_motivations_2015,
	title        = {Motivations, ideas and applications of ramified optimal transportation},
	author       = {Xia, Qinglan},
	year         = 2015,
	journal      = {ESAIM: M2AN},
	volume       = 49,
	number       = 6,
	pages        = {1791--1832},
	abstract     = {In this survey article, the author summarizes the motivations, key ideas and main applications of ramiﬁed optimal transportation that the author has studied in recent years.}
}
@inproceedings{li_metrics_2019,
	title        = {Metrics of graph {Laplacian} eigenvectors},
	author       = {Li, Haotian and Saito, Naoki},
	year         = 2019,
	booktitle    = {Wavelets and {Sparsity} {XVIII}},
	publisher    = {SPIE},
	address      = {San Diego, United States},
	pages        = 56,
	isbn         = {978-1-5106-2969-1 978-1-5106-2970-7},
	abstract     = {The application of graph Laplacian eigenvectors has been quite popular in the graph signal processing ﬁeld: one can use them as ingredients to design smooth multiscale basis. Our long-term goal is to study and understand the dual geometry of graph Laplacian eigenvectors. In order to do that, it is necessary to deﬁne a certain metric to measure the behavioral diﬀerences between each pair of the eigenvectors. Saito (2018) considered the ramiﬁed optimal transportation (ROT) cost between the square of the eigenvectors as such a metric. Clonginger and Steinerberger (2018) proposed a way to measure the aﬃnity (or ‘similarity’) between the eigenvectors based on their Hadamard (HAD) product. In this article, we propose a simpliﬁed ROT metric that is more computational eﬃcient and introduce two more ways to deﬁne the distance between the eigenvectors, i.e., the time-stepping diﬀusion (TSD) metric and the diﬀerence of absolute gradient (DAG) pseudometric. The TSD metric measures the cost of “ﬂattening” the initial graph signal via diﬀusion process up to certain time, hence it can be viewed as a time-dependent version of the ROT metric. The DAG pseudometric is the l2-distance between the feature vectors derived from the eigenvectors, in particular, the absolute gradients of the eigenvectors. We then compare the performance of ROT, HAD and the two new “metrics” on diﬀerent kinds of graphs. Finally, we investigate their relationship as well as their pros and cons.},
	editor       = {Lu, Yue M. and Papadakis, Manos and Van De Ville, Dimitri}
}
@misc{noauthor_mat_nodate,
	title        = {{MAT} 280 {Harmonic} {Analysis} on {Graphs} \& {Networks} {Lecture} {Slides} {Page}}
}
@article{radl_resistance_nodate,
	title        = {The {Resistance} {Distance} is {Meaningless} for {Large} {Random} {Geometric} {Graphs}},
	author       = {Radl, Agnes},
	pages        = 8,
	abstract     = {We study convergence properties of the resistance distance on random geometric graphs for increasing sample size. It turns out that the suitably scaled resistance distance between two ﬁxed points converges to a non-trivial limit. However, this limit no longer takes into account global properties of the graph, as for example the cluster structure. Quite to the opposite, the limit distance function is rather meaningless. As our simulations show, this phenomenon can already be observed for rather small sample sizes. Thus, we discourage the usage of the resistance distance for learning purposes on the type of graph we have analyzed so far.}
}
@article{qiu_clustering_2007,
	title        = {Clustering and {Embedding} {Using} {Commute} {Times}},
	author       = {Qiu, Huaijun and Hancock, Edwin R.},
	year         = 2007,
	journal      = {IEEE Trans. Pattern Anal. Machine Intell.},
	volume       = 29,
	number       = 11,
	pages        = {1873--1890},
	abstract     = {This paper exploits the properties of the commute time between nodes of a graph for the purposes of clustering and embedding and explores its applications to image segmentation and multibody motion tracking. Our starting point is the lazy random walk on the graph, which is determined by the heat kernel of the graph and can be computed from the spectrum of the graph Laplacian. We characterize the random walk using the commute time (that is, the expected time taken for a random walk to travel between two nodes and return) and show how this quantity may be computed from the Laplacian spectrum using the discrete Green’s function. Our motivation is that the commute time can be anticipated to be a more robust measure of the proximity of data than the raw proximity matrix. In this paper, we explore two applications of the commute time. The first is to develop a method for image segmentation using the eigenvector corresponding to the smallest eigenvalue of the commute time matrix. We show that our commute time segmentation method has the property of enhancing the intragroup coherence while weakening intergroup coherence and is superior to the normalized cut. The second application is to develop a robust multibody motion tracking method using an embedding based on the commute time. Our embedding procedure preserves commute time and is closely akin to kernel PCA, the Laplacian eigenmap, and the diffusion map. We illustrate the results on both synthetic image sequences and real-world video sequences and compare our results with several alternative methods.}
}
@article{snow_monge-kantorovich_2018,
	title        = {The {Monge}-{Kantorovich} {Optimal} {Transport} {Distance} for {Image} {Comparison}},
	author       = {Snow, Michael and Van lent, Jan},
	year         = 2018,
	journal      = {arXiv:1804.03531 [cs]},
	abstract     = {This paper focuses on the Monge-Kantorovich formulation of the optimal transport problem and the associated \$L{\textasciicircum}2\$ Wasserstein distance. We use the \$L{\textasciicircum}2\$ Wasserstein distance in the Nearest Neighbour (NN) machine learning architecture to demonstrate the potential power of the optimal transport distance for image comparison. We compare the Wasserstein distance to other established distances - including the partial differential equation (PDE) formulation of the optimal transport problem - and demonstrate that on the well known MNIST optical character recognition dataset, it achieves excellent results.},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition},
	annote       = {Comment: arXiv admin note: substantial text overlap with arXiv:1612.00181}
}
@article{kileel_manifold_2021,
	title        = {Manifold learning with arbitrary norms},
	author       = {Kileel, Joe and Moscovich, Amit and Zelesko, Nathan and Singer, Amit},
	year         = 2021,
	journal      = {arXiv:2012.14172 [cs, stat]},
	abstract     = {Manifold learning methods play a prominent role in nonlinear dimensionality reduction and other tasks involving high-dimensional data sets with low intrinsic dimensionality. Many of these methods are graph-based: they associate a vertex with each data point and a weighted edge with each pair. Existing theory shows that the Laplacian matrix of the graph converges to the Laplace-Beltrami operator of the data manifold, under the assumption that the pairwise affinities are based on the Euclidean norm. In this paper, we determine the limiting differential operator for graph Laplacians constructed using \${\textbackslash}textit\{any\}\$ norm. Our proof involves an interplay between the second fundamental form of the manifold and the convex geometry of the given norm's unit ball. To demonstrate the potential benefits of non-Euclidean norms in manifold learning, we consider the task of mapping the motion of large molecules with continuous variability. In a numerical simulation we show that a modified Laplacian eigenmaps algorithm, based on the Earthmover's distance, outperforms the classic Euclidean Laplacian eigenmaps, both in terms of computational cost and the sample size needed to recover the intrinsic geometry.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: 53 pages, 8 figures, 3 tables, to appear in Journal of Fourier Analysis and Applications}
}
@article{berry_local_2016,
	title        = {Local kernels and the geometric structure of data},
	author       = {Berry, Tyrus and Sauer, Timothy},
	year         = 2016,
	journal      = {Applied and Computational Harmonic Analysis},
	volume       = 40,
	number       = 3,
	pages        = {439--469}
}
@article{chen_comprehensive_2016,
	title        = {A comprehensive approach to mode clustering},
	author       = {Chen, Yen-Chi and Genovese, Christopher R. and Wasserman, Larry},
	year         = 2016,
	journal      = {Electron. J. Statist.},
	volume       = 10,
	number       = 1
}
@article{comaniciu_mean_2002,
	title        = {Mean shift: a robust approach toward feature space analysis},
	shorttitle   = {Mean shift},
	author       = {Comaniciu, D. and Meer, P.},
	year         = 2002,
	journal      = {IEEE Trans. Pattern Anal. Machine Intell.},
	volume       = 24,
	number       = 5,
	pages        = {603--619}
}
@article{fukunaga_estimation_1975,
	title        = {The estimation of the gradient of a density function, with applications in pattern recognition},
	author       = {Fukunaga, K. and Hostetler, L.},
	year         = 1975,
	journal      = {IEEE Trans. Inform. Theory},
	volume       = 21,
	number       = 1,
	pages        = {32--40}
}
@article{little_balancing_2021,
	title        = {Balancing {Geometry} and {Density}: {Path} {Distances} on {High}-{Dimensional} {Data}},
	shorttitle   = {Balancing {Geometry} and {Density}},
	author       = {Little, Anna and McKenzie, Daniel and Murphy, James},
	year         = 2021,
	journal      = {arXiv:2012.09385 [cs, stat]},
	abstract     = {New geometric and computational analyses of power-weighted shortest-path distances (PWSPDs) are presented. By illuminating the way these metrics balance density and geometry in the underlying data, we clarify their key parameters and discuss how they may be chosen in practice. Comparisons are made with related data-driven metrics, which illustrate the broader role of density in kernel-based unsupervised and semi-supervised machine learning. Computationally, we relate PWSPDs on complete weighted graphs to their analogues on weighted nearest neighbor graphs, providing high probability guarantees on their equivalence that are near-optimal. Connections with percolation theory are developed to establish estimates on the bias and variance of PWSPDs in the finite sample setting. The theoretical results are bolstered by illustrative experiments, demonstrating the versatility of PWSPDs for a wide range of data settings. Throughout the paper, our results require only that the underlying data is sampled from a low-dimensional manifold, and depend crucially on the intrinsic dimension of this manifold, rather than its ambient dimension.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Data Structures and Algorithms, 05C85, 05C80, I.5.3}
}
@article{little_path-based_2019,
	title        = {Path-{Based} {Spectral} {Clustering}: {Guarantees}, {Robustness} to {Outliers}, and {Fast} {Algorithms}},
	shorttitle   = {Path-{Based} {Spectral} {Clustering}},
	author       = {Little, Anna and Maggioni, Mauro and Murphy, James M.},
	year         = 2019,
	journal      = {arXiv:1712.06206 [stat]},
	abstract     = {We consider the problem of clustering with the longest-leg path distance (LLPD) metric, which is informative for elongated and irregularly shaped clusters. We prove finite-sample guarantees on the performance of clustering with respect to this metric when random samples are drawn from multiple intrinsically low-dimensional clusters in high-dimensional space, in the presence of a large number of high-dimensional outliers. By combining these results with spectral clustering with respect to LLPD, we provide conditions under which the Laplacian eigengap statistic correctly determines the number of clusters for a large class of data sets, and prove guarantees on the labeling accuracy of the proposed algorithm. Our methods are quite general and provide performance guarantees for spectral clustering with any ultrametric. We also introduce an efficient, easy to implement approximation algorithm for the LLPD based on a multiscale analysis of adjacency graphs, which allows for the runtime of LLPD spectral clustering to be quasilinear in the number of data points.},
	keywords     = {Statistics - Machine Learning},
	annote       = {Comment: 59 pages, 12 figures}
}
@inproceedings{kleinberg_approximation_1999,
	title        = {Approximation algorithms for classification problems with pairwise relationships: metric labeling and {Markov} random fields},
	shorttitle   = {Approximation algorithms for classification problems with pairwise relationships},
	author       = {Kleinberg, J. and Tardos, E.},
	year         = 1999,
	booktitle    = {40th {Annual} {Symposium} on {Foundations} of {Computer} {Science} ({Cat}. {No}.{99CB37039})},
	publisher    = {IEEE Comput. Soc},
	address      = {New York City, NY, USA},
	pages        = {14--23},
	isbn         = {978-0-7695-0409-4},
	abstract     = {In a traditional classiﬁcation problem, we wish to assign one of k labels (or classes) to each of n objects, in a way that is consistent with some observed data that we have about the problem. An active line of research in this area is concerned with classiﬁcation when one has information about pairwise relationships among the objects to be classiﬁed; this issue is one of the principal motivations for the framework of Markov random ﬁelds, and it arises in areas such as image processing, biometry, and document analysis. In its most basic form, this style of analysis seeks to ﬁnd a classiﬁcation that optimizes a combinatorial function consisting of assignment costs—based on the individual choice of label we make for each object—and separation costs—based on the pair of choices we make for two “related” objects.}
}
@article{charikar_similarity_nodate,
	title        = {Similarity {Estimation} {Techniques} from {Rounding} {Algorithms}},
	author       = {Charikar, Moses S},
	pages        = 9
}
@article{andoni_approximate_2018,
	title        = {Approximate {Nearest} {Neighbor} {Search} in {High} {Dimensions}},
	author       = {Andoni, Alexandr and Indyk, Piotr and Razenshteyn, Ilya},
	year         = 2018,
	journal      = {arXiv:1806.09823 [cs, stat]},
	abstract     = {The nearest neighbor problem is defined as follows: Given a set \$P\$ of \$n\$ points in some metric space \$(X,D)\$, build a data structure that, given any point \$q\$, returns a point in \$P\$ that is closest to \$q\$ (its "nearest neighbor" in \$P\$). The data structure stores additional information about the set \$P\$, which is then used to find the nearest neighbor without computing all distances between \$q\$ and \$P\$. The problem has a wide range of applications in machine learning, computer vision, databases and other fields. To reduce the time needed to find nearest neighbors and the amount of memory used by the data structure, one can formulate the \{{\textbackslash}em approximate\} nearest neighbor problem, where the the goal is to return any point \$p' {\textbackslash}in P\$ such that the distance from \$q\$ to \$p'\$ is at most \$c {\textbackslash}cdot {\textbackslash}min\_\{p {\textbackslash}in P\} D(q,p)\$, for some \$c {\textbackslash}geq 1\$. Over the last two decades, many efficient solutions to this problem were developed. In this article we survey these developments, as well as their connections to questions in geometric functional analysis and combinatorial geometry.},
	keywords     = {Statistics - Machine Learning, Computer Science - Computational Geometry, Computer Science - Data Structures and Algorithms, Computer Science - Databases},
	annote       = {Comment: 27 pages, no figures; to appear in the proceedings of ICM 2018 (accompanying the talk by P. Indyk)}
}
@article{bommer_identifying_2021,
	title        = {Identifying nonlinear dynamical systems from multi-modal time series data},
	author       = {Bommer, Philine Lou and Kramer, Daniel and Tombolini, Carlo and Koppe, Georgia and Durstewitz, Daniel},
	year         = 2021,
	abstract     = {Empirically observed time series in physics, biology, or medicine, are commonly generated by some underlying dynamical system (DS) which is the target of scientific interest. There is an increasing interest to harvest machine learning methods to reconstruct this latent DS in a completely data-driven, unsupervised way. In many areas of science it is common to sample time series observations from many data modalities simultaneously, e.g. electrophysiological and behavioral time series in a typical neuroscience experiment. However, current machine learning tools for reconstructing DSs usually focus on just one data modality. Here we propose a general framework for multi-modal data integration for the purpose of nonlinear DS identification and cross-modal prediction. This framework is based on dynamically interpretable recurrent neural networks as general approximators of nonlinear DSs, coupled to sets of modality-specific decoder models from the class of generalized linear models. Both an expectation-maximization and a variational inference algorithm for model training are advanced and compared. We show on nonlinear DS benchmarks that our algorithms can efficiently compensate for too noisy or missing information in one data channel by exploiting other channels, and demonstrate on experimental neuroscience data how the algorithm learns to link different data domains to the underlying dynamics},
	keywords     = {Read}
}
@inproceedings{venkitaraman_gaussian_2020,
	title        = {Gaussian {Processes} {Over} {Graphs}},
	author       = {Venkitaraman, Arun and Chatterjee, Saikat and Handel, Peter},
	year         = 2020,
	booktitle    = {{ICASSP} 2020 - 2020 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	pages        = {5640--5644},
	abstract     = {Kernel Regression over Graphs (KRG) was recently proposed for predicting graph signals in a supervised learning setting, where the inputs are agnostic to the graph. KRG model predicts targets that are smooth graph signals as over the given graph, given the input when all the signals are deterministic. In this work, we consider the development of a stochastic or Bayesian variant of KRG. Using priors and likelihood functions, our goal is to systematically derive a predictive distribution for the smooth graph signal target given the training data and a new input. We show that this naturally results in a Gaussian process formulation which we call Gaussian Processes over Graphs (GPG). Experiments with real-world datasets show that the performance of GPG is superior to a conventional Gaussian Process (without the graph-structure) for small training data sizes and under noisy training.},
	keywords     = {Kernel, Graph signal processing, Bayes methods, Bayesian estimation, Gaussian processes, graph-Laplacian, kernel regression, Noise measurement, Supervised learning, Training, Training data}
}
@article{zecevic_relating_2021,
	title        = {Relating {Graph} {Neural} {Networks} to {Structural} {Causal} {Models}},
	author       = {Zečević, Matej and Dhami, Devendra Singh and Veličković, Petar and Kersting, Kristian},
	year         = 2021,
	journal      = {arXiv:2109.04173 [cs, stat]},
	abstract     = {Causality can be described in terms of a structural causal model (SCM) that carries information on the variables of interest and their mechanistic relations. For most processes of interest the underlying SCM will only be partially observable, thus causal inference tries leveraging the exposed. Graph neural networks (GNN) as universal approximators on structured input pose a viable candidate for causal learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis from first principles that establishes a more general view on neural-causal models, revealing several novel connections between GNN and SCM. We establish a new model class for GNN-based causal inference that is necessary and sufficient for causal effect identification. Our empirical illustration on simulations and standard benchmarks validate our theoretical proofs.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Main paper: 12 pages, References: 2 pages, Appendix: 13 pages; Main paper: 4 figures, Appendix: 2 figures}
}
@article{xia_causal-neural_nodate,
	title        = {The {Causal}-{Neural} {Connection}: {Expressiveness}, {Learnability}, and {Inference}},
	author       = {Xia, Kevin and Lee, Kai-Zhan and Bengio, Yoshua and Bareinboim, Elias},
	pages        = 53,
	abstract     = {One of the central elements of any causal inference is an object called structural causal model (SCM), which represents a collection of mechanisms and exogenous sources of random variation of the system under investigation (Pearl, 2000). An important property of many kinds of neural networks is universal approximability: the ability to approximate any function to arbitrary precision. Given this property, one may be tempted to surmise that a collection of neural nets is capable of learning any SCM by training on data generated by that SCM. In this paper, we show this is not the case by disentangling the notions of expressivity and learnability. Speciﬁcally, we show that the causal hierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits of what can be learned from data, still holds for neural models. For instance, an arbitrarily complex and expressive neural net is unable to predict the effects of interventions given observational data alone. Given this result, we introduce a special type of SCM called a neural causal model (NCM), and formalize a new type of inductive bias to encode structural constraints necessary for performing causal inferences. Building on this new class of models, we focus on solving two canonical tasks found in the literature known as causal identiﬁcation and estimation. Leveraging the neural toolbox, we develop an algorithm that is both sufﬁcient and necessary to determine whether a causal effect can be learned from data (i.e., causal identiﬁability); it then estimates the effect whenever identiﬁability holds (causal estimation). Simulations corroborate the proposed approach.}
}
@article{shanmugam_elements_2018,
	title        = {Elements of causal inference: foundations and learning algorithms},
	shorttitle   = {Elements of causal inference},
	author       = {Shanmugam, Ramalingam},
	year         = 2018,
	journal      = {Journal of Statistical Computation and Simulation},
	volume       = 88,
	number       = 16,
	pages        = {3248--3248}
}
@article{sanchez-martin_vaca_2021,
	title        = {{VACA}: {Design} of {Variational} {Graph} {Autoencoders} for {Interventional} and {Counterfactual} {Queries}},
	shorttitle   = {{VACA}},
	author       = {Sanchez-Martin, Pablo and Rateike, Miriam and Valera, Isabel},
	year         = 2021,
	journal      = {arXiv:2110.14690 [cs, stat]},
	abstract     = {In this paper, we introduce VACA, a novel class of variational graph autoencoders for causal inference in the absence of hidden confounders, when only observational data and the causal graph are available. Without making any parametric assumptions, VACA mimics the necessary properties of a Structural Causal Model (SCM) to provide a flexible and practical framework for approximating interventions (do-operator) and abduction-action-prediction steps. As a result, and as shown by our empirical results, VACA accurately approximates the interventional and counterfactual distributions on diverse SCMs. Finally, we apply VACA to evaluate counterfactual fairness in fair classification problems, as well as to learn fair classifiers without compromising performance.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
@article{zecevic_interventional_2021,
	title        = {Interventional {Sum}-{Product} {Networks}: {Causal} {Inference} with {Tractable} {Probabilistic} {Models}},
	shorttitle   = {Interventional {Sum}-{Product} {Networks}},
	author       = {Zečević, Matej and Dhami, Devendra Singh and Karanam, Athresh and Natarajan, Sriraam and Kersting, Kristian},
	year         = 2021,
	journal      = {arXiv:2102.10440 [cs]},
	abstract     = {While probabilistic models are an important tool for studying causality, doing so suffers from the intractability of inference. As a step towards tractable causal models, we consider the problem of learning interventional distributions using sum-product networks (SPNs) that are over-parameterized by gate functions, e.g., neural networks. Providing an arbitrarily intervened causal graph as input, effectively subsuming Pearl's do-operator, the gate function predicts the parameters of the SPN. The resulting interventional SPNs are motivated and illustrated by a structural causal model themed around personal health. Our empirical evaluation on three benchmark data sets as well as a synthetic health data set clearly demonstrates that interventional SPNs indeed are both expressive in modelling and flexible in adapting to the interventions.},
	keywords     = {Computer Science - Machine Learning},
	annote       = {Comment: Main paper: 10 pages, References: 3 pages, Appendix: 8 pages. Main paper: 6 figures, Appendix: 5 figures}
}
@article{zecevic_relating_2021-1,
	title        = {Relating {Graph} {Neural} {Networks} to {Structural} {Causal} {Models}},
	author       = {Zečević, Matej and Dhami, Devendra Singh and Veličković, Petar and Kersting, Kristian},
	year         = 2021,
	journal      = {arXiv:2109.04173 [cs, stat]},
	abstract     = {Causality can be described in terms of a structural causal model (SCM) that carries information on the variables of interest and their mechanistic relations. For most processes of interest the underlying SCM will only be partially observable, thus causal inference tries leveraging the exposed. Graph neural networks (GNN) as universal approximators on structured input pose a viable candidate for causal learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis from first principles that establishes a more general view on neural-causal models, revealing several novel connections between GNN and SCM. We establish a new model class for GNN-based causal inference that is necessary and sufficient for causal effect identification. Our empirical illustration on simulations and standard benchmarks validate our theoretical proofs.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {paper in Graph RG scetion}
}
@article{poli_graph_2019,
	title        = {Graph {Neural} {Ordinary} {Differential} {Equations}},
	author       = {Poli, Michael and Massaroli, Stefano and Park, Junyoung and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
	year         = 2019,
	pages        = 18,
	abstract     = {We introduce the framework of continuous–depth graph neural networks (GNNs). Graph neural ordinary differential equations (GDEs) are formalized as the counterpart to GNNs where the input–output relationship is determined by a continuum of GNN layers, blending discrete topological structures and differential equations. The proposed framework is shown to be compatible with various static and autoregressive GNN models. Results prove general effectiveness of GDEs: in static settings they offer computational advantages by incorporating numerical methods in their forward pass; in dynamic settings, on the other hand, they are shown to improve performance by exploiting the geometry of the underlying dynamics.},
	keywords     = {Read}
}
@misc{noauthor_dream5_nodate,
	title        = {{DREAM5} - {Network} {Inference} - syn2787209 - {Wiki}}
}
@misc{noauthor_regulondb_nodate,
	title        = {{RegulonDB}}
}
@misc{noauthor_hill_nodate,
	title        = {Hill {Function} - bio-physics-wiki}
}
@misc{noauthor_multi-modal_nodate,
	title        = {A {Multi}-{Modal} {Simulator} for {Spearheading} {Single}-{Cell} {Omics} {Analyses}},
	abstract     = {A novel, multi-modal simulation engine for studying dynamic cellular processes at single-cell resolution. dyngen is more flexible than current single-cell simulation engines. It allows better method development and benchmarking, thereby stimulating development and testing of novel computational methods. Cannoodt et al. (2021) {\textless}doi:10.1038/s41467-021-24152-2{\textgreater}.}
}
@article{cannoodt_spearheading_2021,
	title        = {Spearheading future omics analyses using dyngen, a multi-modal simulator of single cells},
	author       = {Cannoodt, Robrecht and Saelens, Wouter and Deconinck, Louise and Saeys, Yvan},
	year         = 2021,
	journal      = {Nat Commun},
	volume       = 12,
	number       = 1,
	pages        = 3942,
	copyright    = {2021 The Author(s)},
	abstract     = {We present dyngen, a multi-modal simulation engine for studying dynamic cellular processes at single-cell resolution. dyngen is more flexible than current single-cell simulation engines, and allows better method development and benchmarking, thereby stimulating development and testing of computational methods. We demonstrate its potential for spearheading computational methods on three applications: aligning cell developmental trajectories, cell-specific regulatory network inference and estimation of RNA velocity.},
	keywords     = {Computational models, Gene regulatory networks, Machine learning, Quality control, Read}
}
@article{sanchez-gonzalez_hamiltonian_2019,
	title        = {Hamiltonian {Graph} {Networks} with {ODE} {Integrators}},
	author       = {Sanchez-Gonzalez, Alvaro and Bapst, Victor and Cranmer, Kyle and Battaglia, Peter},
	year         = 2019,
	journal      = {arXiv:1909.12790 [physics]},
	abstract     = {We introduce an approach for imposing physically informed inductive biases in learned simulation models. We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states, and a Hamiltonian as an internal representation. We find that our approach outperforms baselines without these biases in terms of predictive accuracy, energy accuracy, and zero-shot generalization to time-step sizes and integrator orders not experienced during training. This advances the state-of-the-art of learned simulation, and in principle is applicable beyond physical domains.},
	keywords     = {Computer Science - Machine Learning, Physics - Computational Physics}
}
@article{chamberlain_beltrami_2021-1,
	title        = {Beltrami {Flow} and {Neural} {Diffusion} on {Graphs}},
	author       = {Chamberlain, Benjamin Paul and Rowbottom, James and Eynard, Davide and Di Giovanni, Francesco and Dong, Xiaowen and Bronstein, Michael M.},
	year         = 2021,
	journal      = {arXiv:2110.09443 [cs, stat]},
	abstract     = {We propose a novel class of graph neural networks based on the discretised Beltrami flow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with positional encodings derived from the graph topology and jointly evolved by the Beltrami flow, producing simultaneously continuous feature learning and topology evolution. The resulting model generalises many popular graph neural networks and achieves state-of-the-art results on several benchmarks.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Read},
	annote       = {Comment: 21 pages, 5 figures. Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS) 2021}
}
@inproceedings{tong_trajectorynet_2020,
	title        = {{TrajectoryNet}: {A} {Dynamic} {Optimal} {Transport} {Network} for {Modeling} {Cellular} {Dynamics}},
	shorttitle   = {{TrajectoryNet}},
	author       = {Tong, Alexander and Huang, Jessie and Wolf, Guy and Dijk, David Van and Krishnaswamy, Smita},
	year         = 2020,
	booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {9526--9536},
	abstract     = {It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present {\textbackslash}emph\{TrajectoryNet\}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.},
	keywords     = {Read}
}


@article{chen_neural_2019,
	title        = {Neural {Ordinary} {Differential} {Equations}},
	author       = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	year         = 2019,
	journal      = {arXiv:1806.07366 [cs, stat]},
	abstract     = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing ﬂows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Read}
}
@article{zhuang_ordinary_2019,
	title        = {Ordinary differential equations on graph networks},
	author       = {Zhuang, Juntang and Dvornek, Nicha and Li, Xiaoxiao and Duncan, James S.},
	year         = 2019,
	abstract     = {Apply ordinary differential equation model on graph structured data}
}
@inproceedings{niu_permutation_2020,
	title        = {Permutation {Invariant} {Graph} {Generation} via {Score}-{Based} {Generative} {Modeling}},
	author       = {Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
	year         = 2020,
	booktitle    = {Proceedings of the {Twenty} {Third} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher    = {PMLR},
	pages        = {4474--4484},
	abstract     = {Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.}
}
@inproceedings{song_bridging_2020,
	title        = {Bridging the {Gap} {Between} f-{GANs} and {Wasserstein} {GANs}},
	author       = {Song, Jiaming and Ermon, Stefano},
	year         = 2020,
	booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {9078--9087},
	abstract     = {Generative adversarial networks (GANs) variants approximately minimize divergences between the model and the data distribution using a discriminator. Wasserstein GANs (WGANs) enjoy superior empirical performance, however, unlike in f-GANs, the discriminator does not provide an estimate for the ratio between model and data densities, which is useful in applications such as inverse reinforcement learning. To overcome this limitation, we propose an new training objective where we additionally optimize over a set of importance weights over the generated samples. By suitably constraining the feasible set of importance weights, we obtain a family of objectives which includes and generalizes the original f-GAN and WGAN objectives. We show that a natural extension outperforms WGANs while providing density ratios as in f-GAN, and demonstrate empirical success on distribution modeling, density ratio estimation and image generation.}
}
@article{song_denoising_2021,
	title        = {Denoising {Diffusion} {Implicit} {Models}},
	author       = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
	year         = 2021,
	journal      = {arXiv:2010.02502 [cs]},
	abstract     = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 {\textbackslash}times\$ to \$50 {\textbackslash}times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
	keywords     = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote       = {Comment: ICLR 2021; updated connections with ODEs at page 6}
}
@book{corduneanu_functional_2002,
	title        = {Functional {Equations} with {Causal} {Operators}},
	author       = {Corduneanu, C.},
	year         = 2002,
	publisher    = {CRC Press},
	isbn         = {978-0-203-16637-6},
	abstract     = {Functional equations encompass most of the equations used in applied science and engineering: ordinary differential equations, integral equations of the Volterra type, equations with delayed argument, and integro-differential equations of the Volterra type. The basic theory of functional equations includes functional differential equations with cau},
	keywords     = {Mathematics / Differential Equations / General, Mathematics / Functional Analysis, Science / Physics / Mathematical \& Computational}
}
@article{zecevic_relating_2021-2,
	title        = {Relating {Graph} {Neural} {Networks} to {Structural} {Causal} {Models}},
	author       = {Zečević, Matej and Dhami, Devendra Singh and Veličković, Petar and Kersting, Kristian},
	year         = 2021,
	journal      = {arXiv:2109.04173 [cs, stat]},
	abstract     = {Causality can be described in terms of a structural causal model (SCM) that carries information on the variables of interest and their mechanistic relations. For most processes of interest the underlying SCM will only be partially observable, thus causal inference tries leveraging the exposed. Graph neural networks (GNN) as universal approximators on structured input pose a viable candidate for causal learning, suggesting a tighter integration with SCM. To this effect we present a theoretical analysis from first principles that establishes a more general view on neural-causal models, revealing several novel connections between GNN and SCM. We establish a new model class for GNN-based causal inference that is necessary and sufficient for causal effect identification. Our empirical illustration on simulations and standard benchmarks validate our theoretical proofs.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Main paper: 12 pages, References: 2 pages, Appendix: 13 pages; Main paper: 4 figures, Appendix: 2 figures}
}
@article{liu_graph_2019,
	title        = {Graph {Normalizing} {Flows}},
	author       = {Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
	year         = 2019,
	journal      = {arXiv:1905.13177 [cs, stat]},
	abstract     = {We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
@article{deng_continuous_2019,
	title        = {Continuous {Graph} {Flow}},
	author       = {Deng, Zhiwei and Nawhal, Megha and Meng, Lili and Mori, Greg},
	year         = 2019,
	journal      = {arXiv:1908.02436 [cs, stat]},
	abstract     = {In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data. Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs. This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to state-of-the-art models.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Read}
}
@article{battaglia_relational_2018,
	title        = {Relational inductive biases, deep learning, and graph networks},
	author       = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	year         = 2018,
	journal      = {arXiv:1806.01261 [cs, stat]},
	abstract     = {Artiﬁcial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have ﬁt the natural strengths of deep learning. However, many deﬁning characteristics of human intelligence, which developed under much diﬀerent pressures, remain out of reach for current approaches. In particular, generalizing beyond one’s experiences—a hallmark of human intelligence from infancy—remains a formidable challenge for modern AI.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}
@article{brody_how_2021,
	title        = {How {Attentive} are {Graph} {Attention} {Networks}?},
	author       = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	year         = 2021,
	journal      = {arXiv:2105.14491 [cs]},
	abstract     = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how\_attentive\_are\_gats , and GATv2 is available as part of the PyTorch Geometric library.},
	keywords     = {Computer Science - Machine Learning}
}
@misc{noauthor_colab_nodate,
	title        = {Colab {Notebooks} and {Video} {Tutorials} — pytorch\_geometric 2.0.2 documentation}
}
@article{freidlin_diffusion_2000,
	title        = {Diffusion processes on graphs: stochastic differential equations, large deviation principle},
	shorttitle   = {Diffusion processes on graphs},
	author       = {Freidlin, Mark and Sheu, Shuenn-Jyi},
	year         = 2000,
	journal      = {Probab Theory Relat Fields},
	volume       = 116,
	number       = 2,
	pages        = {181--220},
	abstract     = {Ito's rule is established for the diffusion processes on the graphs. We also consider a family of diffusions processes with small noise on a graph. Large deviation principle is proved for these diffusion processes and their local times at the vertices.}
}
@article{kipf_semi-supervised_2017,
	title        = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	author       = {Kipf, Thomas N. and Welling, Max},
	year         = 2017,
	journal      = {arXiv:1609.02907 [cs, stat]},
	abstract     = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efﬁcient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized ﬁrst-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a signiﬁcant margin.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Published as a conference paper at ICLR 2017}
}
@inproceedings{sochen_stochastic_2001,
	title        = {Stochastic processes in vision: from {Langevin} to {Beltrami}},
	shorttitle   = {Stochastic processes in vision},
	author       = {Sochen, N.A.},
	year         = 2001,
	booktitle    = {Proceedings {Eighth} {IEEE} {International} {Conference} on {Computer} {Vision}. {ICCV} 2001},
	publisher    = {IEEE Comput. Soc},
	address      = {Vancouver, BC, Canada},
	volume       = 1,
	pages        = {288--293},
	isbn         = {978-0-7695-1143-6}
}
@article{monmarche_kinetic_2020,
	title        = {Kinetic walks for sampling},
	author       = {Monmarché, Pierre},
	year         = 2020,
	journal      = {arXiv:1903.00550 [math]},
	abstract     = {The persistent walk is a classical model in kinetic theory, which has also been studied as a toy model for MCMC questions. Its continuous limit, the telegraph process, has recently been extended to various velocity jump processes (Bouncy Particle Sampler, Zig-Zag process, etc.) in order to sample general target distributions on \${\textbackslash}mathbb R{\textasciicircum}d\$. This paper studies, from a sampling point of view, general kinetic walks that are natural discrete-time (and possibly discrete-space) counterparts of these continuous-space processes. The main contributions of the paper are the definition and study of a discrete-space Zig-Zag sampler and the definition and time-discretisation of hybrid jump/diffusion kinetic samplers for multi-scale potentials on \${\textbackslash}mathbb R{\textasciicircum}d\$.},
	keywords     = {Mathematics - Probability, 60J10, 65C40}
}
@article{chamberlain_grand_2021,
	title        = {{GRAND}: {Graph} {Neural} {Diffusion}},
	shorttitle   = {{GRAND}},
	author       = {Chamberlain, Benjamin Paul and Rowbottom, James and Gorinova, Maria and Webb, Stefan and Rossi, Emanuele and Bronstein, Michael M.},
	year         = 2021,
	journal      = {arXiv:2106.10934 [cs, stat]},
	abstract     = {We present Graph Neural Diffusion (GRAND) that approaches deep learning on graphs as a continuous diffusion process and treats Graph Neural Networks (GNNs) as discretisations of an underlying PDE. In our model, the layer structure and topology correspond to the discretisation choices of temporal and spatial operators. Our approach allows a principled development of a broad new class of GNNs that are able to address the common plights of graph learning models such as depth, oversmoothing, and bottlenecks. Key to the success of our models are stability with respect to perturbations in the data and this is addressed for both implicit and explicit discretisation schemes. We develop linear and nonlinear versions of GRAND, which achieve competitive results on many standard graph benchmarks.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: 15 pages, 4 figures. Proceedings of the 38th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s)}
}
@book{wu_daily_nodate,
	title        = {Daily {Coding}},
	author       = {Wu, Lawrence and MIller, Alex}
}
@misc{noauthor_leetcode_nodate,
	title        = {{LeetCode} - {The} {World}'s {Leading} {Online} {Programming} {Learning} {Platform}},
	abstract     = {Level up your coding skills and quickly land a job. This is the best place to expand your knowledge and get prepared for your next interview.}
}
@inproceedings{tierny_topology_2007,
	title        = {Topology driven {3D} mesh hierarchical segmentation},
	author       = {Tierny, Julien and Vandeborre, Jean-Philippe and Daoudi, Mohamed},
	year         = 2007,
	booktitle    = {{IEEE} {International} {Conference} on {Shape} {Modeling} and {Applications} 2007 ({SMI} '07)},
	pages        = {215--220},
	abstract     = {In this paper, we propose to address the semantic- oriented 3D mesh hierarchical segmentation problem, using enhanced topological skeletons. This high level information drives both the feature boundary computation as well as the feature hierarchy definition. Proposed hierarchical scheme is based on the key idea that the topology of a feature is a more important decomposition criterion than its geometry. First, the enhanced topological skeleton of the input triangulated surface is constructed. Then it is used to delimit the core of the object and to identify junction areas. This second step results in a fine segmentation of the object. Finally, a fine to coarse strategy enables a semantic- oriented hierarchical composition of features, subdividing human limbs into arms and hands for example. Method performance is evaluated according to seven criteria enumerated in latest segmentation surveys [3]. Thanks to the high level description it uses as an input, presented approach results, with low computation times, in robust and meaningful compatible hierarchical decompositions.},
	keywords     = {Arm, Deformable models, Geometry, Humans, Level measurement, Robustness, Shape, Skeleton, Telecommunication computing, Topology}
}
@article{weed_sharp_2017,
	title        = {Sharp asymptotic and finite-sample rates of convergence of empirical measures in {Wasserstein} distance},
	author       = {Weed, Jonathan and Bach, Francis},
	year         = 2017,
	journal      = {arXiv:1707.00087 [math, stat]},
	abstract     = {The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from \$n\$ independent samples from \${\textbackslash}mu\$ approaches \${\textbackslash}mu\$ in the Wasserstein distance of any order. We prove sharp asymptotic and finite-sample results for this rate of convergence for general measures on general compact metric spaces. Our finite-sample results show the existence of multi-scale behavior, where measures can exhibit radically different rates of convergence as \$n\$ grows.},
	keywords     = {Mathematics - Statistics Theory, Mathematics - Probability, 60B10, 62E17}
}
@article{sato_survey_2020,
	title        = {A {Survey} on {The} {Expressive} {Power} of {Graph} {Neural} {Networks}},
	author       = {Sato, Ryoma},
	year         = 2020,
	journal      = {arXiv:2003.04078 [cs, stat]},
	abstract     = {Graph neural networks (GNNs) are effective machine learning models for various graph learning problems. Despite their empirical successes, the theoretical limitations of GNNs have been revealed recently. Consequently, many GNN models have been proposed to overcome these limitations. In this survey, we provide a comprehensive overview of the expressive power of GNNs and provably powerful variants of GNNs.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: 42 pages}
}
@book{noauthor_pytorch_nodate,
	title        = {Pytorch}
}
@article{poli_continuous-depth_2021,
	title        = {Continuous-{Depth} {Neural} {Models} for {Dynamic} {Graph} {Prediction}},
	author       = {Poli, Michael and Massaroli, Stefano and Rabideau, Clayton M. and Park, Junyoung and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
	year         = 2021,
	journal      = {arXiv:2106.11581 [cs, stat]},
	abstract     = {We introduce the framework of continuous-depth graph neural networks (GNNs). Neural graph differential equations (Neural GDEs) are formalized as the counterpart to GNNs where the input-output relationship is determined by a continuum of GNN layers, blending discrete topological structures and differential equations. The proposed framework is shown to be compatible with static GNN models and is extended to dynamic and stochastic settings through hybrid dynamical system theory. Here, Neural GDEs improve performance by exploiting the underlying dynamics geometry, further introducing the ability to accommodate irregularly sampled data. Results prove the effectiveness of the proposed models across applications, such as traffic forecasting or prediction in genetic regulatory networks.},
	keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote       = {Comment: Extended version of the workshop paper "Graph Neural Ordinary Differential Equations". arXiv admin note: substantial text overlap with arXiv:1911.07532}
}
@book{alon_introduction_nodate,
	title        = {An introduction to system biology},
	author       = {Alon, Uri}
}
@book{van_der_schaft_introduction_2000,
	title        = {An introduction to hybrid dynamical systems},
	author       = {van der Schaft, Arjan and Schumacher, Hans},
	year         = 2000,
	publisher    = {Springer-Verlag London},
	isbn         = {978-1-85233-233-4 1-85233-233-6}
}
@misc{noauthor_cytotrace_nodate,
	title        = {{CytoTRACE}}
}
@misc{compsysbiocancer_institut_curie_single_2020,
	title        = {Single cell trajectory reconstruction: ideas, methods and problems},
	shorttitle   = {Single cell trajectory reconstruction},
	author       = {{CompSysBioCancer, Institut Curie}},
	year         = 2020,
	abstract     = {A short talk made by Andrei Zinovyev at OPEN single cell data analysis club}
}
@misc{khanacademymedicine_cellular_2015,
	title        = {Cellular specialization (differentiation) {\textbar} {Cells} {\textbar} {MCAT} {\textbar} {Khan} {Academy}},
	author       = {{khanacademymedicine}},
	year         = 2015,
	abstract     = {
		Visit us (http://www.khanacademy.org/science/he...) for health and medicine content or (http://www.khanacademy.org/test-prep/...) for MCAT related content. These videos do not provide medical advice and are for informational purposes only. The videos are not intended to be a substitute for professional medical advice, diagnosis or treatment. Always seek the advice of a qualified health provider with any questions you may have regarding a medical condition. Never disregard professional medical advice or delay in seeking it because of something you have read or seen in any Khan Academy video. Created by Vishal Punwani.

		Watch the next lesson: https://www.khanacademy.org/test-prep...

		Missed the previous lesson? https://www.khanacademy.org/test-prep...

		MCAT on Khan Academy: Go ahead and practice some passage-based questions!

		About Khan Academy: Khan Academy offers practice exercises, instructional videos, and a personalized learning dashboard that empower learners to study at their own pace in and outside of the classroom. We tackle math, science, computer programming, history, art history, economics, and more. Our math missions guide learners from kindergarten to calculus using state-of-the-art, adaptive technology that identifies strengths and learning gaps. We've also partnered with institutions like NASA, The Museum of Modern Art, The California Academy of Sciences, and MIT to offer specialized content.

		For free. For everyone. Forever. \#YouCanLearnAnything

		Subscribe to Khan Academy’s MCAT channel: https://www.youtube.com/channel/UCDkK... Subscribe to Khan Academy: https://www.youtube.com/subscription\_...
	}
}
@misc{compsysbiocancer_institut_curie_single_2020-1,
	title        = {Single cell trajectory reconstruction: ideas, methods and problems},
	shorttitle   = {Single cell trajectory reconstruction},
	author       = {{CompSysBioCancer, Institut Curie}},
	year         = 2020,
	abstract     = {A short talk made by Andrei Zinovyev at OPEN single cell data analysis club}
}
@techreport{saelens_comparison_2018,
	title        = {A comparison of single-cell trajectory inference methods: towards more accurate and robust tools},
	shorttitle   = {A comparison of single-cell trajectory inference methods},
	author       = {Saelens, Wouter and Cannoodt, Robrecht and Todorov, Helena and Saeys, Yvan},
	year         = 2018,
	type         = {preprint},
	abstract     = {Using single-cell -omics data, it is now possible to computationally order cells along trajectories, allowing the unbiased study of cellular dynamic processes. Since 2014, more than 50 trajectory inference methods have been developed, each with its own set of methodological characteristics. As a result, choosing a method to infer trajectories is often challenging, since a comprehensive assessment of the performance and robustness of each method is still lacking. In order to facilitate the comparison of the results of these methods to each other and to a gold standard, we developed a global framework to benchmark trajectory inference tools. Using this framework, we compared the trajectories from a total of 29 trajectory inference methods, on a large collection of real and synthetic datasets. We evaluate methods using several metrics, including accuracy of the inferred ordering, correctness of the network topology, code quality and user friendliness. We found that some methods, including Slingshot, TSCAN and Monocle DDRTree, clearly outperform other methods, although their performance depended on the type of trajectory present in the data. Based on our benchmarking results, we therefore developed a set of guidelines for method users. However, our analysis also indicated that there is still a lot of room for improvement, especially for methods detecting complex trajectory topologies. Our evaluation pipeline can therefore be used to spearhead the development of new scalable and more accurate methods, and is available at github.com/dynverse/dynverse.},
	institution  = {Bioinformatics}
}
@article{chen_single-cell_2019,
	title        = {Single-cell trajectories reconstruction, exploration and mapping of omics data with {STREAM}},
	author       = {Chen, Huidong and Albergante, Luca and Hsu, Jonathan Y. and Lareau, Caleb A. and Lo Bosco, Giosuè and Guan, Jihong and Zhou, Shuigeng and Gorban, Alexander N. and Bauer, Daniel E. and Aryee, Martin J. and Langenau, David M. and Zinovyev, Andrei and Buenrostro, Jason D. and Yuan, Guo-Cheng and Pinello, Luca},
	year         = 2019,
	journal      = {Nat Commun},
	volume       = 10,
	number       = 1,
	pages        = 1903
}
@article{diaconis_group_1988,
	title        = {Group {Representations} in {Probability} and {Statistics}},
	author       = {Diaconis, Persi},
	year         = 1988,
	journal      = {Lecture Notes-Monograph Series},
	volume       = 11,
	pages        = {i--192}
}
@misc{jack_kang_stat115_2021,
	title        = {{STAT115} {Chapter} 20.3 {Pseudo} time and {RNA} velocity},
	author       = {{Jack Kang}},
	year         = 2021
}
@inproceedings{hofer_graph_2020,
	title        = {Graph {Filtration} {Learning}},
	author       = {Hofer, Christoph and Graf, Florian and Rieck, Bastian and Niethammer, Marc and Kwitt, Roland},
	year         = 2020,
	booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {4314--4323},
	abstract     = {We propose an approach to learning with graph-structured data in the problem domain of graph classification. In particular, we present a novel type of readout operation to aggregate node features into a graph-level representation. To this end, we leverage persistent homology computed via a real-valued, learnable, filter function. We establish the theoretical foundation for differentiating through the persistent homology computation. Empirically, we show that this type of readout operation compares favorably to previous techniques, especially when the graph connectivity structure is informative for the learning problem.}
}
@article{horn_topological_2021,
	title        = {Topological {Graph} {Neural} {Networks}},
	author       = {Horn, Max and De Brouwer, Edward and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
	year         = 2021,
	journal      = {arXiv:2102.07835 [cs, math, stat]},
	abstract     = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive in terms of the Weisfeiler-Lehman graph isomorphism test. Augmenting GNNs with our layer leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets (which can be classified by humans using their topology but not by ordinary GNNs) and on real-world data.},
	keywords     = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Algebraic Topology}
}
@article{howick_malaria_2019,
	title        = {The {Malaria} {Cell} {Atlas}: {Single} parasite transcriptomes across the complete {Plasmodium} life cycle},
	shorttitle   = {The {Malaria} {Cell} {Atlas}},
	author       = {Howick, Virginia M. and Russell, Andrew J. C. and Andrews, Tallulah and Heaton, Haynes and Reid, Adam J. and Natarajan, Kedar and Butungi, Hellen and Metcalf, Tom and Verzier, Lisa H. and Rayner, Julian C. and Berriman, Matthew and Herren, Jeremy K. and Billker, Oliver and Hemberg, Martin and Talman, Arthur M. and Lawniczak, Mara K. N.},
	year         = 2019,
	journal      = {Science},
	volume       = 365,
	number       = 6455,
	pages        = {eaaw2619},
	abstract     = {Malaria parasites adopt a remarkable variety of morphological life stages as they transition through multiple mammalian host and mosquito vector environments. We profiled the single-cell transcriptomes of thousands of individual parasites, deriving the first high-resolution transcriptional atlas of the entire Plasmodium berghei life cycle. We then used our atlas to precisely define developmental stages of single cells from three different human malaria parasite species, including parasites isolated directly from infected individuals. The Malaria Cell Atlas provides both a comprehensive view of gene usage in a eukaryotic parasite and an open-access reference dataset for the study of malaria parasites.},
	pmid         = 31439762,
	pmcid        = {PMC7056351}
}
@article{la_manno_rna_2018,
	title        = {{RNA} velocity of single cells},
	author       = {La Manno, Gioele and Soldatov, Ruslan and Zeisel, Amit and Braun, Emelie and Hochgerner, Hannah and Petukhov, Viktor and Lidschreiber, Katja and Kastriti, Maria E. and Lönnerberg, Peter and Furlan, Alessandro and Fan, Jean and Borm, Lars E. and Liu, Zehua and van Bruggen, David and Guo, Jimin and He, Xiaoling and Barker, Roger and Sundström, Erik and Castelo-Branco, Gonçalo and Cramer, Patrick and Adameyko, Igor and Linnarsson, Sten and Kharchenko, Peter V.},
	year         = 2018,
	journal      = {Nature},
	volume       = 560,
	number       = 7719,
	pages        = {494--498},
	copyright    = {2018 Springer Nature Limited},
	abstract     = {RNA abundance is a powerful indicator of the state of individual cells. Single-cell RNA sequencing can reveal RNA abundance with high quantitative accuracy, sensitivity and throughput1. However, this approach captures only a static snapshot at a point in time, posing a challenge for the analysis of time-resolved phenomena such as embryogenesis or tissue regeneration. Here we show that RNA velocity—the time derivative of the gene expression state—can be directly estimated by distinguishing between unspliced and spliced mRNAs in common single-cell RNA sequencing protocols. RNA velocity is a high-dimensional vector that predicts the future state of individual cells on a timescale of hours. We validate its accuracy in the neural crest lineage, demonstrate its use on multiple published datasets and technical platforms, reveal the branching lineage tree of the developing mouse hippocampus, and examine the kinetics of transcription in human embryonic brain. We expect RNA velocity to greatly aid the analysis of developmental lineages and cellular dynamics, particularly in humans.},
	keywords     = {Differentiation, Genome informatics}
}
@misc{chipster_tutorials_10_2019,
	title        = {10. {Trajectory} inference analysis of {scRNA}-seq data},
	author       = {{Chipster Tutorials}},
	year         = 2019,
	abstract     = {This lecture by Paulo Czarnewski (NBIS, ELIXIR-SE) is part of the course "Single cell RNA-seq data analysis with R" (27.-29.5.2019). Please see https://www.csc.fi/web/training/-/scr... for the full course description and all the materials.},
	annote       = {Also includes the RNA velocity}
}
@misc{noauthor_scrnaseq_nodate,
	title        = {scrnaseq - {Training} - {CSC} {Company} {Site}}
}
@misc{broad_institute_mia_2019,
	title        = {{MIA}: {Volker} {Bergen}, {RNA} velocity generalized to transient cell states through dynamical modeling},
	shorttitle   = {{MIA}},
	author       = {{Broad Institute}},
	year         = 2019,
	abstract     = {
		Models, Inference and Algorithms December 6, 2019

		MIA Meeting: https://www.youtube.com/watch?v=QWfd\_...

		Volker Bergen Theis Lab, Institute of Computational Biology

		RNA velocity generalized to transient cell states through dynamical modeling

		The introduction of RNA velocity in single cells has opened up new ways of studying cellular differentiation. The originally proposed framework obtains velocities as the deviation of the observed ratio of spliced and unspliced mRNA from an inferred steady state. Errors in velocity estimates arise if the central assumptions of a common splicing rate and the observation of the full splicing dynamics with steady-state mRNA levels are violated. With scVelo (https://scvelo.org), we address these restrictions by solving the full transcriptional dynamics of splicing kinetics using a likelihood-based dynamical model. This generalizes RNA velocity to a wide variety of systems comprising transient cell states, which are common in development and in response to perturbations. We infer gene-specific rates of transcription, splicing and degradation, and recover the latent time of the underlying cellular processes. This latent time represents the cell’s internal clock and is based only on its transcriptional dynamics. Moreover, scVelo allows us to identify regimes of regulatory changes such as stages of cell fate commitment and, therein, systematically detects putative driver genes. We demonstrate that scVelo enables disentangling heterogeneous subpopulation kinetics with unprecedented resolution in hippocampal dentate gyrus neurogenesis and pancreatic endocrinogenesis. We anticipate that scVelo will greatly facilitate the study of lineage decisions, gene regulation, and pathway activity identification.

		For more information on the Broad Institute and MIA visit: http://www.broadinstitute.org/MIA

		Copyright Broad Institute, 2019. All rights reserved.
	}
}
@article{bergen_generalizing_2020,
	title        = {Generalizing {RNA} velocity to transient cell states through dynamical modeling},
	author       = {Bergen, Volker and Lange, Marius and Peidli, Stefan and Wolf, F. Alexander and Theis, Fabian J.},
	year         = 2020,
	journal      = {Nat Biotechnol},
	volume       = 38,
	number       = 12,
	pages        = {1408--1414},
	copyright    = {2020 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	abstract     = {RNA velocity has opened up new ways of studying cellular differentiation in single-cell RNA-sequencing data. It describes the rate of gene expression change for an individual gene at a given time point based on the ratio of its spliced and unspliced messenger RNA (mRNA). However, errors in velocity estimates arise if the central assumptions of a common splicing rate and the observation of the full splicing dynamics with steady-state mRNA levels are violated. Here we present scVelo, a method that overcomes these limitations by solving the full transcriptional dynamics of splicing kinetics using a likelihood-based dynamical model. This generalizes RNA velocity to systems with transient cell states, which are common in development and in response to perturbations. We apply scVelo to disentangling subpopulation kinetics in neurogenesis and pancreatic endocrinogenesis. We infer gene-specific rates of transcription, splicing and degradation, recover each cell’s position in the underlying differentiation processes and detect putative driver genes. scVelo will facilitate the study of lineage decisions and gene regulation.},
	keywords     = {Computational models, Machine learning, Biochemical reaction networks, Computational biology and bioinformatics, RNA metabolism}
}
@article{bergen_rna_2021,
	title        = {{RNA} velocity—current challenges and future perspectives},
	author       = {Bergen, Volker and Soldatov, Ruslan A. and Kharchenko, Peter V. and Theis, Fabian J.},
	year         = 2021,
	journal      = {Molecular Systems Biology},
	volume       = 17,
	number       = 8,
	pages        = {e10282},
	abstract     = {This Review discusses the emerging challenges and potential pitfalls of current RNA velocity modeling approaches and provides guidance on how to address them.}
}
@article{zeisel_coupled_2011,
	title        = {Coupled pre-{mRNA} and {mRNA} dynamics unveil operational strategies underlying transcriptional responses to stimuli},
	author       = {Zeisel, Amit and Köstler, Wolfgang J. and Molotski, Natali and Tsai, Jonathan M. and Krauthgamer, Rita and Jacob-Hirsch, Jasmine and Rechavi, Gideon and Soen, Yoav and Jung, Steffen and Yarden, Yosef and Domany, Eytan},
	year         = 2011,
	journal      = {Molecular Systems Biology},
	volume       = 7,
	number       = 1,
	pages        = 529,
	abstract     = {Genome-wide simultaneous measurements of pre-mRNA and mRNA expression reveal unexpected time-dependent transcript production and degradation profiles in response to external stimulus, as well as a st...}
}
@article{coifman_diffusion_2006,
	title        = {Diffusion maps},
	author       = {Coifman, Ronald R. and Lafon, Stéphane},
	year         = 2006,
	journal      = {Applied and Computational Harmonic Analysis},
	volume       = 21,
	number       = 1,
	pages        = {5--30},
	abstract     = {In this paper, we provide a framework based upon diffusion processes for ﬁnding meaningful geometric descriptions of data sets. We show that eigenfunctions of Markov matrices can be used to construct coordinates called diffusion maps that generate efﬁcient representations of complex geometric structures. The associated family of diffusion distances, obtained by iterating the Markov matrix, deﬁnes multiscale geometries that prove to be useful in the context of data parametrization and dimensionality reduction. The proposed framework relates the spectral properties of Markov processes to their geometric counterparts and it uniﬁes ideas arising in a variety of contexts such as machine learning, spectral graph theory and eigenmap methods.}
}
@article{jang_dynamics_2017,
	title        = {Dynamics of embryonic stem cell differentiation inferred from single-cell transcriptomics show a series of transitions through discrete cell states},
	author       = {Jang, Sumin and Choubey, Sandeep and Furchtgott, Leon and Zou, Ling-Nan and Doyle, Adele and Menon, Vilas and Loew, Ethan B and Krostag, Anne-Rachel and Martinez, Refugio A and Madisen, Linda and Levi, Boaz P and Ramanathan, Sharad},
	year         = 2017,
	journal      = {eLife},
	volume       = 6,
	pages        = {e20487},
	abstract     = {The complexity of gene regulatory networks that lead multipotent cells to acquire different cell fates makes a quantitative understanding of differentiation challenging. Using a statistical framework to analyze single-cell transcriptomics data, we infer the gene expression dynamics of early mouse embryonic stem (mES) cell differentiation, uncovering discrete transitions across nine cell states. We validate the predicted transitions across discrete states using flow cytometry. Moreover, using live-cell microscopy, we show that individual cells undergo abrupt transitions from a naïve to primed pluripotent state. Using the inferred discrete cell states to build a probabilistic model for the underlying gene regulatory network, we further predict and experimentally verify that these states have unique response to perturbations, thus defining them functionally. Our study provides a framework to infer the dynamics of differentiation from single cell transcriptomics data and to build predictive models of the gene regulatory networks that drive the sequence of cell fate decisions during development.},
	editor       = {Yosef, Nir},
	keywords     = {context dependence, germ layer differentiation, Single-cell RNA-seq}
}
@article{jahnke_solving_2006,
	title        = {Solving the chemical master equation for monomolecular reaction systems analytically},
	author       = {Jahnke, Tobias and Huisinga, Wilhelm},
	year         = 2006,
	journal      = {J. Math. Biol.},
	volume       = 54,
	number       = 1,
	pages        = {1--26},
	abstract     = {The stochastic dynamics of a well-stirred mixture of molecular species interacting through different biochemical reactions can be accurately modelled by the chemical master equation (CME). Research in the biology and scientiﬁc computing community has concentrated mostly on the development of numerical techniques to approximate the solution of the CME via many realizations of the associated Markov jump process. The domain of exact and/or efﬁcient methods for directly solving the CME is still widely open, which is due to its large dimension that grows exponentially with the number of molecular species involved. In this article, we present an exact solution formula of the CME for arbitrary initial conditions in the case where the underlying system is governed by monomolecular reactions. The solution can be expressed in terms of the convolution of multinomial and product Poisson distributions with time-dependent parameters evolving according to the traditional reaction-rate equations. This very structured representation allows to deduce easily many properties of the solution. The model class includes many interesting examples. For more complex reaction systems, our results can be seen as a ﬁrst step towards the construction of new numerical integrators, because solutions to the monomolecular case provide promising ansatz functions for Galerkin-type methods.}
}
@misc{swain_lecture_nodate,
	title        = {Lecture notes on stochastic models in systems biology},
	author       = {Swain, Peter}
}
@article{cao_sci-fate_2020,
	title        = {Sci-fate characterizes the dynamics of gene expression in single cells},
	author       = {Cao, Junyue and Zhou, Wei and Steemers, Frank and Trapnell, Cole and Shendure, Jay},
	year         = 2020,
	journal      = {Nat Biotechnol},
	volume       = 38,
	number       = 8,
	pages        = {980--988}
}
@article{moon_visualizing_2019,
	title        = {Visualizing structure and transitions in high-dimensional biological data},
	author       = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and Elzen, Antonia van den and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
	year         = 2019,
	journal      = {Nat Biotechnol},
	volume       = 37,
	number       = 12,
	pages        = {1482--1492},
	copyright    = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	abstract     = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
	keywords     = {Machine learning, Data mining}
}
@misc{noauthor_gene_nodate,
	title        = {Gene {Expression} {\textbar} {Learn} {Science} at {Scitable}},
	abstract     = {In multicellular organisms, nearly all cells have the same DNA, but different cell types express distinct proteins. Learn how cells adjust these proteins to produce their unique identities.}
}
@misc{noauthor_chromatin_nodate,
	title        = {Chromatin {Potential} {Identified} by {Shared} {Single}-{Cell} {Profiling} of {RNA} and {Chromatin} {\textbar} {Elsevier} {Enhanced} {Reader}}
}
@misc{oxford_academic_oxford_university_press_transcription_2014,
	title        = {Transcription regulation},
	author       = {{Oxford Academic (Oxford University Press)}},
	year         = 2014,
	abstract     = {
		In this animation, we explore the ways in which the expression of genes can be regulated.

		http://ukcatalogue.oup.com/product/97...
	},
	annote       = {recommanded by Jessie}
}
@article{elowitz_synthetic_2000,
	title        = {A synthetic oscillatory network of transcriptional regulators},
	author       = {Elowitz, Michael B. and Leibler, Stanislas},
	year         = 2000,
	journal      = {Nature},
	volume       = 403,
	number       = 6767,
	pages        = {335--338},
	copyright    = {2000 Macmillan Magazines Ltd.},
	abstract     = {Networks of interacting biomolecules carry out many essential functions in living cells1, but the ‘design principles’ underlying the functioning of such intracellular networks remain poorly understood, despite intensive efforts including quantitative analysis of relatively simple systems2. Here we present a complementary approach to this problem: the design and construction of a synthetic network to implement a particular function. We used three transcriptional repressor systems that are not part of any natural biological clock3,4,5 to build an oscillating network, termed the repressilator, in Escherichia coli. The network periodically induces the synthesis of green fluorescent protein as a readout of its state in individual cells. The resulting oscillations, with typical periods of hours, are slower than the cell-division cycle, so the state of the oscillator has to be transmitted from generation to generation. This artificial clock displays noisy behaviour, possibly because of stochastic fluctuations of its components. Such ‘rational network design’ may lead both to the engineering of new cellular behaviours and to an improved understanding of naturally occurring networks.}
}
@misc{noauthor_adaptive_nodate,
	title        = {An adaptive tau-leaping method for stochastic simulations of reaction-diffusion systems: {AIP} {Advances}: {Vol} 6, {No} 3},
	annote       = {Used to simulate the data in Poli et al. 2021 experiment 4.3 about the repressilator reconstruction}
}
@article{shuman_emerging_2013,
	title        = {The emerging field of signal processing on graphs: {Extending} high-dimensional data analysis to networks and other irregular domains},
	shorttitle   = {The emerging field of signal processing on graphs},
	author       = {Shuman, David I and Narang, Sunil K. and Frossard, Pascal and Ortega, Antonio and Vandergheynst, Pierre},
	year         = 2013,
	journal      = {IEEE Signal Processing Magazine},
	volume       = 30,
	number       = 3,
	pages        = {83--98},
	abstract     = {In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogs to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions.}
}
@article{lee_neural_2020,
	title        = {A {Neural} {Dirichlet} {Process} {Mixture} {Model} for {Task}-{Free} {Continual} {Learning}},
	author       = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
	year         = 2020,
	journal      = {arXiv:2001.00689 [cs, stat]},
	abstract     = {Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.},
	annote       = {Comment: Accepted as a conference paper at ICLR 2020}
}
@techreport{soelistyo_learning_2021,
	title        = {Learning the rules of cell competition without prior scientific knowledge},
	author       = {Soelistyo, Christopher J. and Vallardi, Giulia and Charras, Guillaume and Lowe, Alan R.},
	year         = 2021,
	pages        = {2021.11.24.469554},
	copyright    = {© 2021, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	abstract     = {Deep learning is now a powerful tool in microscopy data analysis, and is routinely used for image processing applications such as segmentation and denoising. However, it has rarely been used to directly learn mechanistic models of a biological system, owing to the complexity of the internal representations. Here, we develop an end-to-end machine learning model capable of learning the rules of a complex biological phenomenon, cell competition, directly from a large corpus of time-lapse microscopy data. Cell competition is a quality control mechanism that eliminates unfit cells from a tissue and during which cell fate is thought to be determined by the local cellular neighborhood over time. To investigate this, we developed a new approach (τ-VAE) by coupling a variational autoencoder to a temporal convolution network to predict the fate of each cell in an epithelium. Using the τ-VAE’s latent representation of the local tissue organization and the flow of information in the network, we decode the physical parameters responsible for correct prediction of fate in cell competition. Remarkably, the model autonomously learns that cell density is the single most important factor in predicting cell fate – a conclusion that has taken over a decade of traditional experimental research to reach. Finally, to test the learned internal representation, we challenge the network with experiments performed in the presence of drugs that block signalling pathways involved in competition. We present a novel discriminator network that, using the predictions of the τ-VAE, can identify conditions which deviate from the normal behaviour, paving the way for automated, mechanism-aware drug screening.}
}
@article{peters_causal_2020,
	title        = {Causal models for dynamical systems},
	author       = {Peters, Jonas and Bauer, Stefan and Pfister, Niklas},
	year         = 2020,
	journal      = {arXiv:2001.06208 [math, stat]},
	abstract     = {A probabilistic model describes a system in its observational state. In many situations, however, we are interested in the system's response under interventions. The class of structural causal models provides a language that allows us to model the behaviour under interventions. It can been taken as a starting point to answer a plethora of causal questions, including the identification of causal effects or causal structure learning. In this chapter, we provide a natural and straight-forward extension of this concept to dynamical systems, focusing on continuous time models. In particular, we introduce two types of causal kinetic models that differ in how the randomness enters into the model: it may either be considered as observational noise or as systematic driving noise. In both cases, we define interventions and therefore provide a possible starting point for causal inference. In this sense, the book chapter provides more questions than answers. The focus of the proposed causal kinetic models lies on the dynamics themselves rather than corresponding stationary distributions, for example. We believe that this is beneficial when the aim is to model the full time evolution of the system and data are measured at different time points. Under this focus, it is natural to consider interventions in the differential equations themselves.}
}
@misc{noauthor_fourier_nodate,
	title        = {‪{Fourier} neural operator for parametric partial differential equations‬},
	abstract     = {‪Z Li, N Kovachki, K Azizzadenesheli, B Liu, K Bhattacharya, A Stuart, A Anandkumar‬, ‪arXiv preprint arXiv:2010.08895, 2020‬ - ‪Cité(e) 147 fois‬}
}
@article{li_physics-informed_2021,
	title        = {Physics-{Informed} {Neural} {Operator} for {Learning} {Partial} {Differential} {Equations}},
	author       = {Li, Zongyi and Zheng, Hongkai and Kovachki, Nikola and Jin, David and Chen, Haoxuan and Liu, Burigede and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	year         = 2021,
	journal      = {arXiv:2111.03794 [cs, math]},
	abstract     = {Machine learning methods have recently shown promise in solving partial differential equations (PDEs). They can be classified into two broad categories: approximating the solution function and learning the solution operator. The Physics-Informed Neural Network (PINN) is an example of the former while the Fourier neural operator (FNO) is an example of the latter. Both these approaches have shortcomings. The optimization in PINN is challenging and prone to failure, especially on multi-scale dynamic systems. FNO does not suffer from this optimization issue since it carries out supervised learning on a given dataset, but obtaining such data may be too expensive or infeasible. In this work, we propose the physics-informed neural operator (PINO), where we combine the operating-learning and function-optimization frameworks. This integrated approach improves convergence rates and accuracy over both PINN and FNO models. In the operator-learning phase, PINO learns the solution operator over multiple instances of the parametric PDE family. In the test-time optimization phase, PINO optimizes the pre-trained operator ansatz for the querying instance of the PDE. Experiments show PINO outperforms previous ML methods on many popular PDE families while retaining the extraordinary speed-up of FNO compared to solvers. In particular, PINO accurately solves challenging long temporal transient flows and Kolmogorov flows where other baseline ML methods fail to converge.}
}
@book{noauthor_dl_nodate,
	title        = {{DL} with {Pytorch}}
}
@book{stevens_deep_2020,
	title        = {Deep learning with {PyTorch}},
	author       = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
	year         = 2020,
	publisher    = {Manning Publications Co},
	address      = {Shelter Island, NY},
	isbn         = {978-1-61729-526-3}
}
@article{deng_continuous_2019-1,
	title        = {Continuous {Graph} {Flow}},
	author       = {Deng, Zhiwei and Nawhal, Megha and Meng, Lili and Mori, Greg},
	year         = 2019,
	journal      = {arXiv:1908.02436 [cs, stat]},
	abstract     = {In this paper, we propose Continuous Graph Flow, a generative continuous flow based method that aims to model complex distributions of graph-structured data. Once learned, the model can be applied to an arbitrary graph, defining a probability density over the random variables represented by the graph. It is formulated as an ordinary differential equation system with shared and reusable functions that operate over the graphs. This leads to a new type of neural graph message passing scheme that performs continuous message passing over time. This class of models offers several advantages: a flexible representation that can generalize to variable data dimensions; ability to model dependencies in complex data distributions; reversible and memory-efficient; and exact and efficient computation of the likelihood of the data. We demonstrate the effectiveness of our model on a diverse set of generation tasks across different domains: graph generation, image puzzle generation, and layout generation from scene graphs. Our proposed model achieves significantly better performance compared to state-of-the-art models.}
}
@misc{noauthor_transcription_2021,
	title        = {Transcription factor},
	year         = 2021,
	journal      = {Wikipedia},
	copyright    = {Creative Commons Attribution-ShareAlike License},
	abstract     = {In molecular biology, a transcription factor (TF) (or sequence-specific DNA-binding factor) is a protein that controls the rate of transcription of genetic information from DNA to messenger RNA, by binding to a specific DNA sequence. The function of TFs is to regulate—turn on and off—genes in order to make sure that they are expressed in the right cell at the right time and in the right amount throughout the life of the cell and the organism. Groups of TFs function in a coordinated fashion to direct cell division, cell growth, and cell death throughout life; cell migration and organization (body plan) during embryonic development; and intermittently in response to signals from outside the cell, such as a hormone.  There are up to 1600 TFs in the human genome. Transcription factors are members of the proteome as well as regulome. TFs work alone or with other proteins in a complex, by promoting (as an activator), or blocking (as a repressor) the recruitment of RNA polymerase (the enzyme that performs the transcription of genetic information from DNA to RNA) to specific genes.A defining feature of TFs is that they contain at least one DNA-binding domain (DBD), which attaches to a specific sequence of DNA adjacent to the genes that they regulate.  TFs are grouped into classes based on their DBDs. Other proteins such as coactivators, chromatin remodelers, histone acetyltransferases, histone deacetylases, kinases, and methylases are also essential to gene regulation, but lack DNA-binding domains, and therefore are not TFs.TFs are of interest in medicine because TF mutations can cause specific diseases, and medications can be potentially targeted toward them.}
}
@inproceedings{xhonneux_continuous_2020,
	title        = {Continuous {Graph} {Neural} {Networks}},
	author       = {Xhonneux, Louis-Pascal and Qu, Meng and Tang, Jian},
	year         = 2020,
	booktitle    = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {10432--10441},
	abstract     = {This paper builds on the connection between graph neural networks and traditional dynamical systems. We propose continuous graph neural networks (CGNN), which generalise existing graph neural networks with discrete dynamics in that they can be viewed as a specific discretisation scheme. The key idea is how to characterise the continuous dynamics of node representations, i.e. the derivatives of node representations, w.r.t. time.Inspired by existing diffusion-based methods on graphs (e.g. PageRank and epidemic models on social networks), we define the derivatives as a combination of the current node representations,the representations of neighbors, and the initial values of the nodes. We propose and analyse two possible dynamics on graphs\{—\}including each dimension of node representations (a.k.a. the feature channel) change independently or interact with each other\{—\}both with theoretical justification. The proposed continuous graph neural net-works are robust to over-smoothing and hence allow us to build deeper networks, which in turn are able to capture the long-range dependencies between nodes. Experimental results on the task of node classification demonstrate the effectiveness of our proposed approach over competitive baselines.}
}
@misc{noauthor_workshop_nodate,
	title        = {Workshop},
	journal      = {Krishnaswamy Lab}
}
@misc{noauthor_time_nodate,
	title        = {‪{Time} {Coupled} {Diffusion} {Maps}‬},
	abstract     = {‪NF Marshall, MJ Hirn‬, ‪Applied and Computational Harmonic Analysis, 2016‬ - ‪Cité(e) 31 fois‬}
}
@misc{noauthor_diffusion_nodate,
	title        = {Diffusion maps for changing data {\textbar} {Elsevier} {Enhanced} {Reader}}
}
@article{bilos_neural_2021,
	title        = {Neural {Flows}: {Efficient} {Alternative} to {Neural} {ODEs}},
	shorttitle   = {Neural {Flows}},
	author       = {Biloš, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and Günnemann, Stephan},
	year         = 2021,
	journal      = {arXiv:2110.13040 [cs, math]},
	abstract     = {Neural ordinary differential equations describe how values change in time. This is the reason why they gained importance in modeling sequential data, especially when the observations are made at irregular intervals. In this paper we propose an alternative by directly modeling the solution curves - the flow of an ODE - with a neural network. This immediately eliminates the need for expensive numerical solvers while still maintaining the modeling capability of neural ODEs. We propose several flow architectures suitable for different applications by establishing precise conditions on when a function defines a valid flow. Apart from computational efficiency, we also provide empirical evidence of favorable generalization performance via applications in time series modeling, forecasting, and density estimation.},
	annote       = {Comment: Conference on Neural Information Processing Systems (NeurIPS 2021)}
}
@article{belkin_semi-supervised_2004,
	title        = {Semi-{Supervised} {Learning} on {Riemannian} {Manifolds}},
	author       = {Belkin, Mikhail and Niyogi, Partha},
	year         = 2004,
	journal      = {Machine Learning},
	volume       = 56,
	number       = {1-3},
	pages        = {209--239},
	abstract     = {We consider the general problem of utilizing both labeled and unlabeled data to improve classiﬁcation accuracy. Under the assumption that the data lie on a submanifold in a high dimensional space, we develop an algorithmic framework to classify a partially labeled data set in a principled manner. The central idea of our approach is that classiﬁcation functions are naturally deﬁned only on the submanifold in question rather than the total ambient space. Using the Laplace-Beltrami operator one produces a basis (the Laplacian Eigenmaps) for a Hilbert space of square integrable functions on the submanifold. To recover such a basis, only unlabeled examples are required. Once such a basis is obtained, training can be performed using the labeled data set.}
}
@book{ma_manifold_nodate,
	title        = {Manifold learning, theory and application},
	author       = {Ma, Y. and Fu, Y.}
}
@article{packer_lineage-resolved_2019,
	title        = {A lineage-resolved molecular atlas of \textit{{C}. elegans} embryogenesis at single-cell resolution},
	author       = {Packer, Jonathan S. and Zhu, Qin and Huynh, Chau and Sivaramakrishnan, Priya and Preston, Elicia and Dueck, Hannah and Stefanik, Derek and Tan, Kai and Trapnell, Cole and Kim, Junhyong and Waterston, Robert H. and Murray, John I.},
	year         = 2019,
	journal      = {Science},
	volume       = 365,
	number       = 6459,
	pages        = {eaax1971}
}
@misc{da_veiga_beltrame_packer_2021,
	title        = {Packer et al 2019 {scRNAseq} dataset wrangled into standard {WormBase} anndata- 89k cells profiled with 10xv2 across multiple timepoints of development},
	author       = {Da Veiga Beltrame, Eduardo},
	year         = 2021,
	publisher    = {CaltechDATA},
	copyright    = {Creative Commons Zero v1.0 Universal},
	annote       = {Other This is part of a curated collection of all C. elegans single cell RNA sequencing high throughput data wrangled into the anndata format in .h5ad files with standard fields, plus any number of optional fields that vary depending on the metadata the authors provide. As possible, we attempt to keep the field names lower case, short, descriptive, and only using valid Python variable names so they may be accessed via the syntax adata.var.field\_name For the convention used to wrangle the h5ad files see https://github.com/WormBase/single-cell/blob/main/data\_wrangling\_convention.md Notebook used to wrangle the data: https://github.com/WormBase/wormcells-notebooks/blob/main/wormcells\_wrangle\_packer2019\_h5ad.ipynb Original study: A lineage-resolved molecular atlas of C. elegans embryogenesis at single-cell resolution Packer, Jonathan S. and Zhu, Qin and Huynh, Chau and Sivaramakrishnan, Priya and Preston, Elicia and Dueck, Hannah and Stefanik, Derek and Tan, Kai and Trapnell, Cole and Kim, Junhyong and Waterston, Robert H. and Murray, John I. Science 20 Sep 2019: Vol. 365, Issue 6459, eaax1971 DOI: 10.1126/science.aax1971 https://science.sciencemag.org/content/365/6459/eaax1971.editor-summary Data description: 89,701 cells profiled with 10xv2 across multiple timepoints of development Data available at: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE126954 Fields of the anndata object as printed from Python: ``` print(adata) AnnData object with n\_obs × n\_vars = 20222 × 89701 obs: 'gene\_id', 'gene\_name' var: 'study', 'batch', 'sample', 'sample\_description', 'barcode', 'cell\_type', 'n\_umi', 'time\_point', 'size\_factor', 'cell\_subtype', 'plot\_cell\_type', 'raw\_embryo\_time', 'embryo\_time', 'embryo\_time\_bin', 'raw\_embryo\_time\_bin', 'lineage', 'passed\_qc' print(adata.var.head(1).T) AnnData object with n\_obs × n\_vars = 20222 × 89701 obs: 'gene\_id', 'gene\_name' var: 'study', 'batch', 'sample', 'sample\_description', 'barcode', 'cell\_type', 'n\_umi', 'time\_point', 'size\_factor', 'cell\_subtype', 'plot\_cell\_type', 'raw\_embryo\_time', 'embryo\_time', 'embryo\_time\_bin', 'raw\_embryo\_time\_bin', 'lineage', 'passed\_qc' print(adata.obs.head(1).T) WBGene00010957 gene\_id WBGene00010957 gene\_name nduo-6 ``` Other Cite this record as:Da Veiga Beltrame, E. (2021). Packer et al 2019 scRNAseq dataset wrangled into standard WormBase anndata- 89k cells profiled with 10xv2 across multiple timepoints of development (Version 1.0) [Data set]. CaltechDATA. https://doi.org/10.22002/D1.1945 or choose a different citation style.Download CitationOther Unique Views: 27Unique Downloads: 0 between April 06, 2021 and July 12, 2021More info on how stats are collected}
}
@article{solomon_earth_2014,
	title        = {Earth mover's distances on discrete surfaces},
	author       = {Solomon, Justin and Rustamov, Raif and Guibas, Leonidas and Butscher, Adrian},
	year         = 2014,
	journal      = {ACM Trans. Graph.},
	volume       = 33,
	number       = 4,
	pages        = {1--12},
	abstract     = {We introduce a novel method for computing the earth mover’s distance (EMD) between probability distributions on a discrete surface. Rather than using a large linear program with a quadratic number of variables, we apply the theory of optimal transportation and pass to a dual differential formulation with linear scaling. After discretization using ﬁnite elements (FEM) and development of an accompanying optimization method, we apply our new EMD to problems in graphics and geometry processing. In particular, we uncover a class of smooth distances on a surface transitioning from a purely spectral distance to the geodesic distance between points; these distances also can be extended to the volume inside and outside the surface. A number of additional applications of our machinery to geometry problems in graphics are presented.}
}
@article{benamou_computational_2000,
	title        = {A computational fluid mechanics solution to the {Monge}-{Kantorovich} mass transfer problem},
	author       = {Benamou, Jean-David and Brenier, Yann},
	year         = 2000,
	journal      = {Numerische Mathematik},
	volume       = 84,
	number       = 3,
	pages        = {375--393}
}
@article{das_nonlinear_2021,
	title        = {Nonlinear {Dimensionality} {Reduction} for {Data} {Visualization}: {An} {Unsupervised} {Fuzzy} {Rule}-based {Approach}},
	shorttitle   = {Nonlinear {Dimensionality} {Reduction} for {Data} {Visualization}},
	author       = {Das, Suchismita and Pal, Nikhil R.},
	year         = 2021,
	journal      = {IEEE Trans. Fuzzy Syst.},
	pages        = {1--1},
	abstract     = {Here, we propose an unsupervised fuzzy rule-based dimensionality reduction method primarily for data visualization. It considers the following important issues relevant to dimensionality reduction-based data visualization: (i) preservation of neighborhood relationships, (ii) handling data on a non-linear manifold, (iii) the capability of predicting projections for new test data points, (iv) interpretability of the system, and (v) the ability to reject test points if required. For this, we use a first-order Takagi-Sugeno type model. We generate rule antecedents using clusters in the input data. In this context, we also propose a new variant of the Geodesic c-means clustering algorithm. We estimate the rule parameters by minimizing an error function that preserves the inter-point geodesic distances (distances over the manifold) as Euclidean distances on the projected space. We apply the proposed method on three synthetic and three real-world data sets and visually compare the results with four other standard data visualization methods. The obtained results show that the proposed method behaves desirably and performs better than or comparable to the methods compared with. The proposed method is found to be robust to the initial conditions. The predictability of the proposed method for test points is validated by experiments. We also assess the ability of our method to reject output points when it should. Then, we extend this concept to provide a general framework for learning an unsupervised fuzzy model for data projection with different objective functions. To the best of our knowledge, this is the first attempt to manifold learning using unsupervised fuzzy modeling.}
}
@article{makkuva_optimal_2019,
	title        = {Optimal transport mapping via input convex neural networks},
	author       = {Makkuva, Ashok Vardhan and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason D.},
	year         = 2019,
	abstract     = {In this paper, we present a novel and principled approach to learn the optimal transport between two distributions, from samples. Guided by the optimal transport theory, we learn the optimal Kantorovich potential which induces the optimal transport map. This involves learning two convex functions, by solving a novel minimax optimization. Building upon recent advances in the field of input convex neural networks, we propose a new framework where the gradient of one convex function represents the optimal transport mapping. Numerical experiments confirm that we learn the optimal transport mapping. This approach ensures that the transport mapping we find is optimal independent of how we initialize the neural networks. Further, target distributions from a discontinuous support can be easily captured, as gradient of a convex function naturally models a \{{\textbackslash}em discontinuous\} transport mapping.}
}
@techreport{bunne_learning_2021,
	title        = {Learning {Single}-{Cell} {Perturbation} {Responses} using {Neural} {Optimal} {Transport}},
	author       = {Bunne, Charlotte and Stark, Stefan G. and Gut, Gabriele and Castillo, Jacobo Sarabia del and Lehmann, Kjong-Van and Pelkmans, Lucas and Krause, Andreas and Rätsch, Gunnar},
	year         = 2021,
	pages        = {2021.12.15.472775},
	copyright    = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	abstract     = {The ability to understand and predict molecular responses towards external perturbations is a core question in molecular biology. Technological advancements in the recent past have enabled the generation of high-resolution single-cell data, making it possible to profile individual cells under different experimentally controlled perturbations. However, cells are typically destroyed during measurement, resulting in unpaired distributions over either perturbed or non-perturbed cells. Leveraging the theory of optimal transport and the recent advents of convex neural architectures, we learn a coupling describing the response of cell populations upon perturbation, enabling us to predict state trajectories on a single-cell level. We apply our approach, CellOT, to predict treatment responses of 21,650 cells subject to four different drug perturbations. CellOT outperforms current state-of-the-art methods both qualitatively and quantitatively, accurately capturing cellular behavior shifts across all different drugs.}
}
@article{zhu_neural_2021,
	title        = {Neural {Bellman}-{Ford} {Networks}: {A} {General} {Graph} {Neural} {Network} {Framework} for {Link} {Prediction}},
	shorttitle   = {Neural {Bellman}-{Ford} {Networks}},
	author       = {Zhu, Zhaocheng and Zhang, Zuobai and Xhonneux, Louis-Pascal and Tang, Jian},
	year         = 2021,
	journal      = {arXiv:2106.06935 [cs]},
	abstract     = {Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively. The NBFNet is very general, covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.},
	annote       = {Comment: NeurIPS 2021}
}
@article{baras_path_2010,
	title        = {Path {Problems} in {Networks}},
	author       = {Baras, John S. and Theodorakopoulos, George},
	year         = 2010,
	journal      = {Synthesis Lectures on Communication Networks},
	volume       = 3,
	number       = 1,
	pages        = {1--77}
}
@article{fan_scalable_2021,
	title        = {Scalable {Computations} of {Wasserstein} {Barycenter} via {Input} {Convex} {Neural} {Networks}},
	author       = {Fan, Jiaojiao and Taghvaei, Amirhossein and Chen, Yongxin},
	year         = 2021,
	journal      = {arXiv:2007.04462 [cs, math, stat]},
	abstract     = {Wasserstein Barycenter is a principled approach to represent the weighted mean of a given set of probability distributions, utilizing the geometry induced by optimal transport. In this work, we present a novel scalable algorithm to approximate the Wasserstein Barycenters aiming at high-dimensional applications in machine learning. Our proposed algorithm is based on the Kantorovich dual formulation of the Wasserstein-2 distance as well as a recent neural network architecture, input convex neural network, that is known to parametrize convex functions. The distinguishing features of our method are: i) it only requires samples from the marginal distributions; ii) unlike the existing approaches, it represents the Barycenter with a generative model and can thus generate infinite samples from the barycenter without querying the marginal distributions; iii) it works similar to Generative Adversarial Model in one marginal case. We demonstrate the efficacy of our algorithm by comparing it with the state-of-art methods in multiple experiments.},
	annote       = {Comment: 21 pages}
}
@article{korotin_wasserstein-2_2020,
	title        = {Wasserstein-2 {Generative} {Networks}},
	author       = {Korotin, Alexander and Egiazarian, Vage and Asadulaev, Arip and Safin, Alexander and Burnaev, Evgeny},
	year         = 2020,
	journal      = {arXiv:1909.13082 [cs, stat]},
	abstract     = {We propose a novel end-to-end non-minimax algorithm for training optimal transport mappings for the quadratic cost (Wasserstein-2 distance). The algorithm uses input convex neural networks and a cycle-consistency regularization to approximate Wasserstein-2 distance. In contrast to popular entropic and quadratic regularizers, cycle-consistency does not introduce bias and scales well to high dimensions. From the theoretical side, we estimate the properties of the generative mapping fitted by our algorithm. From the practical side, we evaluate our algorithm on a wide range of tasks: image-to-image color transfer, latent space optimal transport, image-to-image style transfer, and domain adaptation.},
	annote       = {Comment: 30 pages, 21 figures, 3 tables},
	annote       = {Extends nt Makkuva et al. (2019), but removing the minimax proble by imposing additional regularization that promotes cycle consistency.}
}
@misc{noauthor_learning_nodate,
	title        = {Learning {Graph} {Cellular} {Automata} - {Recherche} {Google}}
}
@article{grattarola_learning_2021,
	title        = {Learning {Graph} {Cellular} {Automata}},
	author       = {Grattarola, Daniele and Livi, Lorenzo and Alippi, Cesare},
	year         = 2021,
	journal      = {arXiv:2110.14237 [cs]},
	abstract     = {Cellular automata (CA) are a class of computational models that exhibit rich dynamics emerging from the local interaction of cells arranged in a regular lattice. In this work we focus on a generalised version of typical CA, called graph cellular automata (GCA), in which the lattice structure is replaced by an arbitrary graph. In particular, we extend previous work that used convolutional neural networks to learn the transition rule of conventional CA and we use graph neural networks to learn a variety of transition rules for GCA. First, we present a general-purpose architecture for learning GCA, and we show that it can represent any arbitrary GCA with finite and discrete state space. Then, we test our approach on three different tasks: 1) learning the transition rule of a GCA on a Voronoi tessellation; 2) imitating the behaviour of a group of flocking agents; 3) learning a rule that converges to a desired target state.},
	annote       = {Comment: 35th Conference on Neural Information Processing Systems (NeurIPS 2021)}
}
@inproceedings{zhang_learning_2020,
	title        = {Learning to {Generate} {3D} {Shapes} with {Generative} {Cellular} {Automata}},
	author       = {Zhang, Dongsu and Choi, Changwoon and Kim, Jeonghwan and Kim, Young Min},
	year         = 2020,
	abstract     = {In this work, we present a probabilistic 3D generative model, named Generative Cellular Automata, which is able to produce diverse and high quality shapes. We formulate the shape generation process...}
}
@misc{noauthor_python_nodate,
	title        = {Python {Data} {Science} {Handbook}}
}
@inproceedings{stark_3d_2021,
	title        = {{3D} {Infomax} improves {GNNs} for {Molecular} {Property} {Prediction}},
	author       = {Stärk, Hannes and Beaini, Dominique and Corso, Gabriele and Tossou, Prudencio and Dallago, Christian and Günnemann, Stephan and Lio, Pietro},
	year         = 2021,
	abstract     = {We pre-train GNNs to understand the geometry of molecules given only their 2D molecular graph.}
}
@article{zhang_inference_2021,
	title        = {Inference of high-resolution trajectories in single-cell {RNA}-seq data by using {RNA} velocity},
	author       = {Zhang, Ziqi and Zhang, Xiuwei},
	year         = 2021,
	journal      = {Cell Reports Methods},
	volume       = 1,
	number       = 6,
	pages        = 100095,
	abstract     = {Trajectory inference (TI) methods infer cell developmental trajectory from single-cell RNA sequencing data. Current TI methods can be categorized into those using RNA velocity information and those using only single-cell gene expression data. The latter type of methods are restricted to certain trajectory structures, and cannot determine cell developmental direction. Recently proposed TI methods using RNA velocity information have limited accuracy. We present CellPath, a method that infers cell trajectories by integrating single-cell gene expression and RNA velocity information. CellPath overcomes the restrictions of TI methods that do not use RNA velocity information: it can find multiple high-resolution trajectories without constraints on the trajectory structure, and can automatically detect the direction of each trajectory path. We evaluate CellPath on both real and simulated datasets and show that CellPath finds more accurate and detailed trajectories than the state-of-the-art TI methods using or not using RNA velocity information.}
}
@article{weng_vetra_2021,
	title        = {{VeTra}: a tool for trajectory inference based on {RNA} velocity},
	shorttitle   = {{VeTra}},
	author       = {Weng, Guangzheng and Kim, Junil and Won, Kyoung Jae},
	year         = 2021,
	journal      = {Bioinformatics},
	volume       = 37,
	number       = 20,
	pages        = {3509--3513},
	abstract     = {Trajectory inference (TI) for single cell RNA sequencing (scRNAseq) data is a powerful approach to interpret dynamic cellular processes such as cell cycle and development. Still, however, accurate inference of trajectory is challenging. Recent development of RNA velocity provides an approach to visualize cell state transition without relying on prior knowledge.To perform TI and group cells based on RNA velocity we developed VeTra. By applying cosine similarity and merging weakly connected components, VeTra identifies cell groups from the direction of cell transition. Besides, VeTra suggests key regulators from the inferred trajectory. VeTra is a useful tool for TI and subsequent analysis.The Vetra is available at https://github.com/wgzgithub/VeTra.Supplementary data are available at Bioinformatics online.}
}
@article{lange_cellrank_2022,
	title        = {{CellRank} for directed single-cell fate mapping},
	author       = {Lange, Marius and Bergen, Volker and Klein, Michal and Setty, Manu and Reuter, Bernhard and Bakhti, Mostafa and Lickert, Heiko and Ansari, Meshal and Schniering, Janine and Schiller, Herbert B. and Pe’er, Dana and Theis, Fabian J.},
	year         = 2022,
	journal      = {Nature Methods},
	pages        = {1--12},
	copyright    = {2022 The Author(s)},
	abstract     = {Computational trajectory inference enables the reconstruction of cell state dynamics from single-cell RNA sequencing experiments. However, trajectory inference requires that the direction of a biological process is known, largely limiting its application to differentiating systems in normal development. Here, we present CellRank (https://cellrank.org) for single-cell fate mapping in diverse scenarios, including regeneration, reprogramming and disease, for which direction is unknown. Our approach combines the robustness of trajectory inference with directional information from RNA velocity, taking into account the gradual and stochastic nature of cellular fate decisions, as well as uncertainty in velocity vectors. On pancreas development data, CellRank automatically detects initial, intermediate and terminal populations, predicts fate potentials and visualizes continuous gene expression trends along individual lineages. Applied to lineage-traced cellular reprogramming data, predicted fate probabilities correctly recover reprogramming outcomes. CellRank also predicts a new dedifferentiation trajectory during postinjury lung regeneration, including previously unknown intermediate cell states, which we confirm experimentally.}
}
@article{maretic_got_2019,
	title        = {{GOT}: {An} {Optimal} {Transport} framework for {Graph} comparison},
	shorttitle   = {{GOT}},
	author       = {Maretic, Hermina Petric and Gheche, Mireille EL and Chierchia, Giovanni and Frossard, Pascal},
	year         = 2019,
	journal      = {arXiv:1906.02085 [cs, stat]},
	abstract     = {We present a novel framework based on optimal transport for the challenging problem of comparing graphs. Specifically, we exploit the probabilistic distribution of smooth graph signals defined with respect to the graph topology. This allows us to derive an explicit expression of the Wasserstein distance between graph signal distributions in terms of the graph Laplacian matrices. This leads to a structurally meaningful measure for comparing graphs, which is able to take into account the global structure of graphs, while most other measures merely observe local changes independently. Our measure is then used for formulating a new graph alignment problem, whose objective is to estimate the permutation that minimizes the distance between two graphs. We further propose an efficient stochastic algorithm based on Bayesian exploration to accommodate for the non-convexity of the graph alignment problem. We finally demonstrate the performance of our novel framework on different tasks like graph alignment, graph classification and graph signal prediction, and we show that our method leads to significant improvement with respect to the-state-of-art algorithms.},
	annote       = {Comment: 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada}
}
@techreport{zhang_optimal_2021,
	title        = {Optimal transport analysis reveals trajectories in steady-state systems},
	author       = {Zhang, Stephen and Afanassiev, Anton and Greenstreet, Laura and Matsumoto, Tetsuya and Schiebinger, Geoffrey},
	year         = 2021,
	pages        = {2021.03.02.433630},
	copyright    = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	abstract     = {Understanding how cells change their identity and behaviour in living systems is an important question in many fields of biology. The problem of inferring cell trajectories from single-cell measurements has been a major topic in the single-cell analysis community, with different methods developed for equilibrium and non-equilibrium systems (e.g. haematopoeisis vs. embryonic development). We show that optimal transport analysis, a technique originally designed for analysing time-courses, may also be applied to infer cellular trajectories from a single snapshot of a population in equilibrium. Therefore optimal transport provides a unified approach to inferring trajectories, applicable to both stationary and non-stationary systems. Our method, StationaryOT, is mathematically motivated in a natural way from the hypothesis of a Waddington’s epigenetic landscape. We implemented StationaryOT as a software package and demonstrate its efficacy when applied to simulated data as well as single-cell data from Arabidopsis thaliana root development.}
}
@article{weinreb_fundamental_2017,
	title        = {Fundamental limits on dynamic inference from single cell snapshots [preprint]},
	author       = {Weinreb, Caleb and Wolock, Samuel and Tusi, Betsabeh and Socolovsky, Merav and Klein, Allon},
	year         = 2017,
	journal      = {University of Massachusetts Medical School Faculty Publications}
}
@article{solomon_continuous-flow_2016,
	title        = {Continuous-{Flow} {Graph} {Transportation} {Distances}},
	author       = {Solomon, Justin and Rustamov, Raif and Guibas, Leonidas and Butscher, Adrian},
	year         = 2016,
	journal      = {arXiv:1603.06927 [cs]},
	abstract     = {Optimal transportation distances are valuable for comparing and analyzing probability distributions, but larger-scale computational techniques for the theoretically favorable quadratic case are limited to smooth domains or regularized approximations. Motivated by fluid flow-based transportation on \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$, however, this paper introduces an alternative definition of optimal transportation between distributions over graph vertices. This new distance still satisfies the triangle inequality but has better scaling and a connection to continuous theories of transportation. It is constructed by adapting a Riemannian structure over probability distributions to the graph case, providing transportation distances as shortest-paths in probability space. After defining and analyzing theoretical properties of our new distance, we provide a time discretization as well as experiments verifying its effectiveness.}
}
@article{haghverdi_diffusion_2016,
	title        = {Diffusion pseudotime robustly reconstructs lineage branching},
	author       = {Haghverdi, Laleh and Büttner, Maren and Wolf, F. Alexander and Buettner, Florian and Theis, Fabian J.},
	year         = 2016,
	journal      = {Nature Methods},
	volume       = 13,
	number       = 10,
	pages        = {845--848},
	copyright    = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	abstract     = {Diffusion pseudotime (DPT) enables robust and scalable inference of cellular trajectories, branching events, metastable states and underlying gene dynamics from snapshot single-cell gene expression data.}
}
@misc{noauthor_visualizing_nodate,
	title        = {Visualizing structure and transitions in high-dimensional biological data {\textbar} {Nature} {Biotechnology}}
}
@misc{noauthor_dbmap_nodate,
	title        = {{dbMAP}}
}
@article{gigante_compressed_2019,
	title        = {Compressed {Diffusion}},
	author       = {Gigante, Scott and Stanley III, Jay S. and Vu, Ngan and van Dijk, David and Moon, Kevin and Wolf, Guy and Krishnaswamy, Smita},
	year         = 2019,
	journal      = {arXiv:1902.00033 [cs, stat]},
	abstract     = {Diffusion maps are a commonly used kernel-based method for manifold learning, which can reveal intrinsic structures in data and embed them in low dimensions. However, as with most kernel methods, its implementation requires a heavy computational load, reaching up to cubic complexity in the number of data points. This limits its usability in modern data analysis. Here, we present a new approach to computing the diffusion geometry, and related embeddings, from a compressed diffusion process between data regions rather than data points. Our construction is based on an adaptation of the previously proposed measure-based Gaussian correlation (MGC) kernel that robustly captures the local geometry around data points. We use this MGC kernel to efficiently compress diffusion relations from pointwise to data region resolution. Finally, a spectral embedding of the data regions provides coordinates that are used to interpolate and approximate the pointwise diffusion map embedding of data. We analyze theoretical connections between our construction and the original diffusion geometry of diffusion maps, and demonstrate the utility of our method in analyzing big datasets, where it outperforms competing approaches.},
	annote       = {Comment: 4 pages double column, published in SampTA 2019}
}
@article{nielsen_neural_2015,
	title        = {Neural {Networks} and {Deep} {Learning}},
	author       = {Nielsen, Michael A.},
	year         = 2015
}
@misc{noauthor_ift_nodate,
	title        = {{IFT} 6135 - {H2022} - {Lectures}},
	abstract     = {03 – ConvNets (maybe starting 17/01/2022) In this lecture we finish up our discussion of training neural networks and we introduce Convolutional Neural Networks. Lecture 03 CNNs  (some slides are modified from Hugo Larochelle’s course notes) Backprop in CNNs (Slides are from Hiroshi Kuwajima’s},
	annote       = {I have all the notes somewhere on the comp.}
}
@article{balestriero_learning_2021,
	title        = {Learning in {High} {Dimension} {Always} {Amounts} to {Extrapolation}},
	author       = {Balestriero, Randall and Pesenti, Jerome and LeCun, Yann},
	year         = 2021,
	journal      = {arXiv:2110.09485 [cs]},
	abstract     = {The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample \$x\$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when \$x\$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (\${\textgreater}\$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.}
}
@misc{noauthor_siam_nodate,
	title        = {{SIAM} {Conference} on {Mathematics} of {Data} {Science} ({MDS22})},
	journal      = {SIAM},
	abstract     = {SIAM fosters the development of applied mathematical and computational methodologies needed in various application areas. Applied mathematics, in partnership with computational science, is essential in solving many real-world problems. Through publications, research and community, the mission of SIAM is to build cooperation between mathematics and the worlds of science and technology.},
	annote       = {
		Poster and lecture deadline March 28, 2022

		The project could be on the dim reduction idea, similar to Phate
	}
}
@misc{noauthor_tutorials_nodate,
	title        = {Tutorials — {Scanpy} 1.9.0.dev63+gb69015e9 documentation}
}
@article{lafon_diffusion_2006,
	title        = {Diffusion maps and coarse-graining: {A} unified framework for dimensionality reduction, graph partitioning and data set parameterization},
	shorttitle   = {Diffusion maps and coarse-graining},
	author       = {Lafon, Stéphane and Lee, Ann B.},
	year         = 2006,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume       = 28,
	pages        = {1393--1403},
	abstract     = {We provide evidence that non-linear dimensionality reduction, clustering and data set parameterization can be solved within one and the same framework. The main idea is to define a system of coordinates with an explicit metric that reflects the connectivity of a given data set and that is robust to noise. Our construction, which is based on a Markov random walk on the data, offers a general scheme of simultaneously reorganizing and subsampling graphs and arbitrarily shaped data sets in high dimensions using intrinsic geometry. We show that clustering in embedding spaces is equivalent to compressing operators. The objective of data partitioning and clustering is to coarse-grain the random walk on the data while at the same time preserving a diffusion operator for the intrinsic geometry or connectivity of the data set up to some accuracy. We show that the quantization distortion in diffusion space bounds the error of compression of the operator, thus giving a rigorous justification for k-means clustering in diffusion space and a precise measure of the performance of general clustering algorithms.}
}
@misc{noauthor_connected_nodate,
	title        = {Connected {Papers} {\textbar} {Find} and explore academic papers},
	abstract     = {A unique, visual tool to help researchers and applied scientists find and explore papers relevant to their field of work.}
}
@misc{noauthor_latest_nodate,
	title        = {The latest in {Machine} {Learning} {\textbar} {Papers} {With} {Code}}
}
@misc{carlml_d_2022,
	title        = {[{D}] {Tools} alike to arxiv-sanity for finding new papers/creating one's library.},
	author       = {carlml},
	year         = 2022,
	journal      = {r/MachineLearning},
	type         = {Reddit {Post}}
}
@misc{noauthor_lillog_nodate,
	title        = {Lil'{Log}},
	journal      = {Lil'Log},
	abstract     = {Document my learning notes.}
}
@misc{noauthor_daily_nodate,
	title        = {Daily {Papers}}
}
@misc{brownlee_how_2019,
	title        = {How to {Develop} a {Wasserstein} {Generative} {Adversarial} {Network} ({WGAN}) {From} {Scratch}},
	author       = {Brownlee, Jason},
	year         = 2019,
	journal      = {Machine Learning Mastery},
	abstract     = {The Wasserstein Generative Adversarial Network, or Wasserstein GAN, is an extension to the generative adversarial network that both improves the […]}
}
@misc{noauthor_stateoftheart_nodate,
	title        = {Stateoftheart {AI}},
	abstract     = {An open-data and free platform that tracks the evolution, the progress, and the frontier of existing AI research. Built by the community to facilitate the collaborative and transparent development of AI}
}
@misc{noauthor_academic_nodate,
	title        = {Academic {Torrents}},
	journal      = {Academic Torrents},
	abstract     = {A distributed system for sharing enormous datasets - for researchers, by researchers. The result is a scalable, secure, and fault-tolerant repository for data, with blazing fast download speeds.}
}
@misc{noauthor_journal_nodate,
	title        = {The {Journal} {Club}}
}
@article{ba_sublinear_2011,
	title        = {Sublinear {Time} {Algorithms} for {Earth} {Mover}’s {Distance}},
	author       = {Ba, Khanh Do and Nguyen, Huy L. and Nguyen, Huy N. and Rubinfeld, Ronitt},
	year         = 2011,
	journal      = {Theory Comput Syst},
	volume       = 48,
	number       = 2,
	pages        = {428--442},
	abstract     = {We study the problem of estimating the Earth Mover’s Distance (EMD) between probability distributions when given access only to samples of the distribution. We give closeness testers and additive-error estimators over domains in [0,1]d, with sample complexities independent of domain size—permitting the testability even of continuous distributions over infinite domains. Instead, our algorithms depend on the dimension of the domain space and the quality of the result required. We also prove lower bounds for closeness testing, showing the dependencies on these parameters to be essentially optimal. Additionally, we consider whether natural classes of distributions exist for which there are algorithms with better dependence on the dimension, and show that for highly clusterable data, this is indeed the case. Lastly, we consider a variant of the EMD, defined over tree metrics instead of the usual ℓ1 metric, and give tight upper and lower bounds.}
}
@inproceedings{andoni_earth_2008,
	title        = {Earth mover distance over high-dimensional spaces},
	author       = {Andoni, Alexandr and Indyk, Piotr and Krauthgamer, Robert},
	year         = 2008,
	booktitle    = {Proceedings of the nineteenth annual {ACM}-{SIAM} symposium on {Discrete} algorithms},
	publisher    = {Society for Industrial and Applied Mathematics},
	address      = {USA},
	series       = {{SODA} '08},
	pages        = {343--352},
	abstract     = {The Earth Mover Distance (EMD) between two equal-size sets of points in ℝd is defined to be the minimum cost of a bipartite matching between the two pointsets. It is a natural metric for comparing sets of features, and as such, it has received significant interest in computer vision. Motivated by recent developments in that area, we address computational problems involving EMD over high-dimensional pointsets. A natural approach is to embed the EMD metric into l1, and use the algorithms designed for the latter space. However, Khot and Naor [KN06] show that any embedding of EMD over the d-dimensional Hamming cube into l1 must incur a distortion Ω(d), thus practically losing all distance information. We circumvent this roadblock by focusing on sets with cardinalities upper-bounded by a parameter s, and achieve a distortion of only O(log s · log d). Since in applications the feature sets have bounded size, the resulting distortion is much smaller than the Ω(d) lower bound. Our approach is quite general and easily extends to EMD over ℝd. We then provide a strong lower bound on the multi-round communicatic complexity of estimating EMD, which in particular strengthens the known non-embeddability result of [KN06]. Our bound exhibits a smooth tradeoff between approximation and communication, and for example implies that every algorithm that estimates EMD using constant size sketches can only achieve ω(log s) approximation.}
}
@misc{noauthor_machine_nodate,
	title        = {Machine {Learning} {Mastery}},
	journal      = {Machine Learning Mastery},
	abstract     = {Making developers awesome at machine learning.}
}
@techreport{li_single-cell_2021,
	title        = {Single-cell multi-omic velocity infers dynamic and decoupled gene regulation},
	author       = {Li, Chen and Virgilio, Maria and Collins, Kathleen L. and Welch, Joshua D.},
	year         = 2021,
	pages        = {2021.12.13.472472},
	copyright    = {© 2021, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	abstract     = {Single-cell multi-omic datasets, in which multiple molecular modalities are profiled within the same cell, provide a unique opportunity to discover the relationships between cellular epigenomic and transcriptomic changes. To realize this potential, we developed MultiVelo, a mechanistic model of gene expression that extends the RNA velocity framework to incorporate epigenomic data. MultiVelo uses a probabilistic latent variable model to estimate the switch time and rate parameters of chromatin accessibility and gene expression from single-cell data, providing a quantitative summary of the temporal relationship between epigenomic and transcriptomic changes. Incorporating chromatin accessibility data significantly improves the accuracy of cell fate prediction compared to velocity estimates from RNA only. Fitting MultiVelo on single-cell multi-omic datasets from brain, skin, and blood cells reveals two distinct classes of genes distinguished by whether chromatin closes before or after transcription ceases. Our model also identifies four types of cell states–two states in which epigenome and transcriptome are coupled and two distinct decoupled states. The parameters inferred by MultiVelo quantify the length of time for which genes occupy each of the four states, ranking genes by the degree of coupling between transcriptome and epigenome. Finally, we identify time lags between transcription factor expression and binding site accessibility and between disease-associated SNP accessibility and expression of the linked genes. We provide an open-source Python implementation of MultiVelo on PyPI and GitHub (https://github.com/welch-lab/MultiVelo).},
	institution  = {bioRxiv}
}
@misc{noauthor_optimal_nodate,
	title        = {Optimal entropy-transport problems and a new hellinger–kantorovich distance between positive measures - {Recherche} {Google}}
}
@article{schiebinger_reconstruction_2017,
	title        = {Reconstruction of developmental landscapes by optimal-transport analysis of single-cell gene expression sheds light on cellular reprogramming},
	author       = {Schiebinger, Geoffrey and Shu, Jian and Tabaka, Marcin and Cleary, Brian and Subramanian, Vidya and Solomon, Aryeh and Liu, Siyan and Lin, Stacie and Berube, Peter and Lee, Lia and Chen, Jenny and Brumbaugh, Justin and Rigollet, Philippe and Hochedlinger, Konrad and Jaenisch, Rudolf and Regev, Aviv and Lander, Eric S.},
	year         = 2019,
	journal      = {Cell}
}
@article{long_landmark_2019,
	title        = {Landmark {Diffusion} {Maps} ({L}-{dMaps}): {Accelerated} manifold learning out-of-sample extension},
	shorttitle   = {Landmark {Diffusion} {Maps} ({L}-{dMaps})},
	author       = {Long, Andrew W. and Ferguson, Andrew L.},
	year         = 2019,
	journal      = {Applied and Computational Harmonic Analysis},
	volume       = 47,
	number       = 1,
	pages        = {190--211},
	abstract     = {Diffusion maps are a nonlinear manifold learning technique based on harmonic analysis of a diffusion process over the data. Out-of-sample extensions with computational complexity \${\textbackslash}mathcal\{O\}(N)\$, where \$N\$ is the number of points comprising the manifold, frustrate applications to online learning applications requiring rapid embedding of high-dimensional data streams. We propose landmark diffusion maps (L-dMaps) to reduce the complexity to \${\textbackslash}mathcal\{O\}(M)\$, where \$M {\textbackslash}ll N\$ is the number of landmark points selected using pruned spanning trees or k-medoids. Offering \$(N/M)\$ speedups in out-of-sample extension, L-dMaps enables the application of diffusion maps to high-volume and/or high-velocity streaming data. We illustrate our approach on three datasets: the Swiss roll, molecular simulations of a C\$\_\{24\}\$H\$\_\{50\}\$ polymer chain, and biomolecular simulations of alanine dipeptide. We demonstrate up to 50-fold speedups in out-of-sample extension for the molecular systems with less than 4\% errors in manifold reconstruction fidelity relative to calculations over the full dataset.},
	annote       = {Comment: Submitted}
}
@inproceedings{kipf_neural_2018,
	title        = {Neural {Relational} {Inference} for {Interacting} {Systems}},
	author       = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
	year         = 2018,
	booktitle    = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {2688--2697},
	abstract     = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system’s constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.}
}
@article{linderman_clustering_2017,
	title        = {Clustering with t-{SNE}, provably},
	author       = {Linderman, George C. and Steinerberger, Stefan},
	year         = 2017,
	journal      = {arXiv:1706.02582 [cs, stat]},
	abstract     = {t-distributed Stochastic Neighborhood Embedding (t-SNE), a clustering and visualization method proposed by van der Maaten \& Hinton in 2008, has rapidly become a standard tool in a number of natural sciences. Despite its overwhelming success, there is a distinct lack of mathematical foundations and the inner workings of the algorithm are not well understood. The purpose of this paper is to prove that t-SNE is able to recover well-separated clusters; more precisely, we prove that t-SNE in the `early exaggeration' phase, an optimization technique proposed by van der Maaten \& Hinton (2008) and van der Maaten (2014), can be rigorously analyzed. As a byproduct, the proof suggests novel ways for setting the exaggeration parameter \${\textbackslash}alpha\$ and step size \$h\$. Numerical examples illustrate the effectiveness of these rules: in particular, the quality of embedding of topological structures (e.g. the swiss roll) improves. We also discuss a connection to spectral clustering methods.}
}
@misc{noauthor_aleksandar_nodate,
	title        = {Aleksandar {Bojchevski}}
}
@inproceedings{lawrence_gaussian_2004,
	title        = {Gaussian {Process} {Latent} {Variable} {Models} for {Visualisation} of {High} {Dimensional} {Data}},
	author       = {Lawrence, Neil},
	year         = 2004,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher    = {MIT Press},
	volume       = 16
}
@misc{noauthor_teaching_nodate,
	title        = {Teaching - {Homepage} of {Gabriel} {Peyré}}
}
@misc{noauthor_contact_nodate,
	title        = {Contact - {Homepage} of {Gabriel} {Peyré}}
}
@article{peyre_computational_2020,
	title        = {Computational {Optimal} {Transport}},
	author       = {Peyré, Gabriel and Cuturi, Marco},
	year         = 2020,
	journal      = {arXiv:1803.00567},
	abstract     = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
	annote       = {Comment: new version with corrected typo in Eq. 4.43 and 4.44 (minus sign in front of f, g now changed to +) a few more corrected typos}
}
@misc{noauthor_nsl_nodate,
	title        = {{NSL} {Winter} 2022 - {Topics}\&{Papers}},
	abstract     = {Here is a suggested list of topics and papers - still UNDER CONSTRUCTION. If you would like to suggest a relevant paper not in the list, please contact the instructor and/or the TAs (contact info on the course descriptions page).   Here is  paper presentation schedule \& sign up sheet}
}
@article{topping_understanding_2021,
	title        = {Understanding over-squashing and bottlenecks on graphs via curvature},
	author       = {Topping, Jake and Di Giovanni, Francesco and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
	year         = 2021,
	journal      = {arXiv:2111.14522 [cs, stat]},
	abstract     = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of \$k\$-hop neighbors grows rapidly with \$k\$. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing.}
}
@article{maggioni_learning_nodate,
	title        = {Learning by {Unsupervised} {Nonlinear} {Diﬀusion}},
	author       = {Maggioni, Mauro and Murphy, James M},
	pages        = 56,
	abstract     = {This paper proposes and analyzes a novel clustering algorithm, called learning by unsupervised nonlinear diﬀusion (LUND), that combines graph-based diﬀusion geometry with techniques based on density and mode estimation. LUND is suitable for data generated from mixtures of distributions with densities that are both multimodal and supported near nonlinear sets. A crucial aspect of this algorithm is the use of time of a data-adapted diﬀusion process, and associated diﬀusion distances, as a scale parameter that is diﬀerent from the local spatial scale parameter used in many clustering algorithms. We prove estimates for the behavior of diﬀusion distances with respect to this time parameter under a ﬂexible nonparametric data model, identifying a range of times in which the mesoscopic equilibria of the underlying process are revealed, corresponding to a gap between within-cluster and between-cluster diﬀusion distances. These structures may be missed by the top eigenvectors of the graph Laplacian, commonly used in spectral clustering. This analysis is leveraged to prove suﬃcient conditions guaranteeing the accuracy of LUND. We implement LUND and conﬁrm its theoretical properties on illustrative data sets, demonstrating its theoretical and empirical advantages over both spectral and density-based clustering.}
}
@misc{noauthor_optimal-transport_nodate,
	title        = {Optimal-{Transport} {Analysis} of {Single}-{Cell} {Gene} {Expression} {Identifies} {Developmental} {Trajectories} in {Reprogramming} {\textbar} {Elsevier} {Enhanced} {Reader}}
}
@misc{noauthor_python_nodate-1,
	title        = {Python {Data} {Science} {Handbook} {\textbar} {Python} {Data} {Science} {Handbook}}
}
@misc{xu_greens_2020,
	title        = {Green's function, {RKHS} and {Regularisation}},
	author       = {Xu, Yidan},
	year         = 2020,
	journal      = {Yidan Xu},
	abstract     = {With the spectral perspective of RKHS introduced previously, we now look at a special and important category of reproducing kernels, which is the Green's functions to positive systems of differential equations. We will have examples on the eigenvalue problem for Dirichlet Laplace operator; and also on the Heat equation in Euclidean space. An intuitive generalisation is then provided. Finally, we will discuss informally the extension to discrete differential system and RKHS defined by Hermitian matrices.}
}
@article{sriperumbudur_hilbert_nodate,
	title        = {Hilbert {Space} {Embeddings} and {Metrics} on {Probability} {Measures}},
	author       = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Scholkopf, Bernhard and Lanckriet, Gert R G},
	pages        = 45,
	abstract     = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be deﬁned as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that deﬁnes the inner product in the RKHS.}
}
@article{de_vito_reproducing_2019,
	title        = {Reproducing kernel {Hilbert} spaces on manifolds: {Sobolev} and {Diffusion} spaces},
	shorttitle   = {Reproducing kernel {Hilbert} spaces on manifolds},
	author       = {De Vito, Ernesto and Mücke, Nicole and Rosasco, Lorenzo},
	year         = 2019,
	journal      = {arXiv:1905.10913 [cs, math, stat]},
	abstract     = {We study reproducing kernel Hilbert spaces (RKHS) on a Riemannian manifold. In particular, we discuss under which condition Sobolev spaces are RKHS and characterize their reproducing kernels. Further, we introduce and discuss a class of smoother RKHS that we call diffusion spaces. We illustrate the general results with a number of detailed examples.}
}
@misc{noauthor_wot_nodate,
	title        = {wot},
	journal      = {wot},
	abstract     = {A software package for analyzing snapshots of developmental processes}
}
@misc{noauthor_mathjobs_nodate,
	title        = {{MathJobs} from the the {American} {Mathematical} {Society}},
	abstract     = {Mathjobs is an automated job application system sponsored by the AMS.}
}
@misc{noauthor_math_nodate,
	title        = {Math 612}
}
@misc{noauthor_gradient_nodate,
	title        = {Gradient flows in {2D} — {GeomLoss}}
}
@misc{noauthor_jean_nodate,
	title        = {Jean {Feydy}'s home page}
}
@article{chami_machine_2021,
	title        = {Machine {Learning} on {Graphs}: {A} {Model} and {Comprehensive} {Taxonomy}},
	shorttitle   = {Machine {Learning} on {Graphs}},
	author       = {Chami, Ines and Abu-El-Haija, Sami and Perozzi, Bryan and Ré, Christopher and Murphy, Kevin},
	year         = 2021,
	journal      = {arXiv:2005.03675 [cs, stat]},
	abstract     = {There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.}
}
@article{phillips_gentle_2011,
	title        = {A {Gentle} {Introduction} to the {Kernel} {Distance}},
	author       = {Phillips, Jeff M. and Venkatasubramanian, Suresh},
	year         = 2011,
	journal      = {arXiv:1103.1625 [cs]},
	abstract     = {This document reviews the definition of the kernel distance, providing a gentle introduction tailored to a reader with background in theoretical computer science, but limited exposure to technology more common to machine learning, functional analysis and geometric measure theory. The key aspect of the kernel distance developed here is its interpretation as an L\_2 distance between probability measures or various shapes (e.g. point sets, curves, surfaces) embedded in a vector space (specifically an RKHS). This structure enables several elegant and efficient solutions to data analysis problems. We conclude with a glimpse into the mathematical underpinnings of this measure, highlighting its recent independent evolution in two separate fields.}
}
@inproceedings{zhuang_unsupervised_2011,
	title        = {Unsupervised {Multiple} {Kernel} {Learning}},
	author       = {Zhuang, Jinfeng and Wang, Jialei and Hoi, Steven C. H. and Lan, Xiangyang},
	year         = 2011,
	booktitle    = {Proceedings of the {Asian} {Conference} on {Machine} {Learning}},
	publisher    = {PMLR},
	pages        = {129--144},
	abstract     = {Traditional multiple kernel learning (MKL) algorithms are essentially supervised learning in the sense that the kernel learning task requires the class labels of training data. However, class labels may not always be available prior to the kernel learning task in some real world scenarios, e.g., an early preprocessing step of a classification task or an unsupervised learning task such as dimension reduction. In this paper, we investigate a problem of Unsupervised Multiple Kernel Learning (UMKL), which does not require class labels of training data as needed in a conventional multiple kernel learning task. Since a kernel essentially defines pairwise similarity between any two examples, our unsupervised kernel learning method mainly follows two intuitive principles: (1) a good kernel should allow every example to be well reconstructed from its localized bases weighted by the kernel values; (2) a good kernel should induce kernel values that are coincided with the local geometry of the data. We formulate the unsupervised multiple kernel learning problem as an optimization task and propose an efficient alternating optimization algorithm to solve it. Empirical results on both classification and dimension reductions tasks validate the efficacy of the proposed UMKL algorithm.}
}
@misc{noauthor_research_nodate,
	title        = {Research {Journal}}
}
@inproceedings{yu_nonlinear_2009,
	title        = {Nonlinear {Learning} using {Local} {Coordinate} {Coding}},
	author       = {Yu, Kai and Zhang, Tong and Gong, Yihong},
	year         = 2009,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 22
}
@book{demeniconi_proceedings_2020,
	title        = {Proceedings of the 2020 {SIAM} {International} {Conference} on {Data} {Mining}},
	year         = 2020,
	publisher    = {Society for Industrial and Applied Mathematics},
	address      = {Philadelphia, PA},
	isbn         = {978-1-61197-623-6},
	abstract     = {We propose a deep learning approach for discovering kernels tailored to identifying clusters over sample data. Our neural network produces sample embeddings that are motivated by and are at least as expressive as spectral clustering. Our training objective, based on the Hilbert Schmidt Independence Criterion, can be optimized via gradient adaptations on the Stiefel manifold, leading to signiﬁcant acceleration over spectral methods relying on eigen-decompositions. Finally, our trained embedding can be directly applied to out-of-sample data. We show experimentally that our approach outperforms several state-of-the-art deep clustering methods, as well as traditional approaches such as k-means and spectral clustering over a broad array of real and synthetic datasets.},
	editor       = {Demeniconi, Carlotta and Chawla, Nitesh}
}
@misc{noauthor_machine_nodate-1,
	title        = {machine learning - {Maximum} {Mean} {Discrepancy} (distance distribution)},
	journal      = {Cross Validated}
}
@article{noauthor_distance_nodate,
	title        = {Distance for dataset in {RKHS} context}
}
@article{noauthor_symmetric_nodate,
	title        = {Symmetric {Spaces} for {Graph} {Embeddings}: {A} {Finsler}-{Riemannian} {Approach}}
}
@misc{deac_accepted_nodate,
	title        = {Accepted {Papers}},
	author       = {Deac, Andreea},
	abstract     = {ICML 2020 Workshop}
}
@article{puny_graph_nodate,
	title        = {From {Graph} {Low}-{Rank} {Global} {Attention}  to 2-{FWL} {Approximation}},
	author       = {Puny, Omri and Ben-Hamu, Heli and Lipman, Yaron},
	pages        = 8,
	abstract     = {Graph Neural Networks are known to have an expressive power bounded by that of the vertex coloring algorithm (Xu et al., 2019a; Morris et al., 2018). However, for rich node features, such a bound does not exist and GNNs can be shown to be universal. Unfortunately, expressive power alone does not imply good generalization.}
}
@inproceedings{knyazev_understanding_2019,
	title        = {Understanding {Attention} and {Generalization} in {Graph} {Neural} {Networks}},
	author       = {Knyazev, Boris and Taylor, Graham W and Amer, Mohamed},
	year         = 2019,
	booktitle    = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher    = {Curran Associates, Inc.},
	volume       = 32
}
@misc{noauthor_homepage_nodate,
	title        = {Homepage - {Grafana}}
}
@misc{noauthor_jupyterhub_nodate,
	title        = {{JupyterHub}}
}
@misc{noauthor_mila_nodate,
	title        = {Mila technical documentation — {MILA} {Technical} {Documentation} latest documentation}
}
@misc{noauthor_moscot_2022,
	title        = {Moscot: {A} scalable toolbox for optimal transport problems in single cell genomics},
	shorttitle   = {Moscot},
	year         = 2022,
	journal      = {Broad Institute},
	abstract     = {Numerous problems in single-cell genomics have recently been approached using optimal transport (OT), a field of mathematics concerned with comparing probability distributions across spaces. These problems include mapping cells across timepoints, perturbations and experimental batches as well as reconstructing spatial structure from gene expression. Despite their successful applications, OT-based solutions face common challenges that hinder their community-wide adaptation including a fractured tools landscape, limited scalability and lacking support for multimodal data.}
}
@article{leonard_lazy_2016,
	title        = {Lazy random walks and optimal transport on graphs},
	author       = {Léonard, Christian},
	year         = 2016,
	journal      = {The Annals of Probability},
	volume       = 44,
	number       = 3,
	pages        = {1864--1915},
	abstract     = {This paper is about the construction of displacement interpolations of probability distributions on a discrete metric graph. Our approach is based on the approximation of any optimal transport problem whose cost function is a distance on a discrete graph by a sequence of entropy minimization problems under marginal constraints, called Schrödinger problems, which are associated with random walks. Displacement interpolations are defined as the limit of the time-marginal flows of the solutions to the Schrödinger problems as the jump frequencies of the random walks tend down to zero. The main convergence results are based on \${\textbackslash}Gamma\$-convergence of entropy minimization problems. As a by-product, we obtain new results about optimal transport on graphs.}
}
@article{memoli_gromovwasserstein_2011,
	title        = {Gromov–{Wasserstein} {Distances} and the {Metric} {Approach} to {Object} {Matching}},
	author       = {Mémoli, Facundo},
	year         = 2011,
	journal      = {Found Comput Math},
	volume       = 11,
	number       = 4,
	pages        = {417--487},
	abstract     = {This paper discusses certain modiﬁcations of the ideas concerning the Gromov–Hausdorff distance which have the goal of modeling and tackling the practical problems of object matching and comparison. Objects are viewed as metric measure spaces, and based on ideas from mass transportation, a Gromov–Wasserstein type of distance between objects is deﬁned. This reformulation yields a distance between objects which is more amenable to practical computations but retains all the desirable theoretical underpinnings. The theoretical properties of this new notion of distance are studied, and it is established that it provides a strict metric on the collection of isomorphism classes of metric measure spaces. Furthermore, the topology generated by this metric is studied, and sufﬁcient conditions for the pre-compactness of families of metric measure spaces are identiﬁed. A second goal of this paper is to establish links to several other practical methods proposed in the literature for comparing/matching shapes in precise terms. This is done by proving explicit lower bounds for the proposed distance that involve many of the invariants previously reported by researchers. These lower bounds can be computed in polynomial time. The numerical implementations of the ideas are discussed and computational examples are presented.},
	annote       = {Cited in Diffusion Schrodinger Bridges with application to Score Based Generative Modeling (Bortoli et al.) In the discussion they say that their algo DSB could be used to approximate the Gromov-Wasserstein distance}
}
@article{wasserstein_numerical_nodate,
	title        = {Numerical {Optimal} {Transport}},
	author       = {Wasserstein, Gromov},
	pages        = 13
}
@article{chen_optimal_2021,
	title        = {Optimal {Transport} in {Systems} and {Control}},
	author       = {Chen, Yongxin and Georgiou, Tryphon T. and Pavon, Michele},
	year         = 2021,
	journal      = {Annual Review of Control, Robotics, and Autonomous Systems},
	volume       = 4,
	number       = 1,
	pages        = {89--113},
	abstract     = {Optimal transport began as the problem of how to efficiently redistribute goods between production and consumers and evolved into a far-reaching geometric variational framework for studying flows of distributions on metric spaces. This theory enables a class of stochastic control problems to regulate dynamical systems so as to limit uncertainty to within specified limits. Representative control examples include the landing of a spacecraft aimed probabilistically toward a target and the suppression of undesirable effects of thermal noise on resonators; in both of these examples, the goal is to regulate the flow of the distribution of the random state. A most unlikely link turned up between transport of probability distributions and a maximum entropy inference problem posed by Erwin Schrödinger, where the latter is seen as an entropy-regularized version of the former. These intertwined topics of optimal transport, stochastic control, and inference are the subject of this review, which aims to highlight connections, insights, and computational tools while touching on quadratic regulator theory and probabilistic flows in discrete spaces and networks.}
}
@misc{schafer_sensitivity_2021,
	title        = {Sensitivity {Analysis} of {Hybrid} {Differential} {Equations}},
	author       = {Schäfer, Frank},
	year         = 2021,
	journal      = {FS},
	abstract     = {In this post, we discuss sensitivity analysis of differential equations with state changes caused by events triggered at defined moments, for example reflections, bounces off a wall or other sudden forces.}
}
@article{vargas_solving_2021,
	title        = {Solving {Schrödinger} {Bridges} via {Maximum} {Likelihood}},
	author       = {Vargas, Francisco and Thodoroff, Pierre and Lamacraft, Austen and Lawrence, Neil},
	year         = 2021,
	journal      = {Entropy},
	volume       = 23,
	number       = 9,
	pages        = 1134,
	copyright    = {http://creativecommons.org/licenses/by/3.0/},
	abstract     = {The Schrödinger bridge problem (SBP) finds the most likely stochastic evolution between two probability distributions given a prior stochastic evolution. As well as applications in the natural sciences, problems of this kind have important applications in machine learning such as dataset alignment and hypothesis testing. Whilst the theory behind this problem is relatively mature, scalable numerical recipes to estimate the Schrödinger bridge remain an active area of research. Our main contribution is the proof of equivalence between solving the SBP and an autoregressive maximum likelihood estimation objective. This formulation circumvents many of the challenges of density estimation and enables direct application of successful machine learning techniques. We propose a numerical procedure to estimate SBPs using Gaussian process and demonstrate the practical usage of our approach in numerical simulations and experiments.},
	annote       = {Cited in Diffusion Schrodinger bridge with application to SC matching (Bortoli, Doucet et al.)}
}
@article{nutz_introduction_nodate,
	title        = {Introduction to {Entropic} {Optimal} {Transport}},
	author       = {Nutz, Marcel},
	pages        = 75,
	abstract     = {This text develops mathematical foundations for entropic optimal transport and Sinkhorn’s algorithm in a self-contained yet general way. It is a revised version of lecture notes from a course given in Paris during the fall of 2021; some parts date back to an earlier course at Columbia University in 2020.}
}
@misc{noauthor_stanford_nodate,
	title        = {Stanford {University} {CS231n}: {Convolutional} {Neural} {Networks} for {Visual} {Recognition}}
}
@misc{noauthor_current_nodate,
	title        = {Current best practices in single-cell {RNA}-seq analysis: a tutorial - {Recherche} {Google}}
}
@misc{noauthor_meld_2022,
	title        = {{MELD}},
	year         = 2022,
	publisher    = {Krishnaswamy Lab},
	abstract     = {Quantifying experimental perturbations at single cell resolution}
}
@misc{noauthor_managing_nodate,
	title        = {Managing environments — conda 4.12.0.post16+7651023c documentation}
}
@misc{noauthor_moser_nodate,
	title        = {Moser {Flow}: {Divergence}-based {Generative} {Modeling} on {Manifolds} - {Recherche} {Google}}
}
@article{genevay_entropy-regularized_nodate,
	title        = {From {Entropy}-{Regularized} {OT} to   {Sinkhorn} {Divergences}},
	author       = {Genevay, Aude},
	pages        = 47
}
@article{van2019onecut,
	title        = {ONECUT transcription factors induce neuronal characteristics and remodel chromatin accessibility},
	author       = {van der Raadt, Jori and van Gestel, Sebastianus HC and Nadif Kasri, Nael and Albers, Cornelis A},
	year         = 2019,
	journal      = {Nucleic acids research},
	publisher    = {Oxford University Press},
	volume       = 47,
	number       = 11,
	pages        = {5587--5602}
}
@article{lei2011targeted,
	title        = {Targeted deletion of Hand2 in enteric neural precursor cells affects its functions in neurogenesis, neurotransmitter specification and gangliogenesis, causing functional aganglionosis},
	author       = {Lei, Jun and Howard, Marthe J},
	year         = 2011,
	journal      = {Development},
	publisher    = {Company of Biologists},
	volume       = 138,
	number       = 21,
	pages        = {4789--4800}
}
@article{fennell2022non,
	title        = {Non-genetic determinants of malignant clonal fitness at single-cell resolution},
	author       = {Fennell, Katie A and Vassiliadis, Dane and Lam, Enid YN and Martelotto, Luciano G and Balic, Jesse J and Hollizeck, Sebastian and Weber, Tom S and Semple, Timothy and Wang, Qing and Miles, Denise C and others},
	year         = 2022,
	journal      = {Nature},
	publisher    = {Nature Publishing Group},
	volume       = 601,
	number       = 7891,
	pages        = {125--131}
}
@article{gretton2008kernel,
	title        = {A Kernel Method for the Two-Sample Problem},
	author       = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
	year         = 2008,
	journal      = {Journal of Machine Learning Research},
	publisher    = {Citeseer},
	volume       = 1,
	pages        = {1--10}
}
@inproceedings{zhang2020learning,
	title        = {Learning Invariant Representations for Reinforcement Learning without Reconstruction},
	author       = {Zhang, Amy and McAllister, Rowan Thomas and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
	year         = 2020,
	booktitle    = {International Conference on Learning Representations}
}
@article{lasry2007mean,
	title        = {Mean field games},
	author       = {Lasry, Jean-Michel and Lions, Pierre-Louis},
	year         = 2007,
	journal      = {Japanese journal of mathematics},
	publisher    = {Springer},
	volume       = 2,
	number       = 1,
	pages        = {229--260}
}
@article{chen2016relation,
	title        = {On the relation between optimal transport and Schr{\"o}dinger bridges: A stochastic control viewpoint},
	author       = {Chen, Yongxin and Georgiou, Tryphon T and Pavon, Michele},
	year         = 2016,
	journal      = {Journal of Optimization Theory and Applications},
	publisher    = {Springer},
	volume       = 169,
	number       = 2,
	pages        = {671--691}
}
@article{ghoussoub2021solution,
	title        = {A solution to the Monge transport problem for Brownian martingales},
	author       = {Ghoussoub, Nassif and Kim, Young-Heon and Palmer, Aaron Zeff},
	year         = 2021,
	journal      = {the Annals of Probability},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 49,
	number       = 2,
	pages        = {877--907}
}
@article{street2018slingshot,
	title        = {Slingshot: cell lineage and pseudotime inference for single-cell transcriptomics},
	author       = {Street, Kelly and Risso, Davide and Fletcher, Russell B and Das, Diya and Ngai, John and Yosef, Nir and Purdom, Elizabeth and Dudoit, Sandrine},
	year         = 2018,
	journal      = {BMC genomics},
	publisher    = {Springer},
	volume       = 19,
	number       = 1,
	pages        = {1--16}
}
@article{zhang2021optimal,
	title        = {Optimal transport analysis reveals trajectories in steady-state systems},
	author       = {Zhang, Stephen and Afanassiev, Anton and Greenstreet, Laura and Matsumoto, Tetsuya and Schiebinger, Geoffrey},
	year         = 2021,
	journal      = {PLoS computational biology},
	publisher    = {Public Library of Science San Francisco, CA USA},
	volume       = 17,
	number       = 12,
	pages        = {e1009466}
}
@article{saelens2019comparison,
	title        = {A comparison of single-cell trajectory inference methods},
	author       = {Saelens, Wouter and Cannoodt, Robrecht and Todorov, Helena and Saeys, Yvan},
	year         = 2019,
	journal      = {Nature biotechnology},
	publisher    = {Nature Publishing Group},
	volume       = 37,
	number       = 5,
	pages        = {547--554}
}
@inproceedings{hashimoto2016learning,
	title        = {Learning population-level diffusions with generative RNNs},
	author       = {Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi},
	year         = 2016,
	booktitle    = {International Conference on Machine Learning},
	pages        = {2417--2426},
	organization = {PMLR}
}
@article{prasad2020optimal,
	title        = {Optimal transport using GANs for lineage tracing},
	author       = {Prasad, Neha and Yang, Karren and Uhler, Caroline},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2007.12098}
}
@article{cannoodt2021spearheading,
	title        = {Spearheading future omics analyses using dyngen, a multi-modal simulator of single cells},
	author       = {Cannoodt, Robrecht and Saelens, Wouter and Deconinck, Louise and Saeys, Yvan},
	year         = 2021,
	journal      = {Nature Communications},
	publisher    = {Nature Publishing Group},
	volume       = 12,
	number       = 1,
	pages        = {1--9}
}
@book{villani2009optimal,
	title        = {Optimal transport: old and new},
	author       = {Villani, C{\'e}dric},
	year         = 2009,
	publisher    = {Springer},
	volume       = 338
}
@article{monge1781memoire,
	title        = {M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
	author       = {Monge, Gaspard},
	year         = 1781,
	journal      = {Histoire de l'Acad{\'e}mie Royale des Sciences de Paris}
}
@article{chizat2018unbalanced,
	title        = {Unbalanced optimal transport: Dynamic and Kantorovich formulations},
	author       = {Chizat, Lenaic and Peyr{\'e}, Gabriel and Schmitzer, Bernhard and Vialard, Fran{\c{c}}ois-Xavier},
	year         = 2018,
	journal      = {Journal of Functional Analysis},
	publisher    = {Elsevier},
	volume       = 274,
	number       = 11,
	pages        = {3090--3123}
}
@article{mikami2006duality,
	title        = {Duality theorem for the stochastic optimal control problem},
	author       = {Mikami, Toshio and Thieullen, Mich{\`e}le},
	year         = 2006,
	journal      = {Stochastic processes and their applications},
	publisher    = {Elsevier},
	volume       = 116,
	number       = 12,
	pages        = {1815--1835}
}
@article{agrachev2010continuity,
	title        = {Continuity of optimal control costs and its application to weak KAM theory},
	author       = {Agrachev, Andrei and Lee, Paul WY},
	year         = 2010,
	journal      = {Calculus of Variations and Partial Differential Equations},
	publisher    = {Springer},
	volume       = 39,
	number       = 1,
	pages        = {213--232}
}
@article{trapnell2014pseudo,
	title        = {Pseudo-temporal ordering of individual cells reveals dynamics and regulators of cell fate decisions},
	author       = {Trapnell, Cole and Cacchiarelli, Davide and Grimsby, Jonna and Pokharel, Prapti and Li, Shuqiang and Morse, Michael and Lennon, Niall J and Livak, Kenneth J and Mikkelsen, Tarjei S and Rinn, John L},
	year         = 2014,
	journal      = {Nature biotechnology},
	publisher    = {NIH Public Access},
	volume       = 32,
	number       = 4,
	pages        = 381
}
@article{mikami2004monge,
	title        = {Monge’s problem with a quadratic cost by the zero-noise limit of h-path processes},
	author       = {Mikami, Toshio},
	year         = 2004,
	journal      = {Probability theory and related fields},
	publisher    = {Springer},
	volume       = 129,
	number       = 2,
	pages        = {245--260}
}
@article{pavon2021data,
	title        = {The Data-Driven Schr{\"o}dinger Bridge},
	author       = {Pavon, Michele and Trigila, Giulio and Tabak, Esteban G},
	year         = 2021,
	journal      = {Communications on Pure and Applied Mathematics},
	publisher    = {Wiley Online Library},
	volume       = 74,
	number       = 7,
	pages        = {1545--1573}
}
@article{de2021diffusion,
	title        = {Diffusion Schr{\"o}dinger bridge with applications to score-based generative modeling},
	author       = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	year         = 2021,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 34
}

@inproceedings{tong2021diffusion,
	title        = {Diffusion Earth Mover’s Distance and Distribution Embeddings},
	author       = {Tong, Alexander Y and Huguet, Guillaume and Natik, Amine and MacDonald, Kincaid and Kuchroo, Manik and Coifman, Ronald and Wolf, Guy and Krishnaswamy, Smita},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {10336--10346},
	organization = {PMLR}
}
@article{flamary2021pot,
	title        = {POT: Python Optimal Transport},
	author       = {R{\'e}mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur{\'e}lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L{\'e}o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
	year         = 2021,
	journal      = {Journal of Machine Learning Research},
	volume       = 22,
	number       = 78,
	pages        = {1--8}
}
@article{dormand1980family,
	title        = {A family of embedded Runge-Kutta formulae},
	author       = {Dormand, John R and Prince, Peter J},
	year         = 1980,
	journal      = {Journal of computational and applied mathematics},
	publisher    = {Elsevier},
	volume       = 6,
	number       = 1,
	pages        = {19--26}
}
@article{mathieu2020riemannian,
	title        = {Riemannian continuous normalizing flows},
	author       = {Mathieu, Emile and Nickel, Maximilian},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {2503--2515}
}
@book{gardiner1985handbook,
	title        = {Handbook of stochastic methods},
	author       = {Gardiner, Crispin W and others},
	year         = 1985,
	publisher    = {springer Berlin},
	volume       = 3
}
@misc{beitler_pie_2021,
	title        = {{{PIE}}: {{Pseudo-Invertible Encoder}}},
	shorttitle   = {{{PIE}}},
	author       = {Beitler, Jan Jetze and Sosnovik, Ivan and Smeulders, Arnold},
	year         = 2021,
	number       = {arXiv:2111.00619},
	eprint       = {2111.00619},
	eprinttype   = {arxiv},
	primaryclass = {cs},
	institution  = {{arXiv}},
	abstract     = {We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible transformations, we emphasize the importance of invertible compression. We introduce new class of likelihood-based autoencoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperforms WAE and VAE in sharpness of the generated images.},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning}
}
@article{brehmer_flows_2020,
	title        = {Flows for Simultaneous Manifold Learning and Density Estimation},
	author       = {Brehmer, Johann and Cranmer, Kyle},
	year         = 2020,
	journal      = {NeurIPS},
	eprint       = {2003.13913},
	eprinttype   = {arxiv},
	primaryclass = {cs, stat},
	abstract     = {We introduce manifold-learning flows (M-flows), a new class of generative models that simultaneously learn the data manifold as well as a tractable probability density on that manifold. Combining aspects of normalizing flows, GANs, autoencoders, and energy-based models, they have the potential to represent datasets with a manifold structure more faithfully and provide handles on dimensionality reduction, denoising, and out-of-distribution detection. We argue why such models should not be trained by maximum likelihood alone and present a new training algorithm that separates manifold and density updates. In a range of experiments we demonstrate how M-flows learn the data manifold and allow for better inference than standard flows in the ambient data space.},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Machine Learning,Statistics - Machine Learning}
}
@article{bunne_proximal_2022,
	title        = {Proximal {{Optimal Transport Modeling}} of {{Population Dynamics}}},
	author       = {Bunne, Charlotte and {Meng-Papaxanthos}, Laetitia and Krause, Andreas and Cuturi, Marco},
	year         = 2022,
	journal      = {AISTATS},
	eprint       = {2106.06345},
	eprinttype   = {arxiv},
	primaryclass = {cs},
	abstract     = {We propose a new approach to model the collective dynamics of a population of particles evolving with time. As is often the case in challenging scientific applications, notably single-cell genomics, measuring features for these particles requires destroying them. As a result, the population can only be monitored with periodic snapshots, obtained by sampling a few particles that are sacrificed in exchange for measurements. Given only access to these snapshots, can we reconstruct likely individual trajectories for all other particles? We propose to model these trajectories as collective realizations of a causal Jordan-Kinderlehrer-Otto (JKO) flow of measures: The JKO scheme posits that the new configuration taken by a population at time \$t+1\$ is one that trades off an improvement, in the sense that it decreases an energy, while remaining close (in Wasserstein distance) to the previous configuration observed at \$t\$. In order to learn such an energy using only snapshots, we propose JKOnet, a neural architecture that computes (in end-to-end differentiable fashion) the JKO flow given a parametric energy and initial configuration of points. We demonstrate the good performance and robustness of the JKOnet fitting procedure, compared to a more direct forward method.},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Machine Learning}
}
@misc{cunningham_principal_2022,
	title        = {Principal {{Manifold Flows}}},
	author       = {Cunningham, Edmond and Cobb, Adam and Jha, Susmit},
	year         = 2022,
	number       = {arXiv:2202.07037},
	eprint       = {2202.07037},
	eprinttype   = {arxiv},
	primaryclass = {cs, stat},
	institution  = {{arXiv}},
	abstract     = {Normalizing flows map an independent set of latent variables to their samples using a bijective transformation. Despite the exact correspondence between samples and latent variables, their high level relationship is not well understood. In this paper we characterize the geometric structure of flows using principal manifolds and understand the relationship between latent variables and samples using contours. We introduce a novel class of normalizing flows, called principal manifold flows (PF), whose contours are its principal manifolds, and a variant for injective flows (iPF) that is more efficient to train than regular injective flows. PFs can be constructed using any flow architecture, are trained with a regularized maximum likelihood objective and can perform density estimation on all of their principal manifolds. In our experiments we show that PFs and iPFs are able to learn the principal manifolds over a variety of datasets. Additionally, we show that PFs can perform density estimation on data that lie on a manifold with variable dimensionality, which is not possible with existing normalizing flows.},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Machine Learning,Statistics - Machine Learning}
}
@inproceedings{grathwohl_ffjord:_2019,
	title        = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
	shorttitle   = {{{FFJORD}}},
	author       = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	year         = 2019,
	booktitle    = {7th {{International Conference}} on {{Learning Representations}}},
	eprint       = {1810.01367},
	eprinttype   = {arxiv},
	abstract     = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning}
}
@article{ho_denoising_2020,
	title        = {Denoising {{Diffusion Probabilistic Models}}},
	author       = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
    pages = {6840--6851},
    volume = {33},
	eprint       = {2006.11239},
	eprinttype   = {arxiv},
	primaryclass = {cs, stat},
	abstract     = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Machine Learning,Statistics - Machine Learning}
}
@inproceedings{horvat_denoising_2021,
	title        = {Denoising {{Normalizing Flow}}},
	author       = {Horvat, Christian and Pfister, Jean-Pascal},
	year         = 2021,
	booktitle    = {Advances in {{Neural Information Processing Systems}}},
	publisher    = {{Curran Associates, Inc.}},
	volume       = 34,
	pages        = {9099--9111},
	abstract     = {Normalizing flows (NF) are expressive as well as tractable density estimation methods whenever the support of the density is diffeomorphic to the entire data-space. However, real-world data sets typically live on (or very close to) low-dimensional manifolds thereby challenging the applicability of standard NF on real-world problems. Here we propose a novel method - called Denoising Normalizing Flow (DNF) - that estimates the density on the low-dimensional manifold while learning the manifold as well. The DNF works in 3 steps. First, it inflates the manifold - making it diffeomorphic to the entire data-space. Secondly, it learns an NF on the inflated manifold and finally it learns a denoising mapping - similarly to denoising autoencoders. The DNF relies on a single cost function and does not require to alternate between a density estimation phase and a manifold learning phase - as it is the case with other recent methods. Furthermore, we show that the DNF can learn meaningful low-dimensional representations from naturalistic images as well as generate high-quality samples.}
}
@article{song_generative_2019,
	title        = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
	author       = {Song, Yang and Ermon, Stefano},
	year         = 2019,
	journal      = {Advances in Neural Information Processing Systems},
	eprint       = {1907.05600},
	eprinttype   = {arxiv},
	pages={11918--11930},
	primaryclass = {cs, stat},
	abstract     = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	archiveprefix = {arXiv},
	keywords     = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@inproceedings{smola2006maximum,
  title={Maximum mean discrepancy},
  author={Smola, Alexander J and Gretton, A and Borgwardt, K},
  booktitle={13th International Conference, ICONIP 2006, Hong Kong, China, 2006: Proceedings},
  year={2006}
}

@article{solomon2015convolutional,
  title={Convolutional wasserstein distances: Efficient optimal transportation on geometric domains},
  author={Solomon, Justin and De Goes, Fernando and Peyr{\'e}, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
  journal={ACM Transactions on Graphics (ToG)},
  volume={34},
  number={4},
  pages={1--11},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@inproceedings{tong2022embedding,
  title={Embedding Signals on Graphs with Unbalanced Diffusion Earth Mover’s Distance},
  author={Tong, Alexander and Huguet, Guillaume and Shung, Dennis and Natik, Amine and Kuchroo, Manik and Lajoie, Guillaume and Wolf, Guy and Krishnaswamy, Smita},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={5647--5651},
  year={2022},
}

@inproceedings{duque2020extendable,
  title={Extendable and invertible manifold learning with geometry regularized autoencoders},
  author={Duque, Andr{\'e}s F and Morin, Sacha and Wolf, Guy and Moon, Kevin},
  booktitle={2020 IEEE International Conference on Big Data (Big Data)},
  pages={5027--5036},
  year={2020},
  organization={IEEE}
}

@article{mishne2019diffusion,
  title={Diffusion nets},
  author={Mishne, Gal and Shaham, Uri and Cloninger, Alexander and Cohen, Israel},
  journal={Applied and Computational Harmonic Analysis},
  volume={47},
  number={2},
  pages={259--285},
  year={2019},
  publisher={Elsevier}
}


@article{moon_manifold_2018,
  title = {Manifold Learning-Based Methods for Analyzing Single-Cell {{RNA-sequencing}} Data},
  author = {Moon, Kevin R. and Stanley, Jay S. and Burkhardt, Daniel and {van Dijk}, David and Wolf, Guy and Krishnaswamy, Smita},
  year = {2018},
  journal = {Current Opinion in Systems Biology},
  volume = {7},
  pages = {36--46},
}

@article{cheng2022eigen,
  title={Eigen-convergence of Gaussian kernelized graph Laplacian by manifold heat interpolation},
  author={Cheng, Xiuyuan and Wu, Nan},
  journal={Applied and Computational Harmonic Analysis},
  volume={61},
  pages={132--190},
  year={2022},
  publisher={Elsevier}
}


@article{koshizuka2022neural,
  title={Neural Lagrangian Schr\"odinger bridge},
  author={Koshizuka, Takeshi and Sato, Issei},
  journal={arXiv preprint arXiv:2204.04853},
  year={2022}
}

@article{richter2022neural,
  title={Neural Conservation Laws: A Divergence-Free Perspective},
  author={Richter-Powell, Jack and Lipman, Yaron and Chen, Ricky TQ},
  journal={arXiv preprint arXiv:2210.01741},
  year={2022}
}

@inproceedings{finlay2020train,
  title={How to train your neural ODE: the world of Jacobian and kinetic regularization},
  author={Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam},
  booktitle={International conference on machine learning},
  pages={3154--3164},
  year={2020},
  organization={PMLR}
}

@inproceedings{wang2021deep,
  title={Deep generative learning via schr{\"o}dinger bridge},
  author={Wang, Gefei and Jiao, Yuling and Xu, Qian and Wang, Yang and Yang, Can},
  booktitle={International Conference on Machine Learning},
  pages={10794--10804},
  year={2021},
  organization={PMLR}
}

@article{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky TQ and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{song2020score,
  title={Score-Based Generative Modeling through Stochastic Differential Equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{molchanov1975diffusion,
  title={Diffusion processes and Riemannian geometry},
  author={Molchanov, Stanislav A},
  journal={Russian Mathematical Surveys},
  volume={30},
  number={1},
  pages={1},
  year={1975},
  publisher={IOP Publishing}
}

@article{varadhan1967behavior,
  title={On the behavior of the fundamental solution of the heat equation with variable coefficients},
  author={Varadhan, Sathamangalam R Srinivasa},
  journal={Communications on Pure and Applied Mathematics},
  volume={20},
  number={2},
  pages={431--455},
  year={1967},
  publisher={Wiley Online Library}
}

@article{belkin2003laplacian,
  title={Laplacian eigenmaps for dimensionality reduction and data representation},
  author={Belkin, Mikhail and Niyogi, Partha},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1373--1396},
  year={2003},
  publisher={MIT Press}
}

@article{defferrard2016convolutional,
  title={Convolutional neural networks on graphs with fast localized spectral filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{crane2013geodesics,
  title={Geodesics in heat: A new approach to computing distance based on heat flow},
  author={Crane, Keenan and Weischedel, Clarisse and Wardetzky, Max},
  journal={ACM Transactions on Graphics (TOG)},
  volume={32},
  number={5},
  pages={1--11},
  year={2013},
  publisher={ACM New York, NY, USA}
}

@inproceedings{hassani2020contrastive,
  title={Contrastive multi-view representation learning on graphs},
  author={Hassani, Kaveh and Khasahmadi, Amir Hosein},
  booktitle={International conference on machine learning},
  pages={4116--4126},
  year={2020},
  organization={PMLR}
}

@article{lipman2010biharmonic,
  title={Biharmonic distance},
  author={Lipman, Yaron and Rustamov, Raif M and Funkhouser, Thomas A},
  journal={ACM Transactions on Graphics (TOG)},
  volume={29},
  number={3},
  pages={1--11},
  year={2010},
  publisher={ACM New York, NY, USA}
}

@article{spielman2012spectral,
  title={Spectral graph theory},
  author={Spielman, Daniel},
  journal={Combinatorial scientific computing},
  volume={18},
  pages={18},
  year={2012},
  publisher={CRC Press Boca Raton, Florida}
}

@inproceedings{hammond2013graph,
  title={Graph diffusion distance: A difference measure for weighted graphs based on the graph Laplacian exponential kernel},
  author={Hammond, David K and Gur, Yaniv and Johnson, Chris R},
  booktitle={2013 IEEE global conference on signal and information processing},
  pages={419--422},
  year={2013},
  organization={IEEE}
}

@article{carroll1998multidimensional,
  title={Multidimensional scaling},
  author={Carroll, J Douglas and Arabie, Phipps},
  journal={Measurement, judgment and decision making},
  pages={179--250},
  year={1998},
  publisher={Elsevier}
}

@article{hout2013multidimensional,
  title={Multidimensional scaling},
  author={Hout, Michael C and Papesh, Megan H and Goldinger, Stephen D},
  journal={Wiley Interdisciplinary Reviews: Cognitive Science},
  volume={4},
  number={1},
  pages={93--103},
  year={2013},
  publisher={Wiley Online Library}
}

@article{tenenbaum2000global,
  title={A global geometric framework for nonlinear dimensionality reduction},
  author={Tenenbaum, Joshua B and Silva, Vin de and Langford, John C},
  journal={science},
  volume={290},
  number={5500},
  pages={2319--2323},
  year={2000},
  publisher={American Association for the Advancement of Science}
}

@article{kruskal1964multidimensional,
  title={Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
  author={Kruskal, Joseph B},
  journal={Psychometrika},
  volume={29},
  number={1},
  pages={1--27},
  year={1964},
  publisher={Springer}
}

@article{takane1977nonmetric,
  title={Nonmetric individual differences multidimensional scaling: An alternating least squares method with optimal scaling features},
  author={Takane, Yoshio and Young, Forrest W and De Leeuw, Jan},
  journal={Psychometrika},
  volume={42},
  pages={7--67},
  year={1977},
  publisher={Springer}
}

@article{huang2020fast,
  title={Fast polynomial approximation of heat kernel convolution on manifolds and its application to brain sulcal and gyral graph pattern analysis},
  author={Huang, Shih-Gu and Lyu, Ilwoo and Qiu, Anqi and Chung, Moo K},
  journal={IEEE transactions on medical imaging},
  volume={39},
  number={6},
  pages={2201--2212},
  year={2020},
  publisher={IEEE}
}

@article{schiebinger2019optimal,
  title={Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming},
  author={Schiebinger, Geoffrey and Shu, Jian and Tabaka, Marcin and Cleary, Brian and Subramanian, Vidya and Solomon, Aryeh and Gould, Joshua and Liu, Siyan and Lin, Stacie and Berube, Peter and others},
  journal={Cell},
  volume={176},
  number={4},
  pages={928--943},
  year={2019},
  publisher={Elsevier}
}

@article{ziegler2021impaired,
  title={Impaired local intrinsic immunity to SARS-CoV-2 infection in severe COVID-19},
  author={Ziegler, Carly GK and Miao, Vincent N and Owings, Anna H and Navia, Andrew W and Tang, Ying and Bromley, Joshua D and Lotfy, Peter and Sloan, Meredith and Laird, Hannah and Williams, Haley B and others},
  journal={Cell},
  volume={184},
  pages={4713--4733},
  year={2021},
  publisher={Elsevier}
}

@article{sharp2019vector,
  title={The vector heat method},
  author={Sharp, Nicholas and Soliman, Yousuf and Crane, Keenan},
  journal={ACM Transactions on Graphics (TOG)},
  volume={38},
  number={3},
  pages={1--19},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}

@article{bohm2022attraction,
  title={Attraction-repulsion spectrum in neighbor embeddings},
  author={B{\"o}hm, Jan Niklas and Berens, Philipp and Kobak, Dmitry},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={95},
  pages={1--32},
  year={2022}
}

@article{heitz2021ground,
  title={Ground metric learning on graphs},
  author={Heitz, Matthieu and Bonneel, Nicolas and Coeurjolly, David and Cuturi, Marco and Peyr{\'e}, Gabriel},
  journal={Journal of Mathematical Imaging and Vision},
  volume={63},
  pages={89--107},
  year={2021},
  publisher={Springer}
}

@misc{pygsp,
  title = {PyGSP: Graph Signal Processing in Python},
  author = {Defferrard, Micha\"el and Martin, Lionel and Pena, Rodrigo and Perraudin, Nathana\"el},
  doi = {10.5281/zenodo.1003157},
  url = {https://github.com/epfl-lts2/pygsp/},
}

@article{wolf2018scanpy,
  title={SCANPY: large-scale single-cell gene expression data analysis},
  author={Wolf, F Alexander and Angerer, Philipp and Theis, Fabian J},
  journal={Genome biology},
  volume={19},
  pages={1--5},
  year={2018},
  publisher={Springer}
}

@inproceedings{satopaa2011finding,
  title={Finding a" kneedle" in a haystack: Detecting knee points in system behavior},
  author={Satopaa, Ville and Albrecht, Jeannie and Irwin, David and Raghavan, Barath},
  booktitle={2011 31st international conference on distributed computing systems workshops},
  pages={166--171},
  year={2011},
  organization={IEEE}
}

@article{zhou2020learning,
  title={Learning manifold implicitly via explicit heat-kernel learning},
  author={Zhou, Yufan and Chen, Changyou and Xu, Jinhui},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={477--487},
  year={2020}
}

@inproceedings{sun2009concise,
  title={A concise and provably informative multi-scale signature based on heat diffusion},
  author={Sun, Jian and Ovsjanikov, Maks and Guibas, Leonidas},
  booktitle={Computer graphics forum},
  volume={28},
  number={5},
  pages={1383--1392},
  year={2009},
  organization={Wiley Online Library}
}

@article{nowak2019sharp,
  title={Sharp estimates of the spherical heat kernel},
  author={Nowak, Adam and Sj{\"o}gren, Peter and Szarek, Tomasz Z},
  journal={Journal de Math{\'e}matiques Pures et Appliqu{\'e}es},
  volume={129},
  pages={23--33},
  year={2019},
  publisher={Elsevier}
}

@article{villani2009displacement,
  title={Displacement interpolation},
  author={Villani, C{\'e}dric and Villani, C{\'e}dric},
  journal={Optimal Transport: Old and New},
  pages={113--162},
  year={2009},
  publisher={Springer}
}


@article{zheng2018graph,
  title={Graph drawing by stochastic gradient descent},
  author={Zheng, Jonathan X and Pawar, Samraat and Goodman, Dan FM},
  journal={IEEE transactions on visualization and computer graphics},
  volume={25},
  number={9},
  pages={2738--2748},
  year={2018},
  publisher={IEEE}
}

@article{hinton2002stochastic,
  title={Stochastic neighbor embedding},
  author={Hinton, Geoffrey E and Roweis, Sam},
  journal={Advances in neural information processing systems},
  volume={15},
  year={2002}
}
@article{van2008visualizing,
	title        = {Visualizing data using t-SNE.},
	author       = {Van der Maaten, Laurens and Hinton, Geoffrey},
	year         = 2008,
	journal      = {Journal of machine learning research},
	volume       = 9,
	number       = 11
}